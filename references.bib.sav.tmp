 
@TechReport{Singer2022,
  author     = {Singer, Uriel and Polyak, Adam and Hayes, Thomas and Yin, Xi and An, Jie and Zhang, Songyang and Hu, Qiyuan and Yang, Harry and Ashual, Oron and Gafni, Oran and Parikh, Devi and Gupta, Sonal and Taigman, Yaniv},
  title      = {Make-{A}-{Video}: {Text}-to-{Video} {Generation} without {Text}-{Video} {Data}},
  year       = {2022},
  month      = sep,
  note       = {arXiv:2209.14792 [cs] type: article},
  abstract   = {We propose Make-A-Video -- an approach for directly translating the tremendous recent progress in Text-to-Image (T2I) generation to Text-to-Video (T2V). Our intuition is simple: learn what the world looks like and how it is described from paired text-image data, and learn how the world moves from unsupervised video footage. Make-A-Video has three advantages: (1) it accelerates training of the T2V model (it does not need to learn visual and multimodal representations from scratch), (2) it does not require paired text-video data, and (3) the generated videos inherit the vastness (diversity in aesthetic, fantastical depictions, etc.) of today's image generation models. We design a simple yet effective way to build on T2I models with novel and effective spatial-temporal modules. First, we decompose the full temporal U-Net and attention tensors and approximate them in space and time. Second, we design a spatial temporal pipeline to generate high resolution and frame rate videos with a video decoder, interpolation model and two super resolution models that can enable various applications besides T2V. In all aspects, spatial and temporal resolution, faithfulness to text, and quality, Make-A-Video sets the new state-of-the-art in text-to-video generation, as determined by both qualitative and quantitative measures.},
  doi        = {10.48550/arXiv.2209.14792},
  file       = {:Singer2022 - Make a Video_ Text to Video Generation without Text Video Data.pdf:PDF},
  groups     = {Others},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  school     = {arXiv},
  shorttitle = {Make-{A}-{Video}},
  url        = {http://arxiv.org/abs/2209.14792},
  urldate    = {2022-10-07},
}

 
@TechReport{Yang2022,
  author   = {Yang, Ziqing and He, Xinlei and Li, Zheng and Backes, Michael and Humbert, Mathias and Berrang, Pascal and Zhang, Yang},
  title    = {Data {Poisoning} {Attacks} {Against} {Multimodal} {Encoders}},
  year     = {2022},
  month    = sep,
  note     = {arXiv:2209.15266 [cs] type: article},
  abstract = {Traditional machine learning (ML) models usually rely on large-scale labeled datasets to achieve strong performance. However, such labeled datasets are often challenging and expensive to obtain. Also, the predefined categories limit the model's ability to generalize to other visual concepts as additional labeled data is required. On the contrary, the newly emerged multimodal model, which contains both visual and linguistic modalities, learns the concept of images from the raw text. It is a promising way to solve the above problems as it can use easy-to-collect image-text pairs to construct the training dataset and the raw texts contain almost unlimited categories according to their semantics. However, learning from a large-scale unlabeled dataset also exposes the model to the risk of potential poisoning attacks, whereby the adversary aims to perturb the model's training dataset to trigger malicious behaviors in it. Previous work mainly focuses on the visual modality. In this paper, we instead focus on answering two questions: (1) Is the linguistic modality also vulnerable to poisoning attacks? and (2) Which modality is most vulnerable? To answer the two questions, we conduct three types of poisoning attacks against CLIP, the most representative multimodal contrastive learning framework. Extensive evaluations on different datasets and model architectures show that all three attacks can perform well on the linguistic modality with only a relatively low poisoning rate and limited epochs. Also, we observe that the poisoning effect differs between different modalities, i.e., with lower MinRank in the visual modality and with higher Hit@K when K is small in the linguistic modality. To mitigate the attacks, we propose both pre-training and post-training defenses. We empirically show that both defenses can significantly reduce the attack performance while preserving the model's utility.},
  doi      = {10.48550/arXiv.2209.15266},
  file     = {:Yang2022 - Data Poisoning Attacks against Multimodal Encoders.pdf:PDF},
  groups   = {Backdoors},
  keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2209.15266},
  urldate  = {2022-10-07},
}

 
@TechReport{Salem2021,
  author   = {Salem, Ahmed and Backes, Michael and Zhang, Yang},
  title    = {Get a {Model}! {Model} {Hijacking} {Attack} {Against} {Machine} {Learning} {Models}},
  year     = {2021},
  month    = nov,
  note     = {arXiv:2111.04394 [cs] type: article},
  abstract = {Machine learning (ML) has established itself as a cornerstone for various critical applications ranging from autonomous driving to authentication systems. However, with this increasing adoption rate of machine learning models, multiple attacks have emerged. One class of such attacks is training time attack, whereby an adversary executes their attack before or during the machine learning model training. In this work, we propose a new training time attack against computer vision based machine learning models, namely model hijacking attack. The adversary aims to hijack a target model to execute a different task than its original one without the model owner noticing. Model hijacking can cause accountability and security risks since a hijacked model owner can be framed for having their model offering illegal or unethical services. Model hijacking attacks are launched in the same way as existing data poisoning attacks. However, one requirement of the model hijacking attack is to be stealthy, i.e., the data samples used to hijack the target model should look similar to the model's original training dataset. To this end, we propose two different model hijacking attacks, namely Chameleon and Adverse Chameleon, based on a novel encoder-decoder style ML model, namely the Camouflager. Our evaluation shows that both of our model hijacking attacks achieve a high attack success rate, with a negligible drop in model utility.},
  annote   = {Comment: To Appear in NDSS 2022},
  doi      = {10.48550/arXiv.2111.04394},
  file     = {:Salem2021 - Get a Model! Model Hijacking Attack against Machine Learning Models.pdf:PDF},
  keywords = {Computer Science - Cryptography and Security, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2111.04394},
  urldate  = {2022-10-07},
}

 
@TechReport{Salem2020,
  author   = {Salem, Ahmed and Backes, Michael and Zhang, Yang},
  title    = {Don't {Trigger} {Me}! {A} {Triggerless} {Backdoor} {Attack} {Against} {Deep} {Neural} {Networks}},
  year     = {2020},
  month    = oct,
  note     = {arXiv:2010.03282 [cs] type: article},
  abstract = {Backdoor attack against deep neural networks is currently being profoundly investigated due to its severe security consequences. Current state-of-the-art backdoor attacks require the adversary to modify the input, usually by adding a trigger to it, for the target model to activate the backdoor. This added trigger not only increases the difficulty of launching the backdoor attack in the physical world, but also can be easily detected by multiple defense mechanisms. In this paper, we present the first triggerless backdoor attack against deep neural networks, where the adversary does not need to modify the input for triggering the backdoor. Our attack is based on the dropout technique. Concretely, we associate a set o