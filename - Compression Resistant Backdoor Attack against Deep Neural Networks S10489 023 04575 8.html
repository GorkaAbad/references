
    

<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="applicable-device" content="pc,mobile">
    <meta name="access" content="Yes">

    
    
    <meta name="twitter:site" content="@SpringerLink"/>
    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:title" content="Compression-resistant backdoor attack against deep neural networks"/>
    <meta name="twitter:description" content="Applied Intelligence - In recent years, a number of backdoor attacks against deep neural networks (DNN) have been proposed. In this paper, we reveal that backdoor attacks are vulnerable to image..."/>
    <meta name="journal_id" content="10489"/>
    <meta name="dc.title" content="Compression-resistant backdoor attack against deep neural networks"/>
    <meta name="dc.source" content="Applied Intelligence 2023"/>
    <meta name="dc.format" content="text/html"/>
    <meta name="dc.publisher" content="Springer"/>
    <meta name="dc.date" content="2023-04-12"/>
    <meta name="dc.type" content="OriginalPaper"/>
    <meta name="dc.language" content="En"/>
    <meta name="dc.copyright" content="2023 The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature"/>
    <meta name="dc.rights" content="2023 The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature"/>
    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>
    <meta name="dc.description" content="In recent years, a number of backdoor attacks against deep neural networks (DNN) have been proposed. In this paper, we reveal that backdoor attacks are vulnerable to image compressions, as backdoor instances used to trigger backdoor attacks are usually compressed by image compression methods during data transmission. When backdoor instances are compressed, the feature of backdoor trigger will be destroyed, which could result in significant performance degradation for backdoor attacks. As a countermeasure, we propose the first compression-resistant backdoor attack method based on feature consistency training. Specifically, both backdoor images and their compressed versions are used for training, and the feature difference between backdoor images and their compressed versions are minimized through feature consistency training. As a result, the DNN treats the feature of compressed images as the feature of backdoor images in feature space. After training, the backdoor attack will be robust to image compressions. Furthermore, we consider three different image compressions (i.e., JPEG, JPEG2000, WEBP) during the feature consistency training, so that the backdoor attack can be robust to multiple image compression algorithms. Experimental results demonstrate that when the backdoor instances are compressed, the attack success rate of common backdoor attack is 6.63% (JPEG), 6.20% (JPEG2000) and 3.97% (WEBP) respectively, while the attack success rate of the proposed compression-resistant backdoor attack is 98.77% (JPEG), 97.69% (JPEG2000), and 98.93% (WEBP) respectively. The compression-resistant attack is robust under various parameters settings. In addition, extensive experiments have demonstrated that even if only one image compression method is used in the feature consistency training process, the proposed compression-resistant backdoor attack has the generalization ability to resist multiple unseen image compression methods."/>
    <meta name="prism.issn" content="1573-7497"/>
    <meta name="prism.publicationName" content="Applied Intelligence"/>
    <meta name="prism.publicationDate" content="2023-04-12"/>
    <meta name="prism.section" content="OriginalPaper"/>
    <meta name="prism.startingPage" content="1"/>
    <meta name="prism.endingPage" content="16"/>
    <meta name="prism.copyright" content="2023 The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature"/>
    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>
    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10489-023-04575-8"/>
    <meta name="prism.doi" content="doi:10.1007/s10489-023-04575-8"/>
    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10489-023-04575-8.pdf"/>
    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10489-023-04575-8"/>
    <meta name="citation_journal_title" content="Applied Intelligence"/>
    <meta name="citation_journal_abbrev" content="Appl Intell"/>
    <meta name="citation_publisher" content="Springer US"/>
    <meta name="citation_issn" content="1573-7497"/>
    <meta name="citation_title" content="Compression-resistant backdoor attack against deep neural networks"/>
    <meta name="citation_online_date" content="2023/04/12"/>
    <meta name="citation_firstpage" content="1"/>
    <meta name="citation_lastpage" content="16"/>
    <meta name="citation_article_type" content="Article"/>
    <meta name="citation_language" content="en"/>
    <meta name="dc.identifier" content="doi:10.1007/s10489-023-04575-8"/>
    <meta name="DOI" content="10.1007/s10489-023-04575-8"/>
    <meta name="size" content="282715"/>
    <meta name="citation_doi" content="10.1007/s10489-023-04575-8"/>
    <meta name="citation_springer_api_url" content="http://api.springer.com/xmldata/jats?q=doi:10.1007/s10489-023-04575-8&amp;api_key="/>
    <meta name="description" content="In recent years, a number of backdoor attacks against deep neural networks (DNN) have been proposed. In this paper, we reveal that backdoor attacks are vul"/>
    <meta name="dc.creator" content="Xue, Mingfu"/>
    <meta name="dc.creator" content="Wang, Xin"/>
    <meta name="dc.creator" content="Sun, Shichang"/>
    <meta name="dc.creator" content="Zhang, Yushu"/>
    <meta name="dc.creator" content="Wang, Jian"/>
    <meta name="dc.creator" content="Liu, Weiqiang"/>
    <meta name="dc.subject" content="Artificial Intelligence"/>
    <meta name="dc.subject" content="Mechanical Engineering"/>
    <meta name="dc.subject" content="Manufacturing, Machines, Tools, Processes"/>
    <meta name="citation_reference" content="Wang Z., Guo H., Zhang Z., Song M., Zheng S., Wang Q., Niu B. (2020) Towards compression-resistant privacy-preserving photo sharing on social networks. In: the 21st ACM International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing, pp 81&#8211;90"/>
    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Circuits Syst Video Technol; citation_title=Feature consistency training with JPEG compressed images; citation_author=S Wan, T Wu, H Hsu, WH Wong, C Lee; citation_volume=30; citation_issue=12; citation_publication_date=2020; citation_pages=4769-4780; citation_doi=10.1109/TCSVT.2019.2959815; citation_id=CR2"/>
    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Consum Electron; citation_title=The JPEG still picture compression standard; citation_author=GK Wallace; citation_volume=38; citation_issue=1; citation_publication_date=1992; citation_pages=30-44; citation_doi=10.1109/30.125072; citation_id=CR3"/>
    <meta name="citation_reference" content="citation_journal_title=IEEE Signal Process Mag; citation_title=The JPEG 2000 still image compression standard; citation_author=A Skodras, CA Christopoulos, T Ebrahimi; citation_volume=18; citation_issue=5; citation_publication_date=2001; citation_pages=36-58; citation_doi=10.1109/79.952804; citation_id=CR4"/>
    <meta name="citation_reference" content="citation_journal_title=Signal Processing: Image Communication; citation_title=Objective assessment of the WebP image coding algorithm; citation_author=G Ginesu, M Pintus, DD Giusto; citation_volume=27; citation_issue=8; citation_publication_date=2012; citation_pages=867-874; citation_id=CR5"/>
    <meta name="citation_reference" content="citation_journal_title=IEEE Access; citation_title=BadNets: Evaluating backdooring attacks on deep neural networks; citation_author=T Gu, K Liu, B Dolan-Gavitt, S Garg; citation_volume=7; citation_publication_date=2019; citation_pages=47230-47244; citation_doi=10.1109/ACCESS.2019.2909068; citation_id=CR6"/>
    <meta name="citation_reference" content="Chen X., Liu C., Li B., Lu K., Song D. (2017) Targeted backdoor attacks on deep learning systems using data poisoning. arXiv:
                1712.05526
                
              , 1&#8211;18"/>
    <meta name="citation_reference" content="Liu Y., Ma S., Aafer Y., Lee W., Zhai J., Wang W., Zhang X. (2018) Trojaning attack on neural networks. In: 25th Annual Network and Distributed System Security Symposium, pp 1&#8211;15"/>
    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Dependable Secure Comput; citation_title=Invisible backdoor attacks on deep neural networks via steganography and regularization; citation_author=S Li, M Xue, BZH Zhao, H Zhu, X Zhang; citation_volume=18; citation_issue=5; citation_publication_date=2021; citation_pages=2088-2105; citation_id=CR9"/>
    <meta name="citation_reference" content="citation_journal_title=IEEE Transactions on Image Processin; citation_title=Poison ink: Robust and invisible backdoor attack; citation_author=J Zhang, D Chen, Q Huang, J Liao, W Zhang, H Feng, G Hua, N Yu; citation_volume=31; citation_publication_date=2022; citation_pages=5691-5705; citation_doi=10.1109/TIP.2022.3201472; citation_id=CR10"/>
    <meta name="citation_reference" content="Zhong H., Liao C., Squicciarini A.C., Zhu S., Miller D.J. (2020) Backdoor embedding in convolutional neural network models via invisible perturbation. In: 10th ACM Conference on Data and Application Security and Privacy, pp 97&#8211;108"/>
    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Dependable Secure Comput; citation_title=One-to-N &amp; N-to-One: Two advanced backdoor attacks against deep learning models; citation_author=M Xue, C He, J Wang, W Liu; citation_volume=19; citation_issue=3; citation_publication_date=2022; citation_pages=1562-1578; citation_doi=10.1109/TDSC.2020.3028448; citation_id=CR12"/>
    <meta name="citation_reference" content="Salem A., Wen R., Backes M., Ma S., Zhang Y. (2020) Dynamic backdoor attacks against machine learning models. arXiv:
                2003.03675
                
              , 1&#8211;18"/>
    <meta name="citation_reference" content="citation_journal_title=Computer &amp; Security.; citation_title=PTB: Robust physical backdoor attacks against deep neural networks in real world; citation_author=M Xue, C He, Y Wu, S Sun, Y Zhang, J Wang, W Liu; citation_volume=118; citation_issue=102726; citation_publication_date=2022; citation_pages=1-15; citation_id=CR14"/>
    <meta name="citation_reference" content="Shin R., Song D. (2017) JPEG-resistant adversarial images. In: NIPS Workshop on Machine Learning and Computer Security, vol 1, pp 1&#8211;6"/>
    <meta name="citation_reference" content="Cao S., Zou Q., Mao X., Ye D., Wang Z. (2021) Metric learning for anti-compression facial forgery detection. In: ACM Multimedia Conference, pp 1929&#8211;1937"/>
    <meta name="citation_reference" content="Cheng S., Liu Y., Ma S., Zhang X. (2021) Deep feature space Trojan attack of neural networks by controlled detoxification. In: 35th AAAI Conference on Artificial Intelligence, pp 1148&#8211;1156"/>
    <meta name="citation_reference" content="citation_journal_title=IEEE J Sel Areas Commun; citation_title=Defense-resistant backdoor attacks against deep neural networks in outsourced cloud environment; citation_author=X Gong, Y Chen, Q Wang, H Huang, L Meng, C Shen, Q Zhang; citation_volume=39; citation_issue=8; citation_publication_date=2021; citation_pages=2617-2631; citation_doi=10.1109/JSAC.2021.3087237; citation_id=CR18"/>
    <meta name="citation_reference" content="Nguyen T.A., Tran A.T. (2021) WaNet - Imperceptible warping-based backdoor attack. In: 9th International Conference on Learning Representations, pp 1&#8211;16"/>
    <meta name="citation_reference" content="citation_journal_title=Nature; citation_title=Learning representations by back-propagating errors; citation_author=DE Rumelhart, GE Hinton, RJ Williams; citation_volume=323; citation_issue=6088; citation_publication_date=1986; citation_pages=533-536; citation_doi=10.1038/323533a0; citation_id=CR20"/>
    <meta name="citation_reference" content="Xue M., He C., Sun S., Wang J., Liu W. (2021) Robust backdoor attacks against deep neural networks in real physical world. In: 20th IEEE International Conference on Trust, Security and Privacy in Computing and Communications, pp 620&#8211;626"/>
    <meta name="citation_reference" content="Zhang J., Gu Z., Jang J., Wu H., Stoecklin M.P., Huang H., Molloy I.M. (2018) Protecting intellectual property of deep neural networks with watermarking. In: Proceedings of the Asia Conference on Computer and Communications Security , pp 159&#8211;172"/>
    <meta name="citation_reference" content="Krizhevsky A. (2009) Learning multiple layers of features from tiny images. Technical report, University of Toronto"/>
    <meta name="citation_reference" content="Parkhi O.M., Vedaldi A., Zisserman A. (2015) Deep face recognition. In: Proceedings of the British Machine Vision Conference, pp 1&#8211;12"/>
    <meta name="citation_reference" content="citation_journal_title=Commun ACM; citation_title=Imagenet classification with deep convolutional neural networks; citation_author=A Krizhevsky, I Sutskever, GE Hinton; citation_volume=60; citation_issue=6; citation_publication_date=2017; citation_pages=84-90; citation_doi=10.1145/3065386; citation_id=CR25"/>
    <meta name="citation_reference" content="He K., Zhang X., Ren S., Sun J. (2016) Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp 770&#8211;778"/>
    <meta name="citation_reference" content="Simonyan K., Zisserman A. (2015) Very deep convolutional networks for large-scale image recognition. In: 3rd International Conference on Learning Representations, pp 1&#8211;14"/>
    <meta name="citation_reference" content="citation_title=Deep Learning; citation_publication_date=2016; citation_id=CR28; citation_author=I Goodfellow; citation_author=Y Bengio; citation_author=A Courville; citation_publisher=MIT press"/>
    <meta name="citation_reference" content="Wenger E., Passananti J., Bhagoji A.N., Yao Y., Zheng H., Zhao B.Y. (2021) Backdoor attacks against deep learning systems in the physical world. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 6206&#8211;6215"/>
    <meta name="citation_reference" content="Chen Y., Mukherjee D., Han J., Grange A., Xu Y., Liu Z., Parker S., Chen C., Su H., Joshi U., Chiang C., Wang Y., Wilkins P., Bankoski J., Trudeau L.N., Egge N.E., Valin J., Davies T., Midtskogen S., Norkin A., Rivaz P.D. (2018) An overview of core coding tools in the AV1 video codec. In: Picture Coding Symposium, pp 41&#8211;45"/>
    <meta name="citation_reference" content="Bellard F. (2022) BPG Image format. 
                https://bellard.org/bpg
                
              "/>
    <meta name="citation_reference" content="Zhang T. (2004) Solving large scale linear prediction problems using stochastic gradient descent algorithms. In: International Conference on Machine Learning, pp 1&#8211;8"/>
    <meta name="citation_reference" content="Tram&#232;r F., Kurakin A., Papernot N., Goodfellow I.J., Boneh D., McDaniel P.D. (2018) Ensemble adversarial training: Attacks and defenses. In: 6th International Conference on Learning Representations, pp 1&#8211;20"/>
    <meta name="citation_reference" content="Gao Y., Wu D., Zhang J., Gan G., Xia S., Niu G., Sugiyama M. (2022) On the effectiveness of adversarial training against backdoor attacks. arXiv:
                2202.10627
                
              , 1&#8211;12"/>
    <meta name="citation_reference" content="Geiping J., Fowl L., Somepalli G., Goldblum M., Moeller M., Goldstein T. (2021) What doesn&#8217;t kill you makes you robust(er): Adversarial training against poisons and backdoors. arXiv:
                2102.13624
                
              , 1&#8211;25"/>
    <meta name="citation_reference" content="Sarkar E., Benkraouda H., Maniatakos M. (2020) FaceHack: Triggering backdoored facial recognition systems using facial characteristics. arXiv:
                2006.11623
                
              , 1&#8211;13"/>
    <meta name="citation_author" content="Xue, Mingfu"/>
    <meta name="citation_author_email" content="mingfu.xue@nuaa.edu.cn"/>
    <meta name="citation_author_institution" content="College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China"/>
    <meta name="citation_author" content="Wang, Xin"/>
    <meta name="citation_author_email" content="wang.xin@nuaa.edu.cn"/>
    <meta name="citation_author_institution" content="College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China"/>
    <meta name="citation_author" content="Sun, Shichang"/>
    <meta name="citation_author_email" content="sunshichang@nuaa.edu.cn"/>
    <meta name="citation_author_institution" content="College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China"/>
    <meta name="citation_author" content="Zhang, Yushu"/>
    <meta name="citation_author_email" content="yushu@nuaa.edu.cn"/>
    <meta name="citation_author_institution" content="College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China"/>
    <meta name="citation_author" content="Wang, Jian"/>
    <meta name="citation_author_email" content="wangjian@nuaa.edu.cn"/>
    <meta name="citation_author_institution" content="College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China"/>
    <meta name="citation_author" content="Liu, Weiqiang"/>
    <meta name="citation_author_email" content="liuweiqiang@nuaa.edu.cn"/>
    <meta name="citation_author_institution" content="College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China"/>
    <meta name="format-detection" content="telephone=no"/>
    

    
    
    <meta property="og:url" content="https://link.springer.com/article/10.1007/s10489-023-04575-8"/>
    <meta property="og:type" content="article"/>
    <meta property="og:site_name" content="SpringerLink"/>
    <meta property="og:title" content="Compression-resistant backdoor attack against deep neural networks - Applied Intelligence"/>
    <meta property="og:description" content="In recent years, a number of backdoor attacks against deep neural networks (DNN) have been proposed. In this paper, we reveal that backdoor attacks are vulnerable to image compressions, as backdoor instances used to trigger backdoor attacks are usually compressed by image compression methods during data transmission. When backdoor instances are compressed, the feature of backdoor trigger will be destroyed, which could result in significant performance degradation for backdoor attacks. As a countermeasure, we propose the first compression-resistant backdoor attack method based on feature consistency training. Specifically, both backdoor images and their compressed versions are used for training, and the feature difference between backdoor images and their compressed versions are minimized through feature consistency training. As a result, the DNN treats the feature of compressed images as the feature of backdoor images in feature space. After training, the backdoor attack will be robust to image compressions. Furthermore, we consider three different image compressions (i.e., JPEG, JPEG2000, WEBP) during the feature consistency training, so that the backdoor attack can be robust to multiple image compression algorithms. Experimental results demonstrate that when the backdoor instances are compressed, the attack success rate of common backdoor attack is 6.63% (JPEG), 6.20% (JPEG2000) and 3.97% (WEBP) respectively, while the attack success rate of the proposed compression-resistant backdoor attack is 98.77% (JPEG), 97.69% (JPEG2000), and 98.93% (WEBP) respectively. The compression-resistant attack is robust under various parameters settings. In addition, extensive experiments have demonstrated that even if only one image compression method is used in the feature consistency training process, the proposed compression-resistant backdoor attack has the generalization ability to resist multiple unseen image compression methods."/>
    <meta property="og:image" content="https://media.springernature.com/w200/springer-static/cover/journal/10489.jpg"/>
    


    <title>Compression-resistant backdoor attack against deep neural networks | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    
    
        <style>@media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) { html{text-size-adjust:100%;-webkit-font-smoothing:subpixel-antialiased;box-sizing:border-box;color:#333;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:100%;height:100%;line-height:1.61803;overflow-y:scroll}body,img{max-width:100%}body{background:#fcfcfc;font-size:1.125rem;line-height:1.5;min-height:100%}main{display:block}h1{font-family:Georgia,Palatino,serif;font-size:2.25rem;font-style:normal;font-weight:400;line-height:1.4}a{background-color:transparent;color:#004b83;overflow-wrap:break-word;text-decoration:underline;text-decoration-skip-ink:auto;word-break:break-word}b{font-weight:bolder}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}img{border:0;height:auto;vertical-align:middle}button,input{font-family:inherit;font-size:100%}input{line-height:1.15}button,input{overflow:visible}button{text-transform:none}[type=button],[type=submit],button{-webkit-appearance:button}[hidden]{display:none}button{border-radius:0;cursor:pointer;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;line-height:inherit}*{margin:0}h2{font-size:1.75rem}h2,h3{font-family:Georgia,Palatino,serif;font-weight:400;line-height:1.4}h3{font-size:1.5rem}h2,h3{font-style:normal}label{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}*{box-sizing:inherit}body,button,div,form,input,p{margin:0;padding:0}h1,h2,h3{margin:0}h2+*{margin-block-start:1rem}h1+*{margin-block-start:3rem}[style*="display: none"]:first-child+*{margin-block-start:0}p{overflow-wrap:break-word;word-break:break-word}.c-ad{text-align:center}@media only screen and (min-width:320px){.c-ad{padding:8px}}.c-ad--728x90{background-color:#ccc;display:none}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 94px)}.c-ad--728x90 iframe{height:90px;max-width:970px}@media only screen and (min-width:768px){.js .c-ad--728x90{display:none}.js .u-show-following-ad+.c-ad--728x90{display:block}}.c-ad iframe{border:0;overflow:auto;vertical-align:top}.c-ad__label{color:#333;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;font-weight:400;line-height:1.5;margin-bottom:4px}.c-breadcrumbs>li{display:inline}.c-card{background-color:transparent;border:0;box-shadow:none;flex-direction:column;font-size:14px;min-width:0;padding:0}.c-card,.c-card__image{display:flex;overflow:hidden;position:relative}.c-card__image{justify-content:center;padding-bottom:56.25%}@supports (aspect-ratio:1/1){.c-card__image{padding-bottom:0}}.c-card__image img{left:0;min-height:100%;min-width:100%;position:absolute}@supports ((-o-object-fit:cover) or (object-fit:cover)){.c-card__image img{height:100%;object-fit:cover;width:100%}}.c-card__body{flex:1 1 auto;padding:16px}.c-card__link:not(.c-card__link--no-block-link):before{bottom:0;content:"";left:0;position:absolute;right:0;top:0}.c-card--flush .c-card__body{padding:0}.c-skip-link{background:#f7fbfe;bottom:auto;color:#004b83;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;padding:8px;position:absolute;text-align:center;transform:translateY(-100%);z-index:9999}@media (prefers-reduced-motion:reduce){.c-skip-link{transition:top .3s ease-in-out 0s}}@media print{.c-skip-link{display:none}}.c-skip-link:link{color:#004b83}.c-status-message{align-items:center;box-sizing:border-box;display:flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;position:relative;width:100%}.c-status-message :last-child{margin-bottom:0}.c-status-message--boxed{background-color:#fff;border:1px solid #ccc;border-radius:2px;line-height:1.4;padding:16px}.c-status-message__icon{fill:currentcolor;display:inline-block;flex:0 0 auto;height:1.5em;margin-right:8px;transform:translate(0);vertical-align:text-top;width:1.5em}.c-status-message--info .c-status-message__icon{color:#003f8d}.c-status-message--boxed.c-status-message--info{border-bottom:4px solid #003f8d}.c-status-message--success .c-status-message__icon{color:#00b8b0}.c-pagination{align-items:center;display:flex;flex-wrap:wrap;font-size:.875rem;list-style:none;margin:0;padding:16px}@media only screen and (min-width:540px){.c-pagination{justify-content:center}}.c-pagination__item{margin-bottom:8px;margin-right:16px}.c-pagination__item:last-child{margin-right:0}.c-pagination__link{align-items:center;background-color:#f2f2f2;background-image:linear-gradient(#fff,#f2f2f2);border:1px solid #ccc;border-radius:2px;color:#004b83;cursor:pointer;display:inline-flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;min-width:30px;padding:8px;position:relative;text-align:center;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.c-pagination__link svg,.c-pagination__link--disabled svg{fill:currentcolor}.c-pagination__link:visited{color:#004b83}.c-pagination__link:focus,.c-pagination__link:hover{border:1px solid #666;text-decoration:none}.c-pagination__link:focus,.c-pagination__link:hover{background-color:#666;background-image:none;color:#fff}.c-pagination__link:focus svg path,.c-pagination__link:hover svg path{fill:#fff}.c-pagination__link--disabled{align-items:center;background-color:transparent;background-image:none;border-radius:2px;color:#333;cursor:default;display:inline-flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;opacity:.67;padding:8px;position:relative;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.c-pagination__link--disabled:visited{color:#333}.c-pagination__link--disabled,.c-pagination__link--disabled:focus,.c-pagination__link--disabled:hover{border:1px solid #ccc;text-decoration:none}.c-pagination__link--disabled:focus,.c-pagination__link--disabled:hover{background-color:transparent;background-image:none;color:#333}.c-pagination__link--disabled:focus svg path,.c-pagination__link--disabled:hover svg path{fill:#333}.c-pagination__link--active{background-color:#666;background-image:none;border-color:#666;color:#fff;cursor:default}.c-pagination__ellipsis{background:0 0;border:0;min-width:auto;padding-left:0;padding-right:0}.c-pagination__icon{fill:#999;height:12px;width:16px}.c-pagination__icon--active{fill:#004b83}.c-breadcrumbs{color:#000;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;list-style:none;margin:0;padding:0}.c-breadcrumbs__link{color:#666}svg.c-breadcrumbs__chevron{fill:#666;height:10px;margin:4px 4px 0;width:10px}.c-header{background-color:#fff;border-bottom:4px solid #00285a;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.125rem;padding:16px 0}.c-header__container,.c-header__menu{align-items:center;display:flex;flex-wrap:wrap}@supports (gap:2em){.c-header__container,.c-header__menu{gap:2em 2em}}.c-header__menu{list-style:none;margin:0;padding:0}.c-header__item{color:inherit}@supports not (gap:2em){.c-header__item{margin-left:24px}}.c-header__container{justify-content:space-between;margin:0 auto;max-width:1280px;padding:0 16px}@supports not (gap:2em){.c-header__brand{margin-right:32px}}.c-header__brand a{display:block;text-decoration:none}.c-header__link{color:inherit}.c-popup-search{background-color:#eee;box-shadow:0 3px 3px -3px rgba(0,0,0,.21);padding:16px 0;position:relative;z-index:10}@media only screen and (min-width:1024px){.js .c-popup-search{position:absolute;top:100%;width:100%}.c-popup-search__container{margin:auto;max-width:70%}}.c-article-header{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;margin-bottom:40px}.c-article-identifiers{color:#6f6f6f;display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3;list-style:none;margin:0 0 8px;padding:0}.c-article-identifiers__item{border-right:1px solid #6f6f6f;list-style:none;margin-right:8px;padding-right:8px}.c-article-identifiers__item:last-child{border-right:0;margin-right:0;padding-right:0}.c-article-title{font-size:1.5rem;line-height:1.25;margin-bottom:16px}@media only screen and (min-width:768px){.c-article-title{font-size:1.875rem;line-height:1.2}}.c-article-author-list{display:inline;font-size:1rem;list-style:none;margin:0 8px 0 0;padding:0;width:100%}.c-article-author-list__item{display:inline;padding-right:0}.c-article-author-list svg{margin-left:4px}.c-article-author-list__show-more{display:none;margin-right:4px}.c-article-author-list__button,.js .c-article-author-list__item--hide,.js .c-article-author-list__show-more{display:none}.js .c-article-author-list--long .c-article-author-list__show-more,.js .c-article-author-list--long+.c-article-author-list__button{display:inline}@media only screen and (max-width:539px){.js .c-article-author-list__item--hide-small-screen{display:none}.js .c-article-author-list--short .c-article-author-list__show-more,.js .c-article-author-list--short+.c-article-author-list__button{display:inline}}#uptodate-client,.js .c-article-author-list--expanded .c-article-author-list__show-more{display:none!important}.js .c-article-author-list--expanded .c-article-author-list__item--hide-small-screen{display:inline!important}.c-article-author-list__button,.c-button-author-list{background:#ebf1f5;border:4px solid #ebf1f5;border-radius:20px;color:#666;font-size:.875rem;line-height:1.4;padding:2px 11px 2px 8px;text-decoration:none}.c-article-author-list__button svg,.c-button-author-list svg{margin:1px 4px 0 0}.c-article-author-list__button:hover,.c-button-author-list:hover{background:#069;border-color:transparent;color:#fff}.c-article-info-details{font-size:1rem;margin-bottom:8px;margin-top:16px}.c-article-info-details__cite-as{border-left:1px solid #6f6f6f;margin-left:8px;padding-left:8px}.c-article-metrics-bar{display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3}.c-article-metrics-bar__wrapper{margin:0 0 16px}.c-article-metrics-bar__item{align-items:baseline;border-right:1px solid #6f6f6f;margin-right:8px}.c-article-metrics-bar__item:last-child{border-right:0}.c-article-metrics-bar__count{font-weight:700;margin:0}.c-article-metrics-bar__label{color:#626262;font-style:normal;font-weight:400;margin:0 10px 0 5px}.c-article-metrics-bar__details{margin:0}.c-article-main-column{font-family:Georgia,Palatino,serif;margin-right:8.6%;width:60.2%}@media only screen and (max-width:1023px){.c-article-main-column{margin-right:0;width:100%}}.c-article-extras{float:left;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;width:31.2%}@media only screen and (max-width:1023px){.c-article-extras{display:none}}.c-article-section__title{border-bottom:2px solid #d5d5d5;font-size:1.25rem;margin:0;padding-bottom:8px}@media only screen and (min-width:768px){.c-article-section__title{font-size:1.5rem;line-height:1.24}}.c-article-section{clear:both}.c-article-section__content{margin-bottom:40px;margin-top:0;padding-top:8px}@media only screen and (max-width:1023px){.c-article-section__content{padding-left:0}}.c-article-authors-search{margin-bottom:24px;margin-top:0}.c-article-authors-search__item,.c-article-authors-search__title{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-article-authors-search__title{color:#626262;font-size:1.05rem;font-weight:700;margin:0;padding:0}.c-article-authors-search__item{font-size:1rem}.c-article-authors-search__text{margin:0}.c-code-block{border:1px solid #f2f2f2;font-family:monospace;margin:0 0 24px;padding:20px}.c-code-block__heading{font-weight:400;margin-bottom:16px}.c-code-block__line{display:block;overflow-wrap:break-word;white-space:pre-wrap}.c-article-share-box__no-sharelink-info{font-size:.813rem;font-weight:700;margin-bottom:24px;padding-top:4px}.c-article-share-box__only-read-input{border:1px solid #d5d5d5;box-sizing:content-box;display:inline-block;font-size:.875rem;font-weight:700;height:24px;margin-bottom:8px;padding:8px 10px}.c-article-share-box__button--link-like{background-color:transparent;border:0;color:#069;cursor:pointer;font-size:.875rem;margin-bottom:8px;margin-left:10px}.c-article-associated-content__container .c-article-associated-content__collection-label{line-height:1.4}.c-article-associated-content__container .c-article-associated-content__collection-title{line-height:1.3}.c-context-bar{box-shadow:0 0 10px 0 rgba(51,51,51,.2);position:relative;width:100%}.c-context-bar__title{display:none}.c-breadcrumbs--truncated .c-breadcrumbs__link{display:inline-block;max-width:45%;overflow:hidden;text-overflow:ellipsis;vertical-align:bottom;white-space:nowrap}.c-reading-companion{clear:both;min-height:389px}.c-reading-companion__sticky{max-width:389px}.c-reading-companion__scroll-pane{margin:0;min-height:200px;overflow:hidden auto}.c-reading-companion__tabs{display:flex;flex-flow:row nowrap;font-size:1rem;list-style:none;margin:0 0 8px;padding:0}.c-reading-companion__tabs>li{flex-grow:1}.c-reading-companion__tab{background-color:#eee;border:1px solid #d5d5d5;border-image:initial;border-left-width:0;color:#069;font-size:1rem;padding:8px 8px 8px 15px;text-align:left;width:100%}.c-reading-companion__tabs li:first-child .c-reading-companion__tab{border-left-width:1px}.c-reading-companion__tab--active{background-color:#fcfcfc;border-bottom:1px solid #fcfcfc;color:#222;font-weight:700}.c-reading-companion__sections-list{list-style:none;padding:0}.c-reading-companion__figures-list,.c-reading-companion__references-list{list-style:none;min-height:389px;padding:0}.c-reading-companion__references-list--numeric{list-style:decimal inside}.c-reading-companion__sections-list{margin:0 0 8px;min-height:50px}.c-reading-companion__section-item{font-size:1rem;padding:0}.c-reading-companion__section-item a{display:block;line-height:1.5;overflow:hidden;padding:8px 0 8px 16px;text-overflow:ellipsis;white-space:nowrap}.c-reading-companion__figure-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:16px 8px 16px 0}.c-reading-companion__figure-item:first-child{border-top:none;padding-top:8px}.c-reading-companion__reference-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:8px 8px 8px 16px}.c-reading-companion__reference-item:first-child{border-top:none}.c-reading-companion__reference-item a{word-break:break-word}.c-reading-companion__reference-citation{display:inline}.c-reading-companion__reference-links{font-size:.813rem;font-weight:700;list-style:none;margin:8px 0 0;padding:0;text-align:right}.c-reading-companion__reference-links>a{display:inline-block;padding-left:8px}.c-reading-companion__reference-links>a:first-child{display:inline-block;padding-left:0}.c-reading-companion__figure-title{display:block;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.25rem;font-style:normal;font-weight:700;line-height:1.4;margin:0 0 8px}.c-reading-companion__figure-links{display:flex;justify-content:space-between;margin:8px 0 0}.c-reading-companion__figure-links>a{align-items:center;display:flex}.c-reading-companion__figure-full-link svg{height:.8em;margin-left:2px}.c-reading-companion__panel{border-top:none;display:none;margin-top:0;padding-top:0}.c-reading-companion__panel--active{display:block}.c-article-section__figure-description{font-size:1rem}.c-article-section__figure-description>*{margin-bottom:0}.c-cod{display:block;font-size:1rem;width:100%}.c-cod__form{background:#ebf0f3}.c-cod__prompt{font-size:1.125rem;line-height:1.3;margin:0 0 24px}.c-cod__label{display:block;margin:0 0 4px}.c-cod__row{display:flex;margin:0 0 16px}.c-cod__row:last-child{margin:0}.c-cod__input{border:1px solid #d5d5d5;border-radius:2px;flex-basis:75%;flex-shrink:0;margin:0;padding:13px}.c-cod__input--submit{color:#fff;flex-shrink:1;margin-left:8px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-cod__input--submit-single{flex-basis:100%;flex-shrink:0;margin:0}.c-cod__input--submit:focus,.c-cod__input--submit:hover{background-color:#fff}.c-pdf-download__link .u-icon{padding-top:2px}.save-data .c-article-author-institutional-author__sub-division,.save-data .c-article-equation__number,.save-data .c-article-figure-description,.save-data .c-article-fullwidth-content,.save-data .c-article-main-column,.save-data .c-article-satellite-article-link,.save-data .c-article-satellite-subtitle,.save-data .c-article-table-container,.save-data .c-blockquote__body,.save-data .c-code-block__heading,.save-data .c-reading-companion__figure-title,.save-data .c-reading-companion__reference-citation,.save-data .c-site-messages--nature-briefing-email-variant .serif,.save-data .c-site-messages--nature-briefing-email-variant.serif,.save-data .serif,.save-data .u-serif,.save-data h1,.save-data h2,.save-data h3{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-pdf-download{display:flex;margin-bottom:16px;max-height:48px}@media only screen and (min-width:540px){.c-pdf-download{max-height:none}}@media only screen and (min-width:1024px){.c-pdf-download{max-height:48px}}.c-pdf-download__link{display:flex;flex:1 1 0%;padding:13px 24px!important}.c-pdf-download__text{padding-right:4px}@media only screen and (max-width:539px){.c-pdf-download__text{text-transform:capitalize}}@media only screen and (min-width:540px){.c-pdf-download__text{padding-right:8px}}.c-pdf-container{display:flex;justify-content:flex-end}@media only screen and (max-width:539px){.c-pdf-container .c-pdf-download{display:flex;flex-basis:100%}}.c-article-extras .c-pdf-container{flex-wrap:wrap;width:100%}.c-article-extras .c-pdf-container .c-pdf-download{width:100%}.c-status-message--success{border-bottom:2px solid #00b8b0;margin-bottom:16px;padding-bottom:16px}.c-recommendations-header{border-bottom:1px solid #d5d5d5}.c-recommendations-title{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.125rem;font-weight:700;line-height:1.24;margin:0;padding-bottom:16px}.c-recommendations-close{background-color:transparent;border:0;cursor:pointer;height:2em;margin-right:-10px;margin-top:-5px;width:2em}.c-recommendations-authors{line-height:1.24;margin-bottom:0}.c-recommendations-list-container{margin-top:0;position:relative}.c-recommendations-list{display:flex;flex-wrap:nowrap;justify-content:space-between;margin:0 auto;overflow-x:hidden;padding:16px 0;scroll-behavior:smooth;scroll-snap-type:x mandatory;width:calc(100% - 146px)}@media only screen and (max-width:539px){.c-recommendations-list{display:block;height:40vh;overflow-y:auto;width:100%}}.c-recommendations-list__item{display:flex;flex:0 0 calc(33.3333% - 16px);margin:0 8px;scroll-snap-align:center}@media only screen and (max-width:539px){.c-recommendations-list__item{margin:0;padding:0 0 16px}}.c-recommendations-list__item .c-card__image{align-items:baseline;flex:1 1 40%;margin:0 16px 0 0;max-width:150px}.c-recommendations-list__item .c-card__image img{border:1px solid #d5d5d5;height:auto;min-height:0;position:relative;transform:translateY(0)}@media only screen and (max-width:1023px){.c-recommendations-list__item .c-card__image{display:none}}.c-card__layout{display:flex;flex:1 1 auto;justify-content:space-between}.c-card__title-recommendation{-webkit-box-orient:vertical;-webkit-line-clamp:4;display:-webkit-box;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.25rem;font-style:normal;font-weight:700;line-height:1.4rem;margin-bottom:0;max-height:5.6em;overflow:hidden!important;text-overflow:ellipsis}.c-card__title-recommendation .c-card__link{color:#004b83;font-size:1.125rem;text-decoration:none}@media only screen and (max-width:539px){.c-recommendations-column-switch{display:flex;flex-direction:column-reverse}}.js-greyout-page-background{background-color:rgba(34,34,34,.75);bottom:0;left:0;position:fixed;right:0;top:0}.app-search__content{display:flex}.app-search__label{color:#666;display:inline-block;font-size:.875rem;margin-bottom:8px}.app-search__input{border:1px solid #b3b3b3;border-bottom-left-radius:3px;border-top-left-radius:3px;box-shadow:inset 0 1px 3px 0 rgba(0,0,0,.21);flex:0 1 auto;font-size:.875rem;line-height:1.2;padding:.75em 1em;vertical-align:middle;width:100%}.app-search__button{align-items:center;background-color:#33629d;background-image:linear-gradient(#4d76a9,#33629d);border:1px solid rgba(0,59,132,.5);border-radius:0 2px 2px 0;color:#fff;cursor:pointer;display:inline-flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;padding:8px;position:relative;text-align:center;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:50px}.app-search__button svg,.u-button svg,.u-button--primary svg{fill:currentcolor}.app-checklist-banner{border:2px solid #ebf1f5;display:flex;flex:1 1 auto;font-size:1rem;justify-content:space-between;margin-bottom:16px;padding:16px}.app-checklist-banner--on-mobile{display:block;margin-bottom:32px}@media only screen and (min-width:1024px){.app-checklist-banner--on-mobile{display:none}}.app-checklist-banner__title{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.125rem;font-weight:700;margin-bottom:0}.app-checklist-banner__icon-container{align-items:center;display:flex;flex:0 0 60px;justify-content:flex-end;width:60px}.app-checklist-banner__link{align-items:center;color:#004b83;display:inline-flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.app-checklist-banner__arrow-icon,.app-checklist-banner__paper-icon{fill:currentcolor;display:inline-block;transform:translate(0);vertical-align:text-top}.app-checklist-banner__paper-icon{height:36px!important;width:36px!important}.app-checklist-banner__arrow-icon{height:11px;margin:4px 0 0 8px;width:16px}.u-button{align-items:center;background-color:#f2f2f2;background-image:linear-gradient(#fff,#f2f2f2);border:1px solid #ccc;border-radius:2px;color:#004b83;cursor:pointer;display:inline-flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;padding:8px;position:relative;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.u-button--primary{background-color:#33629d;background-image:linear-gradient(#4d76a9,#33629d);border:1px solid rgba(0,59,132,.5);color:#fff}.u-button--full-width{display:flex;width:100%}.u-clearfix:after,.u-clearfix:before{content:"";display:table}.u-clearfix:after{clear:both}.u-display-block{display:block}.u-display-flex{display:flex;width:100%}.u-flex-direction-column{flex-direction:column}.u-align-items-center{align-items:center}.u-justify-content-space-between{justify-content:space-between}.u-flex-static{flex:0 0 auto}.u-display-none{display:none}.js .u-js-hide{display:none;visibility:hidden}@media print{.u-hide-print{display:none}}.u-icon{fill:currentcolor;display:inline-block;height:1em;transform:translate(0);vertical-align:text-top;width:1em}.u-list-reset{list-style:none;margin:0;padding:0}.u-button-reset{background-color:transparent;border:0;padding:0}.u-sans-serif{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.u-mbs-0{margin-block-start:0!important}.u-container{margin:0 auto;max-width:1280px;padding:0 16px}.u-position-relative{position:relative}.u-mt-0{margin-top:0}.u-mt-32{margin-top:32px}.u-mr-24{margin-right:24px}.u-mb-0{margin-bottom:0}.u-mb-8{margin-bottom:8px}.u-mb-16{margin-bottom:16px}.u-mb-24{margin-bottom:24px}.u-mb-32{margin-bottom:32px}.u-ml-8{margin-left:8px}.u-float-left{float:left}.u-hide{display:none;visibility:hidden}.u-hide:first-child+*{margin-block-start:0}@media only screen and (min-width:540px){.u-hide-at-sm{display:none;visibility:hidden}}@media only screen and (min-width:1024px){.u-hide-at-lg{display:none;visibility:hidden}}.u-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}.u-text-sm{font-size:1rem}.u-h4{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.25rem;font-style:normal;font-weight:700;line-height:1.4}.hide{display:none;visibility:hidden}.visually-hidden{clip:rect(1px,1px,1px,1px);height:1px;position:absolute!important;width:1px}.c-article-section__figure-description{font-family:Georgia,Palatino,serif}.c-article-section__content p{line-height:1.8}.c-pagination__input{border:1px solid #bfbfbf;border-radius:2px;box-shadow:inset 0 2px 6px 0 rgba(51,51,51,.2);box-sizing:initial;display:inline-block;height:28px;margin:0;max-width:64px;min-width:16px;padding:0 8px;text-align:center;transition:width .15s ease 0s}.c-pagination__input::-webkit-inner-spin-button,.c-pagination__input::-webkit-outer-spin-button{-webkit-appearance:none;margin:0}@media only screen and (min-width:1024px){.c-article-collection__container{display:none}}.c-article-associated-content__container .c-article-associated-content__collection-label{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.063rem}.c-article-associated-content__container .c-article-associated-content__collection-title{font-size:1.063rem;font-weight:400}.c-reading-companion__sections-list{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-article-section__title,.c-article-title{font-weight:400}.c-header__cart-icon{margin-right:12px}.c-header__navigation{display:flex} }</style>



        <link rel="stylesheet" data-test="critical-css-handler" data-inline-css-source="critical-css" href="/oscar-static/app-springerlink/css/enhanced-article-c09d650dc2.css" media="print" onload="this.media='only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)';this.onload=null">
        
    



    
    <script>
        window.dataLayer = [{"GA Key":"UA-26408784-1","DOI":"10.1007/s10489-023-04575-8","Page":"article","springerJournal":true,"page":{"attributes":{"environment":"live"}},"Country":"ES","japan":false,"doi":"10.1007-s10489-023-04575-8","Journal Title":"Applied Intelligence","Journal Id":10489,"Keywords":"Artificial intelligence security, Backdoor attack, Compression resistance, Deep neural networks, Feature consistency training","kwrd":["Artificial_intelligence_security","Backdoor_attack","Compression_resistance","Deep_neural_networks","Feature_consistency_training"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["3991460755","3902463196","1000862991","3991436001"],"businessPartnerIDString":"3991460755|3902463196|1000862991|3991436001"}},"Access Type":"subscription","Bpids":"3991460755, 3902463196, 1000862991, 3991436001","Bpnames":"IK4-Ikerlan IK4-Ikerlan, Universidad del País Vasco, Universidad del Pais Vasco, Springer  National Consortium Spain Springer National Consortium Spain","BPID":["3991460755","3902463196","1000862991","3991436001"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10489-023-04575-8","Full HTML":"Y","Subject Codes":["SCI","SCI21000","SCT17004","SCT22050"],"pmc":["I","I21000","T17004","T22050"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1573-7497","pissn":"0924-669X"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Artificial Intelligence","2":"Mechanical Engineering","3":"Manufacturing, Machines, Tools, Processes"},"secondarySubjectCodes":{"1":"I21000","2":"T17004","3":"T22050"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article"}];
    </script>

    <script>
    window.dataLayer.push({
        ga4MeasurementId: 'G-B3E4QL2TPR',
        ga360TrackingId: 'UA-26408784-1',
        twitterId: 'o47a7',
        ga4ServerUrl: 'https://collect.springer.com',
        imprint: 'springerlink'
    });
</script>

    <script data-test="gtm-head">
    window.initGTM = function() {
        if (window.config.mustardcut) {
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://collect.springer.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-MRVXSHQ');
        }
    }
</script>
    <script>
    (function(w,d,t) {
        function cc() {
            var h = w.location.hostname;
            var e = d.createElement(t),
                    s = d.getElementsByTagName(t)[0];

            if (h.indexOf('springer.com') > -1) {
                e.src = 'https://cmp-static.springer.com/production_live/consent-bundle-17-28.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
            } else {
                e.src = '/static/js/lib/cookie-consent.min.js';
                e.setAttribute('data-consent', h);
            }
            s.insertAdjacentElement('afterend', e);
        }

        cc();
    })(window,document,'script');
</script>

    <script>
    (function(w, d) {
        w.config = w.config || {};
        w.config.mustardcut = false;

        
        if (w.matchMedia && w.matchMedia('only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)').matches) {
            w.config.mustardcut = true;
            d.classList.add('js');
            d.classList.remove('grade-c');
        }
    })(window, document.documentElement);
</script>


    
<script>
    (function () {
        if ( typeof window.CustomEvent === "function" ) return false;
        function CustomEvent ( event, params ) {
            params = params || { bubbles: false, cancelable: false, detail: null };
            var evt = document.createEvent( 'CustomEvent' );
            evt.initCustomEvent( event, params.bubbles, params.cancelable, params.detail );
            return evt;
        }

        CustomEvent.prototype = window.Event.prototype;

        window.CustomEvent = CustomEvent;
    })();
</script>



    <script class="js-entry">
    if (window.config.mustardcut) {
        (function(w, d) {
            
            
            
                window.Component = {};
                window.suppressShareButton = false;
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                {'src': '/oscar-static/js/polyfill-es5-bundle-51eb718839.js', 'async': false},
                {'src': '/oscar-static/js/airbrake-es5-bundle-9ff1caa227.js', 'async': false},
            ];

            var bodyScripts = [
                
                    {'src': '/oscar-static/js/app-es5-bundle-2a336d0756.js', 'async': false, 'module': false},
                    {'src': '/oscar-static/js/app-es6-bundle-1d39264b6c.js', 'async': false, 'module': true}
                
                
                
                    , {'src': '/oscar-static/js/global-article-es5-bundle-4799bd8c8d.js', 'async': false, 'module': false},
                    {'src': '/oscar-static/js/global-article-es6-bundle-199faa8a7e.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i = 0; i < headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i = 0; i < bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        })(window, document);
    }
</script>

    
    
    <link rel="canonical" href="https://link.springer.com/article/10.1007/s10489-023-04575-8"/>
    

    
    <script type="application/ld+json">{"mainEntity":{"headline":"Compression-resistant backdoor attack against deep neural networks","description":"In recent years, a number of backdoor attacks against deep neural networks (DNN) have been proposed. In this paper, we reveal that backdoor attacks are vulnerable to image compressions, as backdoor instances used to trigger backdoor attacks are usually compressed by image compression methods during data transmission. When backdoor instances are compressed, the feature of backdoor trigger will be destroyed, which could result in significant performance degradation for backdoor attacks. As a countermeasure, we propose the first compression-resistant backdoor attack method based on feature consistency training. Specifically, both backdoor images and their compressed versions are used for training, and the feature difference between backdoor images and their compressed versions are minimized through feature consistency training. As a result, the DNN treats the feature of compressed images as the feature of backdoor images in feature space. After training, the backdoor attack will be robust to image compressions. Furthermore, we consider three different image compressions (i.e., JPEG, JPEG2000, WEBP) during the feature consistency training, so that the backdoor attack can be robust to multiple image compression algorithms. Experimental results demonstrate that when the backdoor instances are compressed, the attack success rate of common backdoor attack is 6.63% (JPEG), 6.20% (JPEG2000) and 3.97% (WEBP) respectively, while the attack success rate of the proposed compression-resistant backdoor attack is 98.77% (JPEG), 97.69% (JPEG2000), and 98.93% (WEBP) respectively. The compression-resistant attack is robust under various parameters settings. In addition, extensive experiments have demonstrated that even if only one image compression method is used in the feature consistency training process, the proposed compression-resistant backdoor attack has the generalization ability to resist multiple unseen image compression methods.","datePublished":"2023-04-12","dateModified":"2023-04-12","pageStart":"1","pageEnd":"16","sameAs":"https://doi.org/10.1007/s10489-023-04575-8","keywords":"Artificial Intelligence,Mechanical Engineering,Manufacturing,Machines,Tools,Processes","image":"https://static-content.springer.com/image/art%3A10.1007%2Fs10489-023-04575-8/MediaObjects/10489_2023_4575_Fig1_HTML.png","isPartOf":{"name":"Applied Intelligence","issn":["1573-7497","0924-669X"],"@type":["Periodical"]},"publisher":{"name":"Springer US","logo":{"url":"https://www.springernature.com/app-sn/public/images/logo-springernature.png","@type":"ImageObject"},"@type":"Organization"},"author":[{"name":"Xue, Mingfu","url":"http://orcid.org/0000-0003-2408-503X","affiliation":[{"name":"Nanjing University of Aeronautics and Astronautics","address":{"name":"College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China","@type":"PostalAddress"},"@type":"Organization"}],"email":"mingfu.xue@nuaa.edu.cn","@type":"Person"},{"name":"Wang, Xin","affiliation":[{"name":"Nanjing University of Aeronautics and Astronautics","address":{"name":"College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Sun, Shichang","affiliation":[{"name":"Nanjing University of Aeronautics and Astronautics","address":{"name":"College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Zhang, Yushu","affiliation":[{"name":"Nanjing University of Aeronautics and Astronautics","address":{"name":"College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Wang, Jian","affiliation":[{"name":"Nanjing University of Aeronautics and Astronautics","address":{"name":"College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Liu, Weiqiang","affiliation":[{"name":"Nanjing University of Aeronautics and Astronautics","address":{"name":"College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"}],"isAccessibleForFree":false,"hasPart":{"isAccessibleForFree":false,"cssSelector":".main-content","@type":"WebPageElement"},"@type":"ScholarlyArticle"},"@context":"https://schema.org","@type":"WebPage"}</script>

</head>
<body class="shared-article-renderer">
    
    
    
        
            <!-- Google Tag Manager (noscript) -->
            <noscript data-test="gtm-body">
                <iframe src="https://collect.springer.com/ns.html?id=GTM-MRVXSHQ"
                height="0" width="0" style="display:none;visibility:hidden"></iframe>
            </noscript>
            <!-- End Google Tag Manager (noscript) -->
        
    


    <div class="u-vh-full">
        <a class="c-skip-link" href="#main-content">Skip to main content</a>
        
            <div class="u-hide u-show-following-ad"></div>
            <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
                <div class="c-ad__inner">
                    <p class="c-ad__label">Advertisement</p>
                    <div id="div-gpt-ad-LB1" data-pa11y-ignore data-gpt data-test="LB1-ad"
                         data-gpt-unitpath="/270604982/springerlink/10489/article" data-gpt-sizes="728x90"
                         style="min-width:728px;min-height:90px" data-gpt-targeting="pos=LB1;articleid=s10489-023-04575-8;"></div>
                </div>
            </aside>


<div class="u-position-relative u-mbs-0">
        <header class="c-header u-mb-24" data-test="publisher-header">
    
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-6c9a864b59.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                <span>Search</span>
                <svg class="u-icon u-flex-static u-ml-8" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>

        <div class="c-header__cart-icon">
            <div id="ecommerce-header-cart-icon-link" class="c-header__item ecommerce-cart" style="display:inline-block;margin-right:10px">
 <form action="https://order.springer.com/public/precheckout" method="post">
  <button class="c-header__link" type="submit" style="appearance:none;border:none;background:none;color:inherit;position:relative">
   <svg aria-hidden="true" focusable="false" height="18" viewbox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg" style="vertical-align:text-bottom">
    <path d="m5 14c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm10 0c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm-10 1c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1 1-.4477153 1-1-.44771525-1-1-1zm10 0c-.5522847 0-1 .4477153-1 1s.4477153 1 1 1 1-.4477153 1-1-.4477153-1-1-1zm-12.82032249-15c.47691417 0 .88746157.33678127.98070211.80449199l.23823144 1.19501025 13.36277974.00045554c.5522847.00001882.9999659.44774934.9999659 1.00004222 0 .07084994-.0075361.14150708-.022474.2107727l-1.2908094 5.98534344c-.1007861.46742419-.5432548.80388386-1.0571651.80388386h-10.24805106c-.59173366 0-1.07142857.4477153-1.07142857 1 0 .5128358.41361449.9355072.94647737.9932723l.1249512.0067277h10.35933776c.2749512 0 .4979349.2228539.4979349.4978051 0 .2749417-.2227336.4978951-.4976753.4980063l-10.35959736.0041886c-1.18346732 0-2.14285714-.8954305-2.14285714-2 0-.6625717.34520317-1.24989198.87690425-1.61383592l-1.63768102-8.19004794c-.01312273-.06561364-.01950005-.131011-.0196107-.19547395l-1.71961253-.00064219c-.27614237 0-.5-.22385762-.5-.5 0-.27614237.22385763-.5.5-.5zm14.53193359 2.99950224h-13.11300004l1.20580469 6.02530174c.11024034-.0163252.22327998-.02480398.33844139-.02480398h10.27064786z" fill="#333"></path>
   </svg><span class="u-screenreader-only visually-hidden">Go to cart</span><span class="cart-info" style="display:none;position:absolute;top:-4px;right:-10px;background-color:#C40606;color:#fff;width:18px;height:18px;font-size:11px;border-radius:50%;line-height:17.5px;text-align:center"></span></button>
 </form>
 <script>(function () { var exports = {}; if (window.fetch) {
            
            "use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.headerWidgetClientInit = void 0;
var headerWidgetClientInit = function (getCartInfo) {
    console.log("listen to updatedCart event");
    document.body.addEventListener("updatedCart", function () {
        console.log("updatedCart happened");
        updateCartIcon().then(function () { return console.log("Cart state update upon event"); });
    }, false);
    return updateCartIcon().then(function () { return console.log("Initial cart state update"); });
    function updateCartIcon() {
        return getCartInfo()
            .then(function (res) { return res.json(); })
            .then(refreshCartState)
            .catch(function () { return console.log("Could not fetch cart info"); });
    }
    function refreshCartState(json) {
        var indicator = document.querySelector("#ecommerce-header-cart-icon-link .cart-info");
        /* istanbul ignore else */
        if (indicator && json.itemCount) {
            indicator.style.display = 'block';
            indicator.textContent = json.itemCount > 9 ? '9+' : json.itemCount.toString();
            var moreThanOneItem = json.itemCount > 1;
            indicator.setAttribute('title', "there ".concat(moreThanOneItem ? "are" : "is", " ").concat(json.itemCount, " item").concat(moreThanOneItem ? "s" : "", " in your cart"));
        }
        return json;
    }
};
exports.headerWidgetClientInit = headerWidgetClientInit;

            
            headerWidgetClientInit(
              function () {
                return window.fetch("https://cart.springer.com/cart-info", {
                  credentials: "include",
                  headers: { Accept: "application/json" }
                })
              }
            )
        }})()</script>
</div>
        </div>

        <nav class="u-position-relative">
            <ul class="c-header__menu">
                
        
            <li class="c-header__item">
                <a
                    data-test="login-link"
                    class="c-header__link"
                    href="https://link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10489-023-04575-8"
                    data-track="click"
                    data-track-category="header"
                    data-track-action="login header"
                    data-track-label="link">Log in</a>
            </li>
        

        


            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        
            <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
                <div class="c-popup-search__content">
                    <div class="u-container">
                        <div class="c-popup-search__container" data-test="springerlink-popup-search">
                            <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="u-icon" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                        </div>
                    </div>
                </div>
            </div>
        
    
</div>
        
                
    
        <nav class="u-container" aria-label="breadcrumbs" data-test="article-breadcrumbs">
            <ol class="c-breadcrumbs c-breadcrumbs--truncated" itemscope itemtype="https://schema.org/BreadcrumbList">
                
                    <li class="c-breadcrumbs__item" id="breadcrumb0" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
                        <a href="/" class="c-breadcrumbs__link" itemprop="item" data-track="click" data-track-category="article" data-track-action="breadcrumbs" data-track-label="breadcrumb1"><span itemprop="name">Home</span></a><meta itemprop="position" content="1">
                            <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10" xmlns="http://www.w3.org/2000/svg">
                                <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                            </svg>
                    </li>
                
                    <li class="c-breadcrumbs__item" id="breadcrumb1" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
                        <a href="/journal/10489" class="c-breadcrumbs__link" itemprop="item" data-track="click" data-track-category="article" data-track-action="breadcrumbs" data-track-label="breadcrumb2"><span itemprop="name">Applied Intelligence</span></a><meta itemprop="position" content="2">
                            <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10" xmlns="http://www.w3.org/2000/svg">
                                <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                            </svg>
                    </li>
                
                    <li class="c-breadcrumbs__item" id="breadcrumb2" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
                        <span itemprop="name">Article</span><meta itemprop="position" content="3">
                    </li>
                
            </ol>
        </nav>
    

        
        
    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
            
                <div class="c-context-bar u-hide"
                     data-test="context-bar"
                     data-context-bar
                     aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Compression-resistant backdoor attack against deep neural networks
                        </div>
                        
                            <div data-test="inCoD">
                                
    <div class="c-pdf-container">
        <div class="c-pdf-download u-clear-both u-mb-16">
            <a href="/content/pdf/10.1007/s10489-023-04575-8.pdf?pdf=button" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button" data-track-external download>
                
                    <span class="c-pdf-download__text">Download PDF</span>
                    <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
                
            </a>
        </div>
    </div>


                            </div>
                        
                    </div>
                    
<div id="recommendations">
    <div class="c-recommendations__container u-container u-display-none" data-component-recommendations>
        <aside class="c-status-message c-status-message--success u-display-none" data-component-status-msg>
            <svg class="c-status-message__icon" width="24" height="24" role="img" aria-label="success:" focusable="false">
                <use xlink:href="#icon-success"></use>
            </svg>
            <div class="c-status-message__message" tabindex="-1" id="success-message">
                Your article has downloaded
            </div>
        </aside>

        <div class="c-recommendations-header u-display-flex u-justify-content-space-between">
            <h2 class="c-recommendations-title" id="recommendation-heading">Similar articles being viewed by others</h2>
            <button class="c-recommendations-close u-flex-static" type="button" aria-label="Close" data-track="click" data-track-action="close recommendations">
                <svg class="u-icon" width="16" height="16" aria-hidden="true" focusable="false"><use xlink:href="#icon-close"></use></svg>
            </button>
        </div>

        <section aria-roledescription="carousel" aria-labelledby="recommendation-heading">
            <p class="u-visually-hidden">Slider with three articles shown per slide. Use the Previous and Next buttons to navigate the slides or the slide controller buttons at the end to navigate through each slide.</p>
            <div class="c-recommendations-list-container">
                <div class="c-recommendations-list">
                    
                    <div class="c-recommendations-list__item" role="group" aria-roledescription="slide" aria-label="Recommendation 1 of 9">
                        <article class="u-full-height c-card c-card--flush">
                            <div class="c-card__layout u-full-height">
                                
                                    <div class="c-card__image"><img src="//media.springernature.com/w136h75/springer-static/image/art%3A10.1007%2Fs11042-021-11135-0/MediaObjects/11042_2021_11135_Fig1_HTML.png" alt=""></div>
                                
                                <div class="c-card__body u-display-flex u-flex-direction-column">
                                    <div class="c-recommendations-column-switch">
                    <h3 class="c-card__title-recommendation"><a class="c-card__link" href="https://link.springer.com/10.1007/s11042-021-11135-0" data-track="click" data-track-action="click recommendations - 1" data-track-label="10.1007/s11042-021-11135-0">BlindNet backdoor: Attack on deep neural network using blind watermark</a></h3>
                                        <p class="u-sans-serif u-mb-0">07 January 2022</p>
                                    </div>
                                    <p class="c-recommendations-authors u-hide-at-sm u-sans-serif">Hyun Kwon &amp; Yongchul Kim</p>
                                </div>
                            </div>
                        </article>
                    </div>
                    
                    <div class="c-recommendations-list__item" role="group" aria-roledescription="slide" aria-label="Recommendation 2 of 9">
                        <article class="u-full-height c-card c-card--flush">
                            <div class="c-card__layout u-full-height">
                                
                                    <div class="c-card__image"><img src="//media.springernature.com/w136h75/springer-static/image/art%3A10.1186%2Fs13635-020-00104-z/MediaObjects/13635_2020_104_Fig1_HTML.png" alt=""></div>
                                
                                <div class="c-card__body u-display-flex u-flex-direction-column">
                                    <div class="c-recommendations-column-switch">
                    <h3 class="c-card__title-recommendation"><a class="c-card__link" href="https://link.springer.com/10.1186/s13635-020-00104-z" data-track="click" data-track-action="click recommendations - 2" data-track-label="10.1186/s13635-020-00104-z">Trembling triggers: exploring the sensitivity of backdoors in DNN-based face recognition</a></h3>
                                        <p class="u-sans-serif u-mb-0">23 June 2020</p>
                                    </div>
                                    <p class="c-recommendations-authors u-hide-at-sm u-sans-serif">Cecilia Pasquini &amp; Rainer Böhme</p>
                                </div>
                            </div>
                        </article>
                    </div>
                    
                    <div class="c-recommendations-list__item" role="group" aria-roledescription="slide" aria-label="Recommendation 3 of 9">
                        <article class="u-full-height c-card c-card--flush">
                            <div class="c-card__layout u-full-height">
                                
                                    <div class="c-card__image"><img src="//media.springernature.com/w136h75/springer-static/image/art%3A10.1007%2Fs10489-022-03339-0/MediaObjects/10489_2022_3339_Fig1_HTML.png" alt=""></div>
                                
                                <div class="c-card__body u-display-flex u-flex-direction-column">
                                    <div class="c-recommendations-column-switch">
                    <h3 class="c-card__title-recommendation"><a class="c-card__link" href="https://link.springer.com/10.1007/s10489-022-03339-0" data-track="click" data-track-action="click recommendations - 3" data-track-label="10.1007/s10489-022-03339-0">Active intellectual property protection for deep neural networks through stealthy backdoor and users’ identities authentication</a></h3>
                                        <p class="u-sans-serif u-mb-0">24 March 2022</p>
                                    </div>
                                    <p class="c-recommendations-authors u-hide-at-sm u-sans-serif">Mingfu Xue, Shichang Sun, … Weiqiang Liu</p>
                                </div>
                            </div>
                        </article>
                    </div>
                    
                    <div class="c-recommendations-list__item" role="group" aria-roledescription="slide" aria-label="Recommendation 4 of 9">
                        <article class="u-full-height c-card c-card--flush">
                            <div class="c-card__layout u-full-height">
                                
                                    <div class="c-card__image"><img src="//media.springernature.com/w136h75/springer-static/image/art%3A10.1007%2Fs13735-021-00219-0/MediaObjects/13735_2021_219_Fig1_HTML.png" alt=""></div>
                                
                                <div class="c-card__body u-display-flex u-flex-direction-column">
                                    <div class="c-recommendations-column-switch">
                    <h3 class="c-card__title-recommendation"><a class="c-card__link" href="https://link.springer.com/10.1007/s13735-021-00219-0" data-track="click" data-track-action="click recommendations - 4" data-track-label="10.1007/s13735-021-00219-0">Towards a high robust neural network via feature matching</a></h3>
                                        <p class="u-sans-serif u-mb-0">13 October 2021</p>
                                    </div>
                                    <p class="c-recommendations-authors u-hide-at-sm u-sans-serif">Jian Li, Yanming Guo, … Yingmei Wei</p>
                                </div>
                            </div>
                        </article>
                    </div>
                    
                    <div class="c-recommendations-list__item" role="group" aria-roledescription="slide" aria-label="Recommendation 5 of 9">
                        <article class="u-full-height c-card c-card--flush">
                            <div class="c-card__layout u-full-height">
                                
                                    <div class="c-card__image"><img src="//media.springernature.com/w136h75/springer-static/image/art%3A10.1007%2Fs11042-019-07753-4/MediaObjects/11042_2019_7753_Fig1_HTML.png" alt=""></div>
                                
                                <div class="c-card__body u-display-flex u-flex-direction-column">
                                    <div class="c-recommendations-column-switch">
                    <h3 class="c-card__title-recommendation"><a class="c-card__link" href="https://link.springer.com/10.1007/s11042-019-07753-4" data-track="click" data-track-action="click recommendations - 5" data-track-label="10.1007/s11042-019-07753-4">A privacy-preserving image retrieval scheme based secure kNN, DNA coding and deep hashing</a></h3>
                                        <p class="u-sans-serif u-mb-0">25 May 2019</p>
                                    </div>
                                    <p class="c-recommendations-authors u-hide-at-sm u-sans-serif">Shu-Li Cheng, Lie-Jun Wang, … An-Yu Du</p>
                                </div>
                            </div>
                        </article>
                    </div>
                    
                    <div class="c-recommendations-list__item" role="group" aria-roledescription="slide" aria-label="Recommendation 6 of 9">
                        <article class="u-full-height c-card c-card--flush">
                            <div class="c-card__layout u-full-height">
                                
                                    <div class="c-card__image"><img src="//media.springernature.com/w136h75/springer-static/image/art%3A10.1007%2Fs11042-022-12583-y/MediaObjects/11042_2022_12583_Fig1_HTML.png" alt=""></div>
                                
                                <div class="c-card__body u-display-flex u-flex-direction-column">
                                    <div class="c-recommendations-column-switch">
                    <h3 class="c-card__title-recommendation"><a class="c-card__link" href="https://link.springer.com/10.1007/s11042-022-12583-y" data-track="click" data-track-action="click recommendations - 6" data-track-label="10.1007/s11042-022-12583-y">Visual image encryption scheme based on vector quantization and content transform</a></h3>
                                        <p class="u-sans-serif u-mb-0">21 February 2022</p>
                                    </div>
                                    <p class="c-recommendations-authors u-hide-at-sm u-sans-serif">Sifei Zheng, Chengyu Liu, … Xiaolong Liu</p>
                                </div>
                            </div>
                        </article>
                    </div>
                    
                    <div class="c-recommendations-list__item" role="group" aria-roledescription="slide" aria-label="Recommendation 7 of 9">
                        <article class="u-full-height c-card c-card--flush">
                            <div class="c-card__layout u-full-height">
                                
                                    <div class="c-card__image"><img src="//media.springernature.com/w136h75/springer-static/image/art%3A10.1007%2Fs00521-020-04724-x/MediaObjects/521_2020_4724_Fig1_HTML.png" alt=""></div>
                                
                                <div class="c-card__body u-display-flex u-flex-direction-column">
                                    <div class="c-recommendations-column-switch">
                    <h3 class="c-card__title-recommendation"><a class="c-card__link" href="https://link.springer.com/10.1007/s00521-020-04724-x" data-track="click" data-track-action="click recommendations - 7" data-track-label="10.1007/s00521-020-04724-x">Subdata image encryption scheme based on compressive sensing and vector quantization</a></h3>
                                        <p class="u-sans-serif u-mb-0">18 January 2020</p>
                                    </div>
                                    <p class="c-recommendations-authors u-hide-at-sm u-sans-serif">Haiju Fan, Kanglei Zhou, … Ming Li</p>
                                </div>
                            </div>
                        </article>
                    </div>
                    
                    <div class="c-recommendations-list__item" role="group" aria-roledescription="slide" aria-label="Recommendation 8 of 9">
                        <article class="u-full-height c-card c-card--flush">
                            <div class="c-card__layout u-full-height">
                                
                                    <div class="c-card__image"><img src="//media.springernature.com/w136h75/springer-static/image/art%3A10.1007%2Fs00530-022-01026-1/MediaObjects/530_2022_1026_Fig1_HTML.png" alt=""></div>
                                
                                <div class="c-card__body u-display-flex u-flex-direction-column">
                                    <div class="c-recommendations-column-switch">
                    <h3 class="c-card__title-recommendation"><a class="c-card__link" href="https://link.springer.com/10.1007/s00530-022-01026-1" data-track="click" data-track-action="click recommendations - 8" data-track-label="10.1007/s00530-022-01026-1">2C-Net: integrate image compression and classification via deep neural network</a></h3>
                                        <p class="u-sans-serif u-mb-0">01 December 2022</p>
                                    </div>
                                    <p class="c-recommendations-authors u-hide-at-sm u-sans-serif">Linfeng Liu, Tong Chen, … Qiu Shen</p>
                                </div>
                            </div>
                        </article>
                    </div>
                    
                    <div class="c-recommendations-list__item" role="group" aria-roledescription="slide" aria-label="Recommendation 9 of 9">
                        <article class="u-full-height c-card c-card--flush">
                            <div class="c-card__layout u-full-height">
                                
                                    <div class="c-card__image"><img src="//media.springernature.com/w136h75/springer-static/image/art%3A10.1007%2Fs11042-018-6737-3/MediaObjects/11042_2018_6737_Fig1_HTML.png" alt=""></div>
                                
                                <div class="c-card__body u-display-flex u-flex-direction-column">
                                    <div class="c-recommendations-column-switch">
                    <h3 class="c-card__title-recommendation"><a class="c-card__link" href="https://link.springer.com/10.1007/s11042-018-6737-3" data-track="click" data-track-action="click recommendations - 9" data-track-label="10.1007/s11042-018-6737-3">Detection of double JPEG compression using modified DenseNet model</a></h3>
                                        <p class="u-sans-serif u-mb-0">08 October 2018</p>
                                    </div>
                                    <p class="c-recommendations-authors u-hide-at-sm u-sans-serif">Ximei Zeng, Guorui Feng &amp; Xinpeng Zhang</p>
                                </div>
                            </div>
                        </article>
                    </div>
                    
                </div>
            </div>
        </section>
    </div>
    <div class="js-greyout-page-background" style="display:none" data-component-grey-background></div>
        <script>
            window.dataLayer = window.dataLayer || [];
            window.dataLayer.push({
                recommendations: {
                    recommender: 'semantic',
                    model: 'specter',
                    policy_id: 'BootstrappedUCB',
                    timestamp: 1683004949
                }
            });
        </script>
    
</div>

                </div>
            

            
                <div class="c-pdf-button__container u-hide-at-lg js-context-bar-sticky-point-mobile">
                    
    <div class="c-pdf-container">
        <div class="c-pdf-download u-clear-both u-mb-16">
            <a href="/content/pdf/10.1007/s10489-023-04575-8.pdf?pdf=button" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button" data-track-external download>
                
                    <span class="c-pdf-download__text">Download PDF</span>
                    <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
                
            </a>
        </div>
    </div>


                </div>
            

            <div class="c-article-collection__container">
                
    

            </div>


            <article lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2023-04-12">12 April 2023</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="">Compression-resistant backdoor attack against deep neural networks</h1>
                        <ul class="c-article-author-list c-article-author-list--short" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Mingfu-Xue" data-author-popup="auth-Mingfu-Xue" data-corresp-id="c1">Mingfu Xue<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a><span class="u-js-hide"> 
            <a class="js-orcid" href="http://orcid.org/0000-0003-2408-503X"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0003-2408-503X</a></span><sup class="u-js-hide"><a href="#Aff1">1</a></sup>, </li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Xin-Wang" data-author-popup="auth-Xin-Wang">Xin Wang</a><sup class="u-js-hide"><a href="#Aff1">1</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Shichang-Sun" data-author-popup="auth-Shichang-Sun">Shichang Sun</a><sup class="u-js-hide"><a href="#Aff1">1</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Yushu-Zhang" data-author-popup="auth-Yushu-Zhang">Yushu Zhang</a><sup class="u-js-hide"><a href="#Aff1">1</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jian-Wang" data-author-popup="auth-Jian-Wang">Jian Wang</a><sup class="u-js-hide"><a href="#Aff1">1</a></sup> &amp; </li><li class="c-article-author-list__show-more" aria-label="Show all 6 authors for this article" title="Show all 6 authors for this article">…</li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Weiqiang-Liu" data-author-popup="auth-Weiqiang-Liu">Weiqiang Liu</a><sup class="u-js-hide"><a href="#Aff2">2</a></sup> </li></ul><button aria-expanded="false" class="c-article-author-list__button"><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-plus"></use></svg><span>Show authors</span></button>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10489" data-track="click" data-track-action="journal homepage" data-track-category="article body" data-track-label="link"><i data-test="journal-title">Applied Intelligence</i></a>

                             (<span data-test="article-publication-year">2023</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    <div class="c-article-metrics-bar__wrapper u-clear-both">
        <ul class="c-article-metrics-bar u-list-reset">
            
                <li class=" c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__count">90 <span class="c-article-metrics-bar__label">Accesses</span></p>
                </li>
            
            
            
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">1 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                    </li>
                
            
            <li class="c-article-metrics-bar__item">
                <p class="c-article-metrics-bar__details"><a href="/article/10.1007/s10489-023-04575-8/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
            </li>
        </ul>
    </div>
</div>

                        </div>
                        
    

    

                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" data-title="Abstract" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>In recent years, a number of backdoor attacks against deep neural networks (DNN) have been proposed. In this paper, we reveal that backdoor attacks are vulnerable to image compressions, as backdoor instances used to trigger backdoor attacks are usually compressed by image compression methods during data transmission. When backdoor instances are compressed, the feature of backdoor trigger will be destroyed, which could result in significant performance degradation for backdoor attacks. As a countermeasure, we propose the first compression-resistant backdoor attack method based on feature consistency training. Specifically, both backdoor images and their compressed versions are used for training, and the feature difference between backdoor images and their compressed versions are minimized through feature consistency training. As a result, the DNN treats the feature of compressed images as the feature of backdoor images in feature space. After training, the backdoor attack will be robust to image compressions. Furthermore, we consider three different image compressions (i.e., JPEG, JPEG2000, WEBP) during the feature consistency training, so that the backdoor attack can be robust to multiple image compression algorithms. Experimental results demonstrate that when the backdoor instances are compressed, the attack success rate of common backdoor attack is 6.63% (JPEG), 6.20% (JPEG2000) and 3.97% (WEBP) respectively, while the attack success rate of the proposed compression-resistant backdoor attack is 98.77% (JPEG), 97.69% (JPEG2000), and 98.93% (WEBP) respectively. The compression-resistant attack is robust under various parameters settings. In addition, extensive experiments have demonstrated that even if only one image compression method is used in the feature consistency training process, the proposed compression-resistant backdoor attack has the generalization ability to resist multiple unseen image compression methods.</p></div></div></section>
                    
    


                    

                    <div data-test="cobranding-download">
                        
                    </div>

                    <div class="app-checklist-banner--on-mobile">
                        
                            
    <div class="app-checklist-banner" data-test="article-checklist-banner">
        <div class="app-checklist-banner__body">
            <h3 class="app-checklist-banner__title">Working on a manuscript?</h3>
            <a class="app-checklist-banner__link" data-track="click" data-track-category="pre-submission-checklist" data-track-action="clicked article page checklist banner test 2 old version" data-track-label="link" href="https://beta.springernature.com/pre-submission?journalId=10489"
            data-test="article-checklist-banner-link">Avoid the common mistakes
            <svg class="app-checklist-banner__arrow-icon" aria-hidden="true" focusable="false">
                <use xlink:href="#icon-springer-arrow-right"></use>
            </svg>
            </a>
        </div>
        <div class="app-checklist-banner__icon-container">
        <svg class="app-checklist-banner__paper-icon" aria-hidden="true" focusable="false">
            <use xlink:href="#icon-checklist-banner"></use>
        </svg>
        </div>
    </div>

                        
                    </div>

                    
                        
                            <div class="main-content">
                                <section data-title="Introduction"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1"><span class="c-article-section__title-number">1 </span>Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Deep neural networks (DNN) have been widely used in many tasks. However, many researches have demonstrated that DNN are vulnerable to backdoor attacks. By embedding the backdoor into the model during training stage, the images with specific backdoor trigger can make the backdoored model output specified target label in the test stage.</p><p>However, most existing backdoor attacks are not robust to image compressions. Generally, images uploaded to the Internet will undergo image compressions, which are widely used to reduce the transmission and storage overhead [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Wang Z., Guo H., Zhang Z., Song M., Zheng S., Wang Q., Niu B. (2020) Towards compression-resistant privacy-preserving photo sharing on social networks. In: the 21st ACM International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing, pp 81–90" href="/article/10.1007/s10489-023-04575-8#ref-CR1" id="ref-link-section-d136029957e475">1</a>]. Wan <i>et al.</i> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Wan S., Wu T., Hsu H., Wong W.H., Lee C. (2020) Feature consistency training with JPEG compressed images. IEEE Trans Circuits Syst Video Technol 30(12):4769–4780" href="/article/10.1007/s10489-023-04575-8#ref-CR2" id="ref-link-section-d136029957e481">2</a>] indicate that image compression distorts the feature of images, which will lead to accuracy degradation of DNN. In this paper, we study the impact of image compressions on DNN backdoor attacks. If backdoor instances are compressed, the backdoor trigger hidden in the images will be destroyed, which will seriously reduce the performance of the backdoor attack. As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10489-023-04575-8#Fig1">1</a>, with the embedded backdoor trigger, the “James Marden” in the image will be incorrectly recognized as the target class “Aamir Khan” specified by the attacker. However, after image compression, the backdoor trigger is destroyed and “James Marden” will be correctly classified as “James Marden” by the model, i.e., the backdoor attack fails when the backdoor instance is compressed. To date, all the existing DNN backdoor attacks do not consider the problem of image compressions.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1" data-title="Fig. 1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10489-023-04575-8/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10489-023-04575-8/MediaObjects/10489_2023_4575_Fig1_HTML.png?as=webp"><img aria-describedby="Fig1" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10489-023-04575-8/MediaObjects/10489_2023_4575_Fig1_HTML.png" alt="figure 1" loading="lazy" width="685" height="257"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>The performance of normal backdoor attack after image compression</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10489-023-04575-8/figures/1" data-track-dest="link:Figure1 Full size image" aria-label="Full size image figure 1" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>In this paper, we study the impact of image compression on backdoor attacks for the first time and propose the first compression-resistant backdoor attack method. We develop the compression-resistant backdoor attack by exploiting feature consistency training [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Wan S., Wu T., Hsu H., Wong W.H., Lee C. (2020) Feature consistency training with JPEG compressed images. IEEE Trans Circuits Syst Video Technol 30(12):4769–4780" href="/article/10.1007/s10489-023-04575-8#ref-CR2" id="ref-link-section-d136029957e507">2</a>]. In the training stage, both the backdoor instances and their compressed versions are used to train the DNN. At each iteration of the training, the feature of both backdoor images and their compressed versions are extracted by internal layers of the DNN. The extracted features will be used to minimize the difference between normal backdoor images and compressed backdoor images. As a result, the trained DNN will treat compressed images as normal backdoor images in the feature space. Moreover, three image compression methods (i.e., JPEG [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Wallace G.K. (1992) The JPEG still picture compression standard. IEEE Trans Consum Electron 38(1):30–44" href="/article/10.1007/s10489-023-04575-8#ref-CR3" id="ref-link-section-d136029957e510">3</a>], JPEG2000 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Skodras A., Christopoulos C.A., Ebrahimi T. (2001) The JPEG 2000 still image compression standard. IEEE Signal Process Mag 18(5):36–58" href="/article/10.1007/s10489-023-04575-8#ref-CR4" id="ref-link-section-d136029957e513">4</a>] and WEBP [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Ginesu G., Pintus M., Giusto D.D. (2012) Objective assessment of the WebP image coding algorithm. Signal Processing: Image Communication 27(8):867–874" href="/article/10.1007/s10489-023-04575-8#ref-CR5" id="ref-link-section-d136029957e516">5</a>]) are considered simultaneously during the feature consistency training, so that the proposed backdoor attack can be robust to multiple image compression algorithms. In the test stage, even if backdoor instances are compressed by an unknown image compression method, the proposed backdoor attack is still able to achieve a high attack success rate. Experimental results demonstrate that the performance of compression-resistant backdoor attack is significantly improved compared to normal backdoor attacks. Meanwhile, the proposed backdoor attack is able to resist a variety of compression methods without performance degradation. In addition, extensive experiments have demonstrated that, even if only one image compression method is used in the feature consistency training process, the compression-resistant backdoor attack can resist multiple “unknown” image compressions which are not considered in the training process.</p><p>The main contributions of this paper are threefold: 
</p><ul class="u-list-style-bullet">
              <li>
                <p>We reveal that the existing backdoor attacks are vulnerable to image compressions. As a countermeasure, a compression-resistant backdoor attack method is proposed. To the best of our knowledge, this is the first compression-resistant backdoor attack against Deep Neural Networks, which is an advanced variant of backdoor attack.</p>
              </li>
              <li>
                <p>We strengthen the proposed backdoor attack using three different image compression methods (JPEG [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Wallace G.K. (1992) The JPEG still picture compression standard. IEEE Trans Consum Electron 38(1):30–44" href="/article/10.1007/s10489-023-04575-8#ref-CR3" id="ref-link-section-d136029957e534">3</a>], JPEG2000 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Skodras A., Christopoulos C.A., Ebrahimi T. (2001) The JPEG 2000 still image compression standard. IEEE Signal Process Mag 18(5):36–58" href="/article/10.1007/s10489-023-04575-8#ref-CR4" id="ref-link-section-d136029957e537">4</a>] and WEBP [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Ginesu G., Pintus M., Giusto D.D. (2012) Objective assessment of the WebP image coding algorithm. Signal Processing: Image Communication 27(8):867–874" href="/article/10.1007/s10489-023-04575-8#ref-CR5" id="ref-link-section-d136029957e540">5</a>]) during the feature consistency training, so that the proposed attack is robust to different image compression methods.</p>
              </li>
              <li>
                <p>Experimental results have demonstrated that, even if only one image compression method is considered during training, the proposed compression-resistant backdoor attack has the generalization ability to resist multiple unknown image compression methods.</p>
              </li>
            </ul><p>The rest of this paper is organized as follows. Related works are reviewed in Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-023-04575-8#Sec2">2</a>. The proposed method is presented in Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-023-04575-8#Sec5">3</a>. Experimental results are discussed in Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-023-04575-8#Sec9">4</a>. This paper is concluded in Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-023-04575-8#Sec15">5</a>.</p></div></div></section><section data-title="Related work"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2"><span class="c-article-section__title-number">2 </span>Related work</h2><div class="c-article-section__content" id="Sec2-content"><p>In this section, first, related works on existing backdoor attacks are reviewed. Second, the few compression-resistant works in the deep learning area (but not for backdoor attacks) are discussed.</p><h3 class="c-article__sub-heading" id="Sec3"><span class="c-article-section__title-number">2.1 </span>Backdoor attack</h3><p>A number of backdoor attacks against deep learning models have been proposed. Gu <i>et al.</i> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Gu T., Liu K., Dolan-Gavitt B., Garg S. (2019) BadNets: Evaluating backdooring attacks on deep neural networks. IEEE Access 7:47230–47244" href="/article/10.1007/s10489-023-04575-8#ref-CR6" id="ref-link-section-d136029957e585">6</a>] use a fixed square pattern as the backdoor trigger to launch the backdoor attack. Chen <i>et al.</i> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Chen X., Liu C., Li B., Lu K., Song D. (2017) Targeted backdoor attacks on deep learning systems using data poisoning. arXiv:&#xA;                1712.05526&#xA;                &#xA;              , 1–18" href="/article/10.1007/s10489-023-04575-8#ref-CR7" id="ref-link-section-d136029957e591">7</a>] use three methods (i.e., blending, accessory and blended accessory) to construct triggers and generate backdoor instances, which are injected into the training set to embed the backdoor during model training. Liu <i>et al.</i> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Liu Y., Ma S., Aafer Y., Lee W., Zhai J., Wang W., Zhang X. (2018) Trojaning attack on neural networks. In: 25th Annual Network and Distributed System Security Symposium, pp 1–15" href="/article/10.1007/s10489-023-04575-8#ref-CR8" id="ref-link-section-d136029957e598">8</a>] generate the backdoor trigger by reverse engineering the DNN, so that attackers can conduct backdoor attacks without the access to the original training dataset.</p><p>Some recent works aim at improving the concealment of backdoor attacks [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Li S., Xue M., Zhao B.Z.H., Zhu H., Zhang X. (2021) Invisible backdoor attacks on deep neural networks via steganography and regularization. IEEE Trans Dependable Secure Comput 18(5):2088–2105" href="#ref-CR9" id="ref-link-section-d136029957e604">9</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Zhang J., Chen D., Huang Q., Liao J., Zhang W., Feng H., Hua G., Yu N. (2022) Poison ink: Robust and invisible backdoor attack. IEEE Transactions on Image Processin 31:5691–5705" href="#ref-CR10" id="ref-link-section-d136029957e604_1">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Zhong H., Liao C., Squicciarini A.C., Zhu S., Miller D.J. (2020) Backdoor embedding in convolutional neural network models via invisible perturbation. In: 10th ACM Conference on Data and Application Security and Privacy, pp 97–108" href="/article/10.1007/s10489-023-04575-8#ref-CR11" id="ref-link-section-d136029957e607">11</a>]. Zhong <i>et al.</i> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Zhong H., Liao C., Squicciarini A.C., Zhu S., Miller D.J. (2020) Backdoor embedding in convolutional neural network models via invisible perturbation. In: 10th ACM Conference on Data and Application Security and Privacy, pp 97–108" href="/article/10.1007/s10489-023-04575-8#ref-CR11" id="ref-link-section-d136029957e613">11</a>] use a perturbation mask as the backdoor trigger, which is invisible. Li <i>et al.</i> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Li S., Xue M., Zhao B.Z.H., Zhu H., Zhang X. (2021) Invisible backdoor attacks on deep neural networks via steganography and regularization. IEEE Trans Dependable Secure Comput 18(5):2088–2105" href="/article/10.1007/s10489-023-04575-8#ref-CR9" id="ref-link-section-d136029957e620">9</a>] conduct invisible backdoor attacks in which they use image steganography and <i>L</i><sub><i>p</i></sub> regularization to generate the backdoor trigger respectively. Zhang et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Zhang J., Chen D., Huang Q., Liao J., Zhang W., Feng H., Hua G., Yu N. (2022) Poison ink: Robust and invisible backdoor attack. IEEE Transactions on Image Processin 31:5691–5705" href="/article/10.1007/s10489-023-04575-8#ref-CR10" id="ref-link-section-d136029957e629">10</a>] generate the backdoor trigger based on image structures. Since the information of image structures is difficult to be perceived by humans, using image structures to generate the backdoor trigger can make the backdoor attack concealed.</p><p>The backdoor triggers in most of the backdoor works are static patterns, which means the backdoor trigger is a single pattern with a fixed position. Recently, few works [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Xue M., He C., Wang J., Liu W. (2022) One-to-N &amp; N-to-One: Two advanced backdoor attacks against deep learning models. IEEE Trans Dependable Secure Comput 19(3):1562–1578" href="/article/10.1007/s10489-023-04575-8#ref-CR12" id="ref-link-section-d136029957e635">12</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Salem A., Wen R., Backes M., Ma S., Zhang Y. (2020) Dynamic backdoor attacks against machine learning models. arXiv:&#xA;                2003.03675&#xA;                &#xA;              , 1–18" href="/article/10.1007/s10489-023-04575-8#ref-CR13" id="ref-link-section-d136029957e638">13</a>] propose backdoor attacks with flexible triggers. Xue <i>et al.</i> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Xue M., He C., Wang J., Liu W. (2022) One-to-N &amp; N-to-One: Two advanced backdoor attacks against deep learning models. IEEE Trans Dependable Secure Comput 19(3):1562–1578" href="/article/10.1007/s10489-023-04575-8#ref-CR12" id="ref-link-section-d136029957e644">12</a>] develop multi-target backdoor attack and multi-trigger backdoor attack. In the multi-target attack, the attacker can use different intensities of the backdoor trigger to control different attack targets. In the multi-trigger attack, the target can only be triggered when all the backdoor triggers are appeared in a backdoor instance. Salem <i>et al.</i> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Salem A., Wen R., Backes M., Ma S., Zhang Y. (2020) Dynamic backdoor attacks against machine learning models. arXiv:&#xA;                2003.03675&#xA;                &#xA;              , 1–18" href="/article/10.1007/s10489-023-04575-8#ref-CR13" id="ref-link-section-d136029957e651">13</a>] design a dynamic backdoor attack, where the backdoor trigger can be flexibly placed in a random position of the image and the backdoor trigger could be a random pattern.</p><p>The above mentioned works focus on the digital domain, which does not consider the constraints (e.g., angle, distance) in real physical world. To this end, Xue <i>et al.</i> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Xue M., He C., Wu Y., Sun S., Zhang Y., Wang J., Liu W. (2022) PTB: Robust physical backdoor attacks against deep neural networks in real world. Computer &amp; Security. 118(102726):1–15" href="/article/10.1007/s10489-023-04575-8#ref-CR14" id="ref-link-section-d136029957e660">14</a>] consider the attack scenarios in real world and propose physical transformations for backdoors. Various transformations are performed on the backdoor instances during the training process to enhance the robustness of the backdoor attack in real physical world.</p><h3 class="c-article__sub-heading" id="Sec4"><span class="c-article-section__title-number">2.2 </span>Compression-resistance in deep learning area</h3><p>Recently, a few researches have shown that DNN are vulnerable to image compressions. In order to generate the JPEG compression-resistant adversarial examples, Shin <i>et al.</i> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Shin R., Song D. (2017) JPEG-resistant adversarial images. In: NIPS Workshop on Machine Learning and Computer Security, vol 1, pp 1–6" href="/article/10.1007/s10489-023-04575-8#ref-CR15" id="ref-link-section-d136029957e674">15</a>] add a differentiable approximation to JPEG compression during the adversarial examples generation process. The approximation operation can maximize the prediction difference between the original image and the compressed adversarial example. Wang <i>et al.</i> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Wang Z., Guo H., Zhang Z., Song M., Zheng S., Wang Q., Niu B. (2020) Towards compression-resistant privacy-preserving photo sharing on social networks. In: the 21st ACM International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing, pp 81–90" href="/article/10.1007/s10489-023-04575-8#ref-CR1" id="ref-link-section-d136029957e680">1</a>] generate compression-resistant adversarial examples to protect the privacy in photos on social networks. They train an encoder-decoder network (named ComModel) to simulate different compression methods. Then, ComModel is used to compress images during the process of adversarial example generation, so that the generated adversarial examples are compression-resistant. Cao <i>et al.</i> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Cao S., Zou Q., Mao X., Ye D., Wang Z. (2021) Metric learning for anti-compression facial forgery detection. In: ACM Multimedia Conference, pp 1929–1937" href="/article/10.1007/s10489-023-04575-8#ref-CR16" id="ref-link-section-d136029957e687">16</a>] propose a facial forgery detection method based on metric learning which can detect compressed forgery images. Wan <i>et al.</i> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Wan S., Wu T., Hsu H., Wong W.H., Lee C. (2020) Feature consistency training with JPEG compressed images. IEEE Trans Circuits Syst Video Technol 30(12):4769–4780" href="/article/10.1007/s10489-023-04575-8#ref-CR2" id="ref-link-section-d136029957e693">2</a>] propose feature consistency training to enhance the robustness of DNN against JPEG compression. It encourages the DNN to learn consistent features of the raw image and the compressed image. Furthermore, in order to resist JPEG compression with different compression levels, a residual mapping block is used during feature consistency training.</p><p>The above works [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Wang Z., Guo H., Zhang Z., Song M., Zheng S., Wang Q., Niu B. (2020) Towards compression-resistant privacy-preserving photo sharing on social networks. In: the 21st ACM International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing, pp 81–90" href="/article/10.1007/s10489-023-04575-8#ref-CR1" id="ref-link-section-d136029957e699">1</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Shin R., Song D. (2017) JPEG-resistant adversarial images. In: NIPS Workshop on Machine Learning and Computer Security, vol 1, pp 1–6" href="/article/10.1007/s10489-023-04575-8#ref-CR15" id="ref-link-section-d136029957e702">15</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Cao S., Zou Q., Mao X., Ye D., Wang Z. (2021) Metric learning for anti-compression facial forgery detection. In: ACM Multimedia Conference, pp 1929–1937" href="/article/10.1007/s10489-023-04575-8#ref-CR16" id="ref-link-section-d136029957e705">16</a>] aim at generating compression-resistant adversarial examples, while the work [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Wan S., Wu T., Hsu H., Wong W.H., Lee C. (2020) Feature consistency training with JPEG compressed images. IEEE Trans Circuits Syst Video Technol 30(12):4769–4780" href="/article/10.1007/s10489-023-04575-8#ref-CR2" id="ref-link-section-d136029957e708">2</a>] aims to improve the robustness of DNN to image compression. To date, no backdoor works have considered image compression. In this paper, for the first time, we propose a compression-resistant backdoor attack.</p></div></div></section><section data-title="The proposed method"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5"><span class="c-article-section__title-number">3 </span>The proposed method</h2><div class="c-article-section__content" id="Sec5-content"><h3 class="c-article__sub-heading" id="Sec6"><span class="c-article-section__title-number">3.1 </span>Overview</h3><p>In this attack scenario, the adversary is assumed to be able to control the training process of the target model, which is the same as the attack scenario in most latest backdoor attacks [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Cheng S., Liu Y., Ma S., Zhang X. (2021) Deep feature space Trojan attack of neural networks by controlled detoxification. In: 35th AAAI Conference on Artificial Intelligence, pp 1148–1156" href="#ref-CR17" id="ref-link-section-d136029957e724">17</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Gong X., Chen Y., Wang Q., Huang H., Meng L., Shen C., Zhang Q. (2021) Defense-resistant backdoor attacks against deep neural networks in outsourced cloud environment. IEEE J Sel Areas Commun 39(8):2617–2631" href="#ref-CR18" id="ref-link-section-d136029957e724_1">18</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Nguyen T.A., Tran A.T. (2021) WaNet - Imperceptible warping-based backdoor attack. In: 9th International Conference on Learning Representations, pp 1–16" href="/article/10.1007/s10489-023-04575-8#ref-CR19" id="ref-link-section-d136029957e727">19</a>]. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10489-023-04575-8#Fig2">2</a> shows the overall flow of the proposed method. First, the attacker prepares training data for model training, which includes clean images, backdoor images and compressed backdoor images. Second, the attacker trains a clean DNN model with clean data, and obtains the model parameters <i>𝜃</i>. Third, the attacker performs feature consistency training [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Wan S., Wu T., Hsu H., Wong W.H., Lee C. (2020) Feature consistency training with JPEG compressed images. IEEE Trans Circuits Syst Video Technol 30(12):4769–4780" href="/article/10.1007/s10489-023-04575-8#ref-CR2" id="ref-link-section-d136029957e736">2</a>] to conduct the compression-resistant backdoor attack, where the aforementioned backdoor data (including the normal backdoor images and the compressed ones) is used to train the model. The model parameters <i>𝜃</i> will be updated during feature consistency training. During feature consistency training [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Wan S., Wu T., Hsu H., Wong W.H., Lee C. (2020) Feature consistency training with JPEG compressed images. IEEE Trans Circuits Syst Video Technol 30(12):4769–4780" href="/article/10.1007/s10489-023-04575-8#ref-CR2" id="ref-link-section-d136029957e743">2</a>], a feature consistency loss is well designed to improve the robustness of the embedded backdoor against image compression, which will be elaborated in Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-023-04575-8#Sec8">3.3</a>. More specifically, due to the well-designed loss function, the DNN will regard the feature of compressed images as a part of backdoor features. Finally, the model parameters <i>𝜃</i> are updated by the back-propagation [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Rumelhart D.E., Hinton G.E., Williams R.J. (1986) Learning representations by back-propagating errors. Nature 323(6088):533–536" href="/article/10.1007/s10489-023-04575-8#ref-CR20" id="ref-link-section-d136029957e752">20</a>] algorithm.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2" data-title="Fig. 2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10489-023-04575-8/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10489-023-04575-8/MediaObjects/10489_2023_4575_Fig2_HTML.png?as=webp"><img aria-describedby="Fig2" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10489-023-04575-8/MediaObjects/10489_2023_4575_Fig2_HTML.png" alt="figure 2" loading="lazy" width="685" height="445"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Overall flow of the proposed method</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10489-023-04575-8/figures/2" data-track-dest="link:Figure2 Full size image" aria-label="Full size image figure 2" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The differences between this paper and the feature consistency training in work [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Wan S., Wu T., Hsu H., Wong W.H., Lee C. (2020) Feature consistency training with JPEG compressed images. IEEE Trans Circuits Syst Video Technol 30(12):4769–4780" href="/article/10.1007/s10489-023-04575-8#ref-CR2" id="ref-link-section-d136029957e775">2</a>] are summarized as follows. First, the work [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Wan S., Wu T., Hsu H., Wong W.H., Lee C. (2020) Feature consistency training with JPEG compressed images. IEEE Trans Circuits Syst Video Technol 30(12):4769–4780" href="/article/10.1007/s10489-023-04575-8#ref-CR2" id="ref-link-section-d136029957e778">2</a>] uses feature consistency training to minimize the impact of the JPEG compression on image classification tasks. Unlike the work [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Wan S., Wu T., Hsu H., Wong W.H., Lee C. (2020) Feature consistency training with JPEG compressed images. IEEE Trans Circuits Syst Video Technol 30(12):4769–4780" href="/article/10.1007/s10489-023-04575-8#ref-CR2" id="ref-link-section-d136029957e781">2</a>], in this paper, feature consistency training is used to improve the robustness of the backdoor attack against image compression. Second, in the work [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Wan S., Wu T., Hsu H., Wong W.H., Lee C. (2020) Feature consistency training with JPEG compressed images. IEEE Trans Circuits Syst Video Technol 30(12):4769–4780" href="/article/10.1007/s10489-023-04575-8#ref-CR2" id="ref-link-section-d136029957e784">2</a>], only clean images and compressed images are used for model training, while in this paper, clean images, backdoor images and compressed backdoor images are used for model training. Last, in the work [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Wan S., Wu T., Hsu H., Wong W.H., Lee C. (2020) Feature consistency training with JPEG compressed images. IEEE Trans Circuits Syst Video Technol 30(12):4769–4780" href="/article/10.1007/s10489-023-04575-8#ref-CR2" id="ref-link-section-d136029957e787">2</a>], only JPEG compression is used during feature consistency training, while in this work, three types of compressions are used during feature consistency training to make backdoor attacks robust to various types (including unseen types) of compression algorithms.</p><p>In this paper, we focus on the discussion about the feature consistency training process, while the process of training the clean DNN (the second step) is a general procedure which will not be elaborated. Therefore, in the following sections, the first step (backdoor data generation) and the third step (backdoor embedding via feature consistency training) are discussed in detail.</p><h3 class="c-article__sub-heading" id="Sec7"><span class="c-article-section__title-number">3.2 </span>Backdoor data generation</h3><p>In normal backdoor attacks, given a clean training set <i>D</i><sub><i>c</i></sub>, a subset of training set <i>D</i><sub><i>p</i></sub> is randomly sampled from <i>D</i><sub><i>c</i></sub>. For each clean image <i>x</i> ∈ <i>D</i><sub><i>p</i></sub>, the attacker generates backdoor instance <i>x</i><sub><i>b</i></sub> ∈ <i>D</i><sub><i>b</i></sub> by adding trigger <i>δ</i> to the clean image <i>x</i> ∈ <i>D</i><sub><i>p</i></sub>, i.e., <i>x</i><sub><i>b</i></sub> = <i>x</i> + <i>δ</i>. These generated backdoor instances <i>x</i><sub><i>b</i></sub> are labelled as the target label <i>y</i><sub><i>t</i></sub> that is specified by the attacker, and the backdoor instance set <i>D</i><sub><i>b</i></sub> is injected into the clean training set. The training set injected with backdoor instances are used to train the target model in order to inject backdoor into the model.</p><p>In normal backdoor attacks, given a deep neural network <i>F</i><sub><i>𝜃</i></sub>, the behavior of a backdoor attack can be denoted as follows [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Chen X., Liu C., Li B., Lu K., Song D. (2017) Targeted backdoor attacks on deep learning systems using data poisoning. arXiv:&#xA;                1712.05526&#xA;                &#xA;              , 1–18" href="/article/10.1007/s10489-023-04575-8#ref-CR7" id="ref-link-section-d136029957e894">7</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Xue M., He C., Sun S., Wang J., Liu W. (2021) Robust backdoor attacks against deep neural networks in real physical world. In: 20th IEEE International Conference on Trust, Security and Privacy in Computing and Communications, pp 620–626" href="/article/10.1007/s10489-023-04575-8#ref-CR21" id="ref-link-section-d136029957e897">21</a>]:
</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ P(F_{\theta}({x_{b}}) = {y_{t}}) &gt; P(F_{\theta}({x_{b}}) = {y_{i}}:i \ne t) $$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>where <i>P</i> represents the probability that is output by the model <i>F</i><sub><i>𝜃</i></sub>, <i>y</i><sub><i>i</i></sub> represents any other class except the target class. In addition to making the model <i>F</i><sub><i>𝜃</i></sub> classify the backdoor instance into the target class, the attack should also not degrade the classification performance of the DNN on clean inputs. In other words, the clean image <i>x</i> should be classified as the ground truth label <i>y</i><sub><i>t</i><i>r</i><i>u</i><i>t</i><i>h</i></sub> by the DNN, i.e., <i>F</i><sub><i>𝜃</i></sub>(<i>x</i>) = <i>y</i><sub><i>t</i><i>r</i><i>u</i><i>t</i><i>h</i></sub>.</p><p>To conduct a compression-resistant backdoor attack, we generate a set of backdoor data <i>D</i><sub><i>b</i></sub> at first. The backdoor data consists of the normal backdoor data <i>x</i><sub><i>b</i></sub> and the compressed backdoor data <i>C</i><i>o</i><i>m</i><i>p</i><i>r</i><i>e</i><i>s</i><i>s</i>(<i>x</i><sub><i>b</i></sub>), where <i>C</i><i>o</i><i>m</i><i>p</i><i>r</i><i>e</i><i>s</i><i>s</i>(⋅) represents an image compression algorithm. For convenience, the compressed backdoor image is denoted as <i>x</i><sub><i>b</i><i>c</i></sub>. Then, in order to enhance the robustness of the backdoor attack, multiple types of compressed training data are used to train the DNN. In this paper, we consider three image compression algorithms, i.e., JPEG [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Wallace G.K. (1992) The JPEG still picture compression standard. IEEE Trans Consum Electron 38(1):30–44" href="/article/10.1007/s10489-023-04575-8#ref-CR3" id="ref-link-section-d136029957e1173">3</a>], JPEG2000 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Skodras A., Christopoulos C.A., Ebrahimi T. (2001) The JPEG 2000 still image compression standard. IEEE Signal Process Mag 18(5):36–58" href="/article/10.1007/s10489-023-04575-8#ref-CR4" id="ref-link-section-d136029957e1177">4</a>], and WEBP [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Ginesu G., Pintus M., Giusto D.D. (2012) Objective assessment of the WebP image coding algorithm. Signal Processing: Image Communication 27(8):867–874" href="/article/10.1007/s10489-023-04575-8#ref-CR5" id="ref-link-section-d136029957e1180">5</a>]. Different compression methods have different compression mechanisms. JPEG [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Wallace G.K. (1992) The JPEG still picture compression standard. IEEE Trans Consum Electron 38(1):30–44" href="/article/10.1007/s10489-023-04575-8#ref-CR3" id="ref-link-section-d136029957e1183">3</a>] uses discrete cosine transform (DCT) for image compression, while JPEG2000 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Skodras A., Christopoulos C.A., Ebrahimi T. (2001) The JPEG 2000 still image compression standard. IEEE Signal Process Mag 18(5):36–58" href="/article/10.1007/s10489-023-04575-8#ref-CR4" id="ref-link-section-d136029957e1186">4</a>] uses discrete wavelet transform (DWT), and WEBP [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Ginesu G., Pintus M., Giusto D.D. (2012) Objective assessment of the WebP image coding algorithm. Signal Processing: Image Communication 27(8):867–874" href="/article/10.1007/s10489-023-04575-8#ref-CR5" id="ref-link-section-d136029957e1189">5</a>] is based on VP8 video codec. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10489-023-04575-8#Fig3">3</a> shows some examples of backdoor images compressed by the above three image compression methods. Gaussian noise trigger [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Zhang J., Gu Z., Jang J., Wu H., Stoecklin M.P., Huang H., Molloy I.M. (2018) Protecting intellectual property of deep neural networks with watermarking. In: Proceedings of the Asia Conference on Computer and Communications Security , pp 159–172" href="/article/10.1007/s10489-023-04575-8#ref-CR22" id="ref-link-section-d136029957e1196">22</a>] (Trigger1), “TEST” logo trigger [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Zhang J., Gu Z., Jang J., Wu H., Stoecklin M.P., Huang H., Molloy I.M. (2018) Protecting intellectual property of deep neural networks with watermarking. In: Proceedings of the Asia Conference on Computer and Communications Security , pp 159–172" href="/article/10.1007/s10489-023-04575-8#ref-CR22" id="ref-link-section-d136029957e1199">22</a>] (Trigger2) and TrojanNN trigger [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Liu Y., Ma S., Aafer Y., Lee W., Zhai J., Wang W., Zhang X. (2018) Trojaning attack on neural networks. In: 25th Annual Network and Distributed System Security Symposium, pp 1–15" href="/article/10.1007/s10489-023-04575-8#ref-CR8" id="ref-link-section-d136029957e1202">8</a>] (Trigger3) are used in these examples.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3" data-title="Fig. 3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10489-023-04575-8/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10489-023-04575-8/MediaObjects/10489_2023_4575_Fig3_HTML.png?as=webp"><img aria-describedby="Fig3" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10489-023-04575-8/MediaObjects/10489_2023_4575_Fig3_HTML.png" alt="figure 3" loading="lazy" width="685" height="469"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Examples of compressed backdoor images</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10489-023-04575-8/figures/3" data-track-dest="link:Figure3 Full size image" aria-label="Full size image figure 3" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec8"><span class="c-article-section__title-number">3.3 </span>Backdoor embedding via feature consistency training</h3><p>We leverage feature consistency training [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Wan S., Wu T., Hsu H., Wong W.H., Lee C. (2020) Feature consistency training with JPEG compressed images. IEEE Trans Circuits Syst Video Technol 30(12):4769–4780" href="/article/10.1007/s10489-023-04575-8#ref-CR2" id="ref-link-section-d136029957e1230">2</a>] to embed the compression-resistant backdoor into the deep neural network.</p><p>Typically, a DNN can be represented as <i>y</i> = <i>F</i><sub><i>𝜃</i></sub>(<i>x</i>), where <i>x</i> is an input image, <i>y</i> is the prediction label with the maximal probability and <i>𝜃</i> denotes the model parameters. In addition, a DNN is usually composed of different layers, such as convolutional layer, fully connected layer, and so on. In this way, the DNN can be denoted as [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Wan S., Wu T., Hsu H., Wong W.H., Lee C. (2020) Feature consistency training with JPEG compressed images. IEEE Trans Circuits Syst Video Technol 30(12):4769–4780" href="/article/10.1007/s10489-023-04575-8#ref-CR2" id="ref-link-section-d136029957e1258">2</a>]:
</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ F_{\theta}(x) = {f_{1}}(x) \circ {f_{2}} \circ {\ldots} \circ {f_{m}} $$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>where <i>m</i> is the number of layers of DNN, and <i>f</i><sub><i>i</i></sub> (<span class="mathjax-tex">\(i=1,2,\dots ,m\)</span>) represents the <i>i</i> th layer of DNN. In the feature space, given an input <i>x</i> of DNN, the output of each layer can be represented as a feature extraction vector <i>E</i>(<i>x</i>). More specifically, the feature extraction vector <i>E</i><sub><i>i</i></sub>(<i>x</i>) outputted by the <i>i</i> th layer is denoted as [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Wan S., Wu T., Hsu H., Wong W.H., Lee C. (2020) Feature consistency training with JPEG compressed images. IEEE Trans Circuits Syst Video Technol 30(12):4769–4780" href="/article/10.1007/s10489-023-04575-8#ref-CR2" id="ref-link-section-d136029957e1439">2</a>]:
</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {E_{i}}(x) = {f_{1}}(x) \circ {f_{2}} \circ {\ldots} \circ {f_{i}}, \ i \le m-1. $$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div><p>Note that, since the last layer (the <i>m</i> layer) of a DNN is usually the output layer, it is not suitable for feature extraction.</p><p>The training goal of the proposed method is to minimize the distance between backdoor images and compressed backdoor images in the feature space. To this end, given two images <i>x</i><sub><i>b</i></sub> and <i>x</i><sub><i>b</i><i>c</i></sub>, we aim to ensure that the extracted features <i>E</i>(<i>x</i>) of these two images are similar, which can be denoted as <i>E</i>(<i>x</i><sub><i>b</i></sub>) ≈ <i>E</i>(<i>x</i><sub><i>b</i><i>c</i></sub>).</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10489-023-04575-8#Fig4">4</a> presents the overall flow of feature consistency training [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Wan S., Wu T., Hsu H., Wong W.H., Lee C. (2020) Feature consistency training with JPEG compressed images. IEEE Trans Circuits Syst Video Technol 30(12):4769–4780" href="/article/10.1007/s10489-023-04575-8#ref-CR2" id="ref-link-section-d136029957e1616">2</a>]. We first select several layers of DNN to extract the features of both normal backdoor images and compressed backdoor images. Inspired by the work [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Wan S., Wu T., Hsu H., Wong W.H., Lee C. (2020) Feature consistency training with JPEG compressed images. IEEE Trans Circuits Syst Video Technol 30(12):4769–4780" href="/article/10.1007/s10489-023-04575-8#ref-CR2" id="ref-link-section-d136029957e1619">2</a>], the last two layers (the last two layers or only the last layer, i.e., the <i>m</i> − 1 layer and the <i>m</i> − 2 layer, or the <i>m</i> − 1 layer only) of DNN are selected. The reason is that, the first few layers of DNN are more sensitive to high frequency features in images than the last few layers. Selecting the first few layers is unable to acquire robust features of images, while selecting the last few layers is effective in acquiring robust features [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Wan S., Wu T., Hsu H., Wong W.H., Lee C. (2020) Feature consistency training with JPEG compressed images. IEEE Trans Circuits Syst Video Technol 30(12):4769–4780" href="/article/10.1007/s10489-023-04575-8#ref-CR2" id="ref-link-section-d136029957e1632">2</a>]. After extracting image features, we utilize a feature consistency constraint to encourage DNN to learn the common features between a backdoor image and its compressed version. More specifically, a feature consistency loss <i>L</i><sub><i>F</i><i>C</i></sub> is added to the objective function of DNN to guide the training. The feature consistency loss <i>L</i><sub><i>F</i><i>C</i></sub> is calculated as follows [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Wan S., Wu T., Hsu H., Wong W.H., Lee C. (2020) Feature consistency training with JPEG compressed images. IEEE Trans Circuits Syst Video Technol 30(12):4769–4780" href="/article/10.1007/s10489-023-04575-8#ref-CR2" id="ref-link-section-d136029957e1651">2</a>]:
</p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \begin{aligned} {L_{FC}}({x_{b}},{x_{bc}}) = \lambda_{1} Dis({E_{m-1}}({x_{b}}),{E_{m-1}}({x_{bc}})) \\ + \lambda_{2} Dis({E_{m-2}}({x_{b}}),{E_{m-2}}({x_{bc}})) \end{aligned} $$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p>where <i>λ</i><sub>1</sub> and <i>λ</i><sub>2</sub> are two constants used to control the strength of the feature consistency constraint, which will be described in Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-023-04575-8#Sec10">4.1</a>. <i>D</i><i>i</i><i>s</i> represents the distance metric, which is used to measure the difference between two images in the feature space. We use <i>L</i><sub>2</sub>-<i>distance</i> to calculate the distance between a backdoor image and its compressed version. In this way, the distance metric <i>D</i><i>i</i><i>s</i>(<i>E</i>(<i>x</i><sub><i>b</i></sub>),<i>E</i>(<i>x</i><sub><i>b</i><i>c</i></sub>)) is calculated as follows:
</p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ Dis({E}({x_{b}}),{E}({x_{bc}}))=\parallel{E}({x_{b}})-{E}({x_{bc}}){\parallel_{2}} $$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div><p>where <i>x</i><sub><i>b</i></sub> and <i>x</i><sub><i>b</i><i>c</i></sub> denote the backdoor instance and its compressed version, <i>E</i>(<i>x</i><sub><i>b</i></sub>) and <i>E</i>(<i>x</i><sub><i>b</i><i>c</i></sub>) denote the extracted features of <i>x</i><sub><i>b</i></sub> and <i>x</i><sub><i>b</i><i>c</i></sub>, respectively. In the training process, we optimize the feature distance <i>D</i><i>i</i><i>s</i>(<i>E</i>(<i>x</i><sub><i>b</i></sub>),<i>E</i>(<i>x</i><sub><i>b</i><i>c</i></sub>)) by minimizing the feature consistency loss <i>L</i><sub><i>F</i><i>C</i></sub> to guide the model training. This makes the internal layers treat the feature of compressed backdoor images as a part of the feature of backdoor images.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4" data-title="Fig. 4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10489-023-04575-8/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10489-023-04575-8/MediaObjects/10489_2023_4575_Fig4_HTML.png?as=webp"><img aria-describedby="Fig4" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10489-023-04575-8/MediaObjects/10489_2023_4575_Fig4_HTML.png" alt="figure 4" loading="lazy" width="685" height="232"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>The overall flow of the feature consistency training [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Wan S., Wu T., Hsu H., Wong W.H., Lee C. (2020) Feature consistency training with JPEG compressed images. IEEE Trans Circuits Syst Video Technol 30(12):4769–4780" href="/article/10.1007/s10489-023-04575-8#ref-CR2" id="ref-link-section-d136029957e2246">2</a>]</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10489-023-04575-8/figures/4" data-track-dest="link:Figure4 Full size image" aria-label="Full size image figure 4" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>In this work, the final objective function of feature consistency training can be formulated as follows [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Wan S., Wu T., Hsu H., Wong W.H., Lee C. (2020) Feature consistency training with JPEG compressed images. IEEE Trans Circuits Syst Video Technol 30(12):4769–4780" href="/article/10.1007/s10489-023-04575-8#ref-CR2" id="ref-link-section-d136029957e2261">2</a>]:
</p><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ L({x_{b}},{x_{bc}}) = {L_{0}}({x_{b}}) + {L_{0}}({x_{bc}}) + \alpha {L_{FC}}({x_{b}},{x_{bc}}) $$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div><p>where <i>L</i><sub>0</sub> is the initial objective function for model training (we use the cross-entropy loss as <i>L</i><sub>0</sub>). <i>α</i> is a hyperparameter which determines the weight of the feature consistency loss in the final objective function.</p><p>After each iteration of the training, the gradient of the objective function is calculated. Then, the model parameters <i>𝜃</i> will be updated by the back-propagation [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Rumelhart D.E., Hinton G.E., Williams R.J. (1986) Learning representations by back-propagating errors. Nature 323(6088):533–536" href="/article/10.1007/s10489-023-04575-8#ref-CR20" id="ref-link-section-d136029957e2471">20</a>] algorithm. After the feature consistency training, a compression-resistant backdoor is embedded into the DNN.</p></div></div></section><section data-title="Experiment"><div class="c-article-section" id="Sec9-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec9"><span class="c-article-section__title-number">4 </span>Experiment</h2><div class="c-article-section__content" id="Sec9-content"><h3 class="c-article__sub-heading" id="Sec10"><span class="c-article-section__title-number">4.1 </span>Experimental setup</h3>
                <h3 class="c-article__sub-heading" id="FPar1">Dataset and DNN models</h3>
                <p>In this work, CIFAR-10 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Krizhevsky A. (2009) Learning multiple layers of features from tiny images. Technical report, University of Toronto" href="/article/10.1007/s10489-023-04575-8#ref-CR23" id="ref-link-section-d136029957e2491">23</a>] and VGGFace [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Parkhi O.M., Vedaldi A., Zisserman A. (2015) Deep face recognition. In: Proceedings of the British Machine Vision Conference, pp 1–12" href="/article/10.1007/s10489-023-04575-8#ref-CR24" id="ref-link-section-d136029957e2494">24</a>] datasets are used to evaluate the effectiveness of the proposed compression-resistant backdoor attack. There are ten categories (6,000 images for each category) in CIFAR-10 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Krizhevsky A. (2009) Learning multiple layers of features from tiny images. Technical report, University of Toronto" href="/article/10.1007/s10489-023-04575-8#ref-CR23" id="ref-link-section-d136029957e2497">23</a>] dataset, where each class contains 5,000 training images and 1,000 test images. The size of each image is 32 × 32. There are over 2,600,000 face images in VGGFace [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Parkhi O.M., Vedaldi A., Zisserman A. (2015) Deep face recognition. In: Proceedings of the British Machine Vision Conference, pp 1–12" href="/article/10.1007/s10489-023-04575-8#ref-CR24" id="ref-link-section-d136029957e2500">24</a>] dataset. These face images are classified as 2622 different people. The size of each image is 224 × 224. 100 classes (100 training images and 20 test images for each class) of the VGGFace dataset are randomly selected for model training and testing.</p>
                <p>We perform the proposed compression-resist backdoor attack on AlexNet [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Krizhevsky A., Sutskever I., Hinton G.E. (2017) Imagenet classification with deep convolutional neural networks. Commun ACM 60(6):84–90" href="/article/10.1007/s10489-023-04575-8#ref-CR25" id="ref-link-section-d136029957e2506">25</a>] model, ResNet-18 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="He K., Zhang X., Ren S., Sun J. (2016) Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp 770–778" href="/article/10.1007/s10489-023-04575-8#ref-CR26" id="ref-link-section-d136029957e2509">26</a>] model and VGG-16 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Simonyan K., Zisserman A. (2015) Very deep convolutional networks for large-scale image recognition. In: 3rd International Conference on Learning Representations, pp 1–14" href="/article/10.1007/s10489-023-04575-8#ref-CR27" id="ref-link-section-d136029957e2512">27</a>] model respectively. AlexNet model consists of 5 convolutional layers and 3 fully connected layers. ResNet-18 model consists of 17 convolutional layers and 1 fully connected layer. VGG-16 model consists of 13 convolutional layers and 3 fully connected layers. The main architecture of the AlexNet model, ResNet-18 model and VGG-16 model is show in Tables <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10489-023-04575-8#Tab1">1</a>, <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10489-023-04575-8#Tab2">2</a> and <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10489-023-04575-8#Tab3">3</a> respectively. In the experiments, we train the AlexNet model and ResNet-18 model on CIFAR-10 dataset, and train the VGG-16 model on VGGFace dataset. The test accuracy of the clean AlexNet model, the clean ResNet-18 model and the clean VGG-16 model on clean test images is 84.40%, 84.36% and 96.30% respectively.
</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 The Main Structure of the AlexNet Model</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10489-023-04575-8/tables/1" aria-label="Full size table 1"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 The Main Structure of the ResNet-18 Model</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10489-023-04575-8/tables/2" aria-label="Full size table 2"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 The Main Structure of the VGG-16 Model</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10489-023-04575-8/tables/3" aria-label="Full size table 3"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              
                <h3 class="c-article__sub-heading" id="FPar2">Evaluation Metrics</h3>
                <p>We evaluate the performance of the proposed backdoor attack by using the following metrics. 
</p><ul class="u-list-style-dash">
                  <li>
                    <p>Test accuracy (<i>TA</i>) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Goodfellow I., Bengio Y., Courville A. (2016) Deep Learning. MIT press, Cambridge" href="/article/10.1007/s10489-023-04575-8#ref-CR28" id="ref-link-section-d136029957e4487">28</a>]. Given a batch of test images, the test accuracy denotes the percentage of images classified as the correct classes among all test images.</p>
                  </li>
                  <li>
                    <p>Injection rate (<i>IR</i>) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Wenger E., Passananti J., Bhagoji A.N., Yao Y., Zheng H., Zhao B.Y. (2021) Backdoor attacks against deep learning systems in the physical world. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 6206–6215" href="/article/10.1007/s10489-023-04575-8#ref-CR29" id="ref-link-section-d136029957e4499">29</a>] of the backdoor attack. In this paper, <i>IR</i> represents the proportion of backdoor instances in clean images. The <i>IR</i> is calculated by <span class="mathjax-tex">\(\frac {{{N_{b}}}}{N} \times 100\% \)</span>, where <i>N</i><sub><i>b</i></sub> denotes the number of backdoor instances (containing normal backdoor instances and compressed backdoor instances), <i>N</i> denotes the number of clean instances.</p>
                  </li>
                  <li>
                    <p>Attack success rate (<i>ASR</i>) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Chen X., Liu C., Li B., Lu K., Song D. (2017) Targeted backdoor attacks on deep learning systems using data poisoning. arXiv:&#xA;                1712.05526&#xA;                &#xA;              , 1–18" href="/article/10.1007/s10489-023-04575-8#ref-CR7" id="ref-link-section-d136029957e4574">7</a>]. This metric denotes the percentage of backdoor images that are classified as the target label among all backdoor images. Furthermore, we use <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i></sub>, <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i>2000</sub>, <i>A</i><i>S</i><i>R</i><sub><i>w</i><i>e</i><i>b</i><i>p</i></sub>, <i>A</i><i>S</i><i>R</i><sub><i>a</i><i>v</i>1</sub> and <i>A</i><i>S</i><i>R</i><sub><i>b</i><i>p</i><i>g</i></sub> to represent the attack success rate of backdoor attacks when images are compressed by JPEG [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Wallace G.K. (1992) The JPEG still picture compression standard. IEEE Trans Consum Electron 38(1):30–44" href="/article/10.1007/s10489-023-04575-8#ref-CR3" id="ref-link-section-d136029957e4655">3</a>], JPEG2000 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Skodras A., Christopoulos C.A., Ebrahimi T. (2001) The JPEG 2000 still image compression standard. IEEE Signal Process Mag 18(5):36–58" href="/article/10.1007/s10489-023-04575-8#ref-CR4" id="ref-link-section-d136029957e4658">4</a>], WEBP [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Ginesu G., Pintus M., Giusto D.D. (2012) Objective assessment of the WebP image coding algorithm. Signal Processing: Image Communication 27(8):867–874" href="/article/10.1007/s10489-023-04575-8#ref-CR5" id="ref-link-section-d136029957e4662">5</a>], AV1 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Chen Y., Mukherjee D., Han J., Grange A., Xu Y., Liu Z., Parker S., Chen C., Su H., Joshi U., Chiang C., Wang Y., Wilkins P., Bankoski J., Trudeau L.N., Egge N.E., Valin J., Davies T., Midtskogen S., Norkin A., Rivaz P.D. (2018) An overview of core coding tools in the AV1 video codec. In: Picture Coding Symposium, pp 41–45" href="/article/10.1007/s10489-023-04575-8#ref-CR30" id="ref-link-section-d136029957e4665">30</a>] and BPG [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Bellard F. (2022) BPG Image format. &#xA;                https://bellard.org/bpg&#xA;                &#xA;              " href="/article/10.1007/s10489-023-04575-8#ref-CR31" id="ref-link-section-d136029957e4668">31</a>] methods respectively.</p>
                  </li>
                </ul>
              
                <h3 class="c-article__sub-heading" id="FPar3">Backdoor Embedding Settings</h3>
                <p>In the proposed backdoor attack, the “Dog” class in CIFAR-10 dataset [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Krizhevsky A. (2009) Learning multiple layers of features from tiny images. Technical report, University of Toronto" href="/article/10.1007/s10489-023-04575-8#ref-CR23" id="ref-link-section-d136029957e4681">23</a>] and the “Aamir Khan” class in VGGFace dataset [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Parkhi O.M., Vedaldi A., Zisserman A. (2015) Deep face recognition. In: Proceedings of the British Machine Vision Conference, pp 1–12" href="/article/10.1007/s10489-023-04575-8#ref-CR24" id="ref-link-section-d136029957e4684">24</a>] is used as the target class, respectively. The backdoor trigger used in the experiments is Gaussian noise [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Zhang J., Gu Z., Jang J., Wu H., Stoecklin M.P., Huang H., Molloy I.M. (2018) Protecting intellectual property of deep neural networks with watermarking. In: Proceedings of the Asia Conference on Computer and Communications Security , pp 159–172" href="/article/10.1007/s10489-023-04575-8#ref-CR22" id="ref-link-section-d136029957e4687">22</a>], which is denoted as Trigger1 in this paper. The number of backdoor instances is 4,000 (the <i>IR</i> is 8%) on CIFAR-10 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Krizhevsky A. (2009) Learning multiple layers of features from tiny images. Technical report, University of Toronto" href="/article/10.1007/s10489-023-04575-8#ref-CR23" id="ref-link-section-d136029957e4693">23</a>] dataset and 400 (the <i>IR</i> is 4%) on VGGFace [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Parkhi O.M., Vedaldi A., Zisserman A. (2015) Deep face recognition. In: Proceedings of the British Machine Vision Conference, pp 1–12" href="/article/10.1007/s10489-023-04575-8#ref-CR24" id="ref-link-section-d136029957e4700">24</a>] dataset. For the common backdoor attack (as a baseline), all the backdoor instances are normal backdoor instances. For compression-resistant backdoor attack, backdoor instances consist of a number of normal backdoor instances and the compressed backdoor instances. On CIFAR-10 dataset, 1,000 normal backdoor instances and 3,000 compressed backdoor instances are injected into the training set. On VGGFace dataset, 100 normal backdoor instances and 300 compressed backdoor instances are injected into the training set. In addition, we train the backdoor model for 100 epochs by using the stochastic gradient descent (SGD) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Zhang T. (2004) Solving large scale linear prediction problems using stochastic gradient descent algorithms. In: International Conference on Machine Learning, pp 1–8" href="/article/10.1007/s10489-023-04575-8#ref-CR32" id="ref-link-section-d136029957e4703">32</a>] optimizer. The initial learning rate is set to 0.1, and then it is set to 0.01 and 0.001 at 40 and 70 epochs respectively. The ablation study of the training epochs and the initial learning rate will be discussed in Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-023-04575-8#Sec12">4.3</a>.</p>
                <p>For the AlexNet [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Krizhevsky A., Sutskever I., Hinton G.E. (2017) Imagenet classification with deep convolutional neural networks. Commun ACM 60(6):84–90" href="/article/10.1007/s10489-023-04575-8#ref-CR25" id="ref-link-section-d136029957e4712">25</a>] model (which has three fully connected layers), we use the second fully connected layer (i.e., the <i>m</i> − 1 layer) to extract image features. Accordingly, the hyperparameter <i>λ</i><sub>1</sub> is set to 1 and <i>λ</i><sub>2</sub> is set to 0. For the ResNet-18 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="He K., Zhang X., Ren S., Sun J. (2016) Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp 770–778" href="/article/10.1007/s10489-023-04575-8#ref-CR26" id="ref-link-section-d136029957e4727">26</a>] model (which only has one fully connected layer), we utilize the pooling layer (i.e., the <i>m</i> − 1 layer) before the fully connected layer to extract image features. Accordingly, the hyperparameter <i>λ</i><sub>1</sub> is set to 1 and <i>λ</i><sub>2</sub> is set to 0. For the VGG-16 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Simonyan K., Zisserman A. (2015) Very deep convolutional networks for large-scale image recognition. In: 3rd International Conference on Learning Representations, pp 1–14" href="/article/10.1007/s10489-023-04575-8#ref-CR27" id="ref-link-section-d136029957e4741">27</a>] model (which has three fully connected layers), we utilize the first two fully connected layers (i.e., the <i>m</i> − 2 layer and the <i>m</i> − 1 layer) to extract image features. Accordingly, both the hyperparameters <i>λ</i><sub>1</sub> and <i>λ</i><sub>2</sub> in equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10489-023-04575-8#Equ4">4</a>) are set to 0.5. For the three models, we follow the settings in work [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Wan S., Wu T., Hsu H., Wong W.H., Lee C. (2020) Feature consistency training with JPEG compressed images. IEEE Trans Circuits Syst Video Technol 30(12):4769–4780" href="/article/10.1007/s10489-023-04575-8#ref-CR2" id="ref-link-section-d136029957e4763">2</a>] to set the hyperparameter <i>α</i> in equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10489-023-04575-8#Equ6">6</a>) to 0.1. The values of the hyperparameter <i>α</i> is discussed in Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-023-04575-8#Sec12">4.3</a> in detail.</p>
              
                <h3 class="c-article__sub-heading" id="FPar4">Image Compression</h3>
                <p>In the experiments, we use three widely used image compression methods, i.e., JPEG [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Wallace G.K. (1992) The JPEG still picture compression standard. IEEE Trans Consum Electron 38(1):30–44" href="/article/10.1007/s10489-023-04575-8#ref-CR3" id="ref-link-section-d136029957e4786">3</a>], JPEG2000 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Skodras A., Christopoulos C.A., Ebrahimi T. (2001) The JPEG 2000 still image compression standard. IEEE Signal Process Mag 18(5):36–58" href="/article/10.1007/s10489-023-04575-8#ref-CR4" id="ref-link-section-d136029957e4789">4</a>], and WEBP [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Ginesu G., Pintus M., Giusto D.D. (2012) Objective assessment of the WebP image coding algorithm. Signal Processing: Image Communication 27(8):867–874" href="/article/10.1007/s10489-023-04575-8#ref-CR5" id="ref-link-section-d136029957e4792">5</a>], to compress backdoor images. We leverage a Python library, named Pillow<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup>, to implement the above three image compressions. In this library, the compression level of both JPEG [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Wallace G.K. (1992) The JPEG still picture compression standard. IEEE Trans Consum Electron 38(1):30–44" href="/article/10.1007/s10489-023-04575-8#ref-CR3" id="ref-link-section-d136029957e4806">3</a>] and WEBP [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Ginesu G., Pintus M., Giusto D.D. (2012) Objective assessment of the WebP image coding algorithm. Signal Processing: Image Communication 27(8):867–874" href="/article/10.1007/s10489-023-04575-8#ref-CR5" id="ref-link-section-d136029957e4810">5</a>] is determined by the parameter <i>quality</i>, which ranges from 0 to 100. The smaller the value of <i>quality</i>, the larger the loss of image quality (i.e., the higher the compression level). For JPEG2000 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Skodras A., Christopoulos C.A., Ebrahimi T. (2001) The JPEG 2000 still image compression standard. IEEE Signal Process Mag 18(5):36–58" href="/article/10.1007/s10489-023-04575-8#ref-CR4" id="ref-link-section-d136029957e4819">4</a>], the compression level is determined by the parameter <i>quality layers</i>. The larger the <i>quality layers</i>, the larger the loss of image quality (i.e., the higher the compression level). In this work, the <i>quality</i> of JPEG and WEBP is set to 50. The <i>quality layers</i> of JPEG2000 is set to 30.</p>
              <h3 class="c-article__sub-heading" id="Sec11"><span class="c-article-section__title-number">4.2 </span>Experimental results</h3><p>The performance of the common backdoor attack and the proposed compression-resistant attack is shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10489-023-04575-8#Tab4">4</a>. It is shown that before image compression, the attack success rate (<i>ASR</i>) of the common backdoor attack is up to 100%. When the backdoor images are compressed, the <i>ASR</i> of the common backdoor attack (Gaussian noise) is significantly decreased. However, after image compression, the <i>ASR</i> of the proposed compression-resistant backdoor attack is still very high. Specifically, on the AlexNet [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Krizhevsky A., Sutskever I., Hinton G.E. (2017) Imagenet classification with deep convolutional neural networks. Commun ACM 60(6):84–90" href="/article/10.1007/s10489-023-04575-8#ref-CR25" id="ref-link-section-d136029957e4856">25</a>] model, the <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i></sub>, <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i>2000</sub> and <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i></sub> of the common backdoor attack is only 11.98%, 24.13%, 3.70% respectively, while the <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i></sub>, <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i>2000</sub> and <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i></sub> of the proposed compression-resistant backdoor attack is 93.16%, 94.11%, 96.54% respectively. On the ResNet-18 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="He K., Zhang X., Ren S., Sun J. (2016) Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp 770–778" href="/article/10.1007/s10489-023-04575-8#ref-CR26" id="ref-link-section-d136029957e4960">26</a>] model, the <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i></sub>, <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i>2000</sub> and <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i></sub> of the common backdoor attack is only 6.63%, 6.20%, 3.97% respectively, while the <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i></sub>, <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i>2000</sub> and <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i></sub> of the proposed compression-resistant backdoor attack is 98.77%, 97.69%, 98.93% respectively. On the VGG-16 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Simonyan K., Zisserman A. (2015) Very deep convolutional networks for large-scale image recognition. In: 3rd International Conference on Learning Representations, pp 1–14" href="/article/10.1007/s10489-023-04575-8#ref-CR27" id="ref-link-section-d136029957e5064">27</a>] model, after image compression, the <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i></sub>, <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i>2000</sub> and <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i></sub> of common backdoor attack is only 11.45%, 14.60% and 12.85% respectively. As a comparison, after image compression, the <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i></sub>, <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i>2000</sub> and <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i></sub> of the compression-resistant backdoor attack is 81.75%, 98.45% and 98.50% respectively. In addition, our compression-resistant backdoor attack will not affect the normal performance of the models. The test accuracy of backdoored AlexNet model, ResNet-18 model and VGG-16 model on clean images is 83.91%, 83.41% and 96.10%, which is similar to the test accuracy of clean AlexNet model (84.40%), ResNet-18 model (84.36%) and VGG-16 model (96.30%). The experimental results demonstrate that our proposed backdoor attack method can ensure the robustness of the backdoor attack against image compressions, while not degrading the performance of the DNN model.
</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-4"><figure><figcaption class="c-article-table__figcaption"><b id="Tab4" data-test="table-caption">Table 4 The Performance of the Common Backdoor Attack and the Proposed Compression-Resistant Backdoor Attack on the AlexNet model, ResNet-18 model and VGG-16 model</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10489-023-04575-8/tables/4" aria-label="Full size table 4"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="FPar5">Comparison between Adversarial Training and Feature Consistency Training</h3>
                <p>In general, adversarial training [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Tramèr F., Kurakin A., Papernot N., Goodfellow I.J., Boneh D., McDaniel P.D. (2018) Ensemble adversarial training: Attacks and defenses. In: 6th International Conference on Learning Representations, pp 1–20" href="/article/10.1007/s10489-023-04575-8#ref-CR33" id="ref-link-section-d136029957e5571">33</a>] is widely used to defend against adversarial example attacks. There are also a few works [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Gao Y., Wu D., Zhang J., Gan G., Xia S., Niu G., Sugiyama M. (2022) On the effectiveness of adversarial training against backdoor attacks. arXiv:&#xA;                2202.10627&#xA;                &#xA;              , 1–12" href="/article/10.1007/s10489-023-04575-8#ref-CR34" id="ref-link-section-d136029957e5574">34</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Geiping J., Fowl L., Somepalli G., Goldblum M., Moeller M., Goldstein T. (2021) What doesn’t kill you makes you robust(er): Adversarial training against poisons and backdoors. arXiv:&#xA;                2102.13624&#xA;                &#xA;              , 1–25" href="/article/10.1007/s10489-023-04575-8#ref-CR35" id="ref-link-section-d136029957e5577">35</a>] utilize adversarial training to defend against backdoor attacks. In this experiment, we use adversarial training for the opposite purpose of existing works, which aims to enhance the robustness of the backdoor attacks against image compressions. The performance of adversarial training is used as a baseline to evaluate the effectiveness of feature consistency training on the compression-resistant backdoor attack. In this experiment, we use Trigger1 (i.e., Guassian noise [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Zhang J., Gu Z., Jang J., Wu H., Stoecklin M.P., Huang H., Molloy I.M. (2018) Protecting intellectual property of deep neural networks with watermarking. In: Proceedings of the Asia Conference on Computer and Communications Security , pp 159–172" href="/article/10.1007/s10489-023-04575-8#ref-CR22" id="ref-link-section-d136029957e5580">22</a>]) to generate backdoor instances on CIFAR-10 dataset. Then, the generated backdoor instances are compressed by JPEG [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Wallace G.K. (1992) The JPEG still picture compression standard. IEEE Trans Consum Electron 38(1):30–44" href="/article/10.1007/s10489-023-04575-8#ref-CR3" id="ref-link-section-d136029957e5583">3</a>], JPEG2000 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Skodras A., Christopoulos C.A., Ebrahimi T. (2001) The JPEG 2000 still image compression standard. IEEE Signal Process Mag 18(5):36–58" href="/article/10.1007/s10489-023-04575-8#ref-CR4" id="ref-link-section-d136029957e5587">4</a>] and WEBP [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Ginesu G., Pintus M., Giusto D.D. (2012) Objective assessment of the WebP image coding algorithm. Signal Processing: Image Communication 27(8):867–874" href="/article/10.1007/s10489-023-04575-8#ref-CR5" id="ref-link-section-d136029957e5590">5</a>] methods respectively. In the training stage, 4,000 (<i>IR</i> is 8%) backdoor instances (including 1,000 normal backdoor instances and 3,000 compressed backdoor instances) are injected into the training set. Finally, we use feature consistency training and adversarial training to train the ResNet-18 model, respectively.</p>
                <p>As shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10489-023-04575-8#Tab5">5</a>, compared with the DNN model after adversarial training, when the model is trained with feature consistency training, the compression-resistant backdoor attack achieves a higher success rate. Specifically, after adversarial training, the <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i></sub>, <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i>2000</sub> and <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i></sub> of the compressed backdoor instances is 96.76%, 94.30% and 96.72% respectively. After the feature consistency training, the <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i></sub>, <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i>2000</sub> and <i>A</i><i>S</i><i>R</i><sub><i>w</i><i>e</i><i>b</i><i>p</i></sub> of the compression-resistant backdoor attack is 98.77%, 97.69%, 98.93% respectively. In conclusion, experimental results demonstrate that feature consistency training performs better than adversarial training in terms of the robustness of backdoor attacks against image compression.
</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-5"><figure><figcaption class="c-article-table__figcaption"><b id="Tab5" data-test="table-caption">Table 5 Comparison between Adversarial Training and Feature Consistency Training</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10489-023-04575-8/tables/5" aria-label="Full size table 5"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <h3 class="c-article__sub-heading" id="Sec12"><span class="c-article-section__title-number">4.3 </span>Parameters discussion</h3><p>In this section, we discuss the attack performance of compression-resistant backdoor attack from the following six aspects: different types of backdoor triggers, image compressions with different <i>quality</i>, different backdoor injection rates, different training epochs, different initial learning rates, and different values of hyper-parameter <i>α</i>.</p>
                <h3 class="c-article__sub-heading" id="FPar6">Different Types of Backdoor Triggers</h3>
                <p>Gaussian noise trigger [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Zhang J., Gu Z., Jang J., Wu H., Stoecklin M.P., Huang H., Molloy I.M. (2018) Protecting intellectual property of deep neural networks with watermarking. In: Proceedings of the Asia Conference on Computer and Communications Security , pp 159–172" href="/article/10.1007/s10489-023-04575-8#ref-CR22" id="ref-link-section-d136029957e5938">22</a>] (Trigger1), “TEST” logo trigger [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Zhang J., Gu Z., Jang J., Wu H., Stoecklin M.P., Huang H., Molloy I.M. (2018) Protecting intellectual property of deep neural networks with watermarking. In: Proceedings of the Asia Conference on Computer and Communications Security , pp 159–172" href="/article/10.1007/s10489-023-04575-8#ref-CR22" id="ref-link-section-d136029957e5941">22</a>] (Trigger2) and TrojanNN trigger [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Liu Y., Ma S., Aafer Y., Lee W., Zhai J., Wang W., Zhang X. (2018) Trojaning attack on neural networks. In: 25th Annual Network and Distributed System Security Symposium, pp 1–15" href="/article/10.1007/s10489-023-04575-8#ref-CR8" id="ref-link-section-d136029957e5944">8</a>] (Trigger3) are used to conduct the backdoor attack respectively. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10489-023-04575-8#Fig5">5</a> shows some example images of the generated backdoor instances on VGGFace [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Parkhi O.M., Vedaldi A., Zisserman A. (2015) Deep face recognition. In: Proceedings of the British Machine Vision Conference, pp 1–12" href="/article/10.1007/s10489-023-04575-8#ref-CR24" id="ref-link-section-d136029957e5950">24</a>] dataset.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5" data-title="Fig. 5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10489-023-04575-8/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10489-023-04575-8/MediaObjects/10489_2023_4575_Fig5_HTML.png?as=webp"><img aria-describedby="Fig5" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10489-023-04575-8/MediaObjects/10489_2023_4575_Fig5_HTML.png" alt="figure 5" loading="lazy" width="685" height="684"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Example images of three types of backdoor instances on VGGFace dataset</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10489-023-04575-8/figures/5" data-track-dest="link:Figure5 Full size image" aria-label="Full size image figure 5" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10489-023-04575-8#Tab6">6</a> presents the performance of the normal backdoor attack and compression-resistant backdoor attack against the ResNet-18 model on CIFAR-10 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Krizhevsky A. (2009) Learning multiple layers of features from tiny images. Technical report, University of Toronto" href="/article/10.1007/s10489-023-04575-8#ref-CR23" id="ref-link-section-d136029957e5977">23</a>] dataset when using different backdoor triggers. It is shown that, the compression-resistant backdoor attack performs well with different triggers even if the backdoor instances are compressed. For instance, after the JPEG compression, the <i>ASR</i> of the compression-resistant backdoor attack is 98.77% (using Trigger1), 99.39% (using Trigger2), and 99.68% (using Trigger3) respectively. As a comparison, when backdoor instances are compressed, the common backdoor attack performs poorly. After the JPEG compression, the <i>ASR</i> of the common backdoor attack is 6.63% (using Trigger1), 85.58% (using Trigger2), 54.73% (using Trigger3), respectively. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10489-023-04575-8#Tab7">7</a> shows the performance of the normal backdoor attack and the compression-resistant backdoor attack against the VGG-16 model on VGGFace [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Parkhi O.M., Vedaldi A., Zisserman A. (2015) Deep face recognition. In: Proceedings of the British Machine Vision Conference, pp 1–12" href="/article/10.1007/s10489-023-04575-8#ref-CR24" id="ref-link-section-d136029957e5990">24</a>] dataset when using different backdoor triggers. It is shown that when backdoor attacks are launched by using different backdoor triggers, the proposed method is still able to ensure the robustness of backdoor attacks against image compressions. More specifically, after the JPEG compression, the <i>ASR</i> of the compression-resistant backdoor attack is 81.75% (using Trigger1), 99.45% (using Trigger2), 99.70% (using Trigger3) respectively. In conclusion, the proposed method significantly improves the performance of backdoor attacks against image compressions.
</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-6"><figure><figcaption class="c-article-table__figcaption"><b id="Tab6" data-test="table-caption">Table 6 Performance of the Common Backdoor Attack and the Proposed Compression-Resistant Backdoor Attack against ResNet-18 Model Using Three Different Backdoor Triggers Respectively on CIFAR-10 Dataset</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10489-023-04575-8/tables/6" aria-label="Full size table 6"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-7"><figure><figcaption class="c-article-table__figcaption"><b id="Tab7" data-test="table-caption">Table 7 Performance of the Common Backdoor Attack and the Proposed Compression-Resistant Backdoor Attack against VGG-16 Model Using Three Different Backdoor Triggers Respectively on VGGFace Dataset</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10489-023-04575-8/tables/7" aria-label="Full size table 7"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              
                <h3 class="c-article__sub-heading" id="FPar7">Image Compression with Different Compression Quality</h3>
                <p>We also evaluate the influence of different compression quality of JPEG compression on the proposed backdoor attack on the ResNet-18 model (on the CIFAR-10 dataset) and VGG-16 model (on the VGGFace dataset) respectively. The <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i></sub> of the proposed backdoor attack under JPEG compression with different compression quality is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10489-023-04575-8#Fig6">6</a>.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6" data-title="Fig. 6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10489-023-04575-8/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10489-023-04575-8/MediaObjects/10489_2023_4575_Fig6_HTML.png?as=webp"><img aria-describedby="Fig6" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10489-023-04575-8/MediaObjects/10489_2023_4575_Fig6_HTML.png" alt="figure 6" loading="lazy" width="685" height="292"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>The <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i></sub> of compression-resistant backdoor attack under JPEG compression with different <i>quality</i>: (a) On ResNet-18 model; (b) On VGG-16 model</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10489-023-04575-8/figures/6" data-track-dest="link:Figure6 Full size image" aria-label="Full size image figure 6" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10489-023-04575-8#Fig6">6</a>, as the <i>quality</i> of JPEG compression increases, the <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i></sub> of the compression-resistant backdoor attack increases gradually. On the ResNet-18 model, both the compression-resistant backdoor attack using Gaussian noise and the compression-resistant backdoor attack using “TEST” logo have an excellent performance. When the <i>quality</i> of JPEG compression changes, the <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i></sub> of the compression-resistant backdoor attack using Gaussian noise and that using “TEST” logo both remain very high. The compression-resistant backdoor attack using TrojanNN trigger also obtains a high <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i></sub> when the <i>quality</i> of JPEG compression is higher than 30. On the VGG-16 model, as the <i>quality</i> of JPEG compression becomes larger, <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i></sub> of the compression-resistant backdoor attack increases. When the <i>quality</i> of JPEG compression is higher than 50, the <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i></sub> of the compression-resistant backdoor attack is higher than 80% (using Gaussian noise), 99% (using “TEST” logo) and 99% (using TrojanNN trigger) respectively. Generally, in order to maintain the utility of images, the attacker will not compress backdoor images seriously (i.e., the attacker will set the <i>quality</i> of JPEG compression to be higher than 50). In conclusion, even if the backdoor instances are compressed by JPEG compression with different <i>quality</i>, the proposed compression-resistant backdoor attack can still be robust to JPEG compression.</p>
              
                <h3 class="c-article__sub-heading" id="FPar8">Different Backdoor Injection Rates</h3>
                <p>We randomly generate 1,000 normal backdoor instances and inject them into the training set on the CIFAR-10 dataset. Then, we inject 300, 600, 1200, 1800, 2400, 3000 compressed backdoor instances into the training set, respectively, to change the backdoor injection rate <i>IR</i>. In other words, the <i>IR</i> is 2.60%, 3.20%, 4.40%, 5.60%, 6.80%, 8.00% respectively. To the best of our knowledge, the backdoor injection rate in many literatures [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Gu T., Liu K., Dolan-Gavitt B., Garg S. (2019) BadNets: Evaluating backdooring attacks on deep neural networks. IEEE Access 7:47230–47244" href="/article/10.1007/s10489-023-04575-8#ref-CR6" id="ref-link-section-d136029957e6900">6</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Cheng S., Liu Y., Ma S., Zhang X. (2021) Deep feature space Trojan attack of neural networks by controlled detoxification. In: 35th AAAI Conference on Artificial Intelligence, pp 1148–1156" href="/article/10.1007/s10489-023-04575-8#ref-CR17" id="ref-link-section-d136029957e6903">17</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Sarkar E., Benkraouda H., Maniatakos M. (2020) FaceHack: Triggering backdoored facial recognition systems using facial characteristics. arXiv:&#xA;                2006.11623&#xA;                &#xA;              , 1–13" href="/article/10.1007/s10489-023-04575-8#ref-CR36" id="ref-link-section-d136029957e6906">36</a>] is greater than 11% (According to the calculation method in this paper, i.e., <span class="mathjax-tex">\(\frac {{{N_{b}}}}{N} \times 100\%\)</span>, as defined in Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-023-04575-8#Sec10">4.1</a>.). Thus, the backdoor injection rate in this paper is relatively lower.</p>
                <p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10489-023-04575-8#Tab8">8</a> presents the performance of the compression-resistant backdoor attack against the ResNet-18 model under different backdoor injection rates on CIFAR-10 dataset. It can be seen that, after image compression, the <i>ASR</i> will increase as the <i>IR</i> gradually increases. When the backdoor <i>IR</i> reaches 8.00%, after image compression, the <i>ASR</i> of the compression-resistant backdoor attack will be close to 99%. Even when the <i>IR</i> is only set to 3.20%, the <i>ASR</i> of the compression-resistant backdoor attack can still reach a very high value. More specifically, when <i>IR</i> is 3.20%, the <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i></sub>, <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i>2000</sub>, <i>A</i><i>S</i><i>R</i><sub><i>w</i><i>e</i><i>b</i><i>p</i></sub> of the compression-resistant backdoor attack is high up to 95.02%, 93.64%, 96.00%, respectively. In conclusion, the proposed backdoor attack is robust to image compression even with a small backdoor injection rate.
</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-8"><figure><figcaption class="c-article-table__figcaption"><b id="Tab8" data-test="table-caption">Table 8 Performance of the Compression-Resistant Backdoor Attack against the ResNet-18 Model under Different Backdoor Injection Rates on CIFAR-10 Dataset</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10489-023-04575-8/tables/8" aria-label="Full size table 8"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              
                <h3 class="c-article__sub-heading" id="FPar9">Different Training Epochs</h3>
                <p>To evaluate the performance of the proposed backdoor attack under different training epochs, we train the model for 20, 40, 60, 80 and 100 epochs respectively during the training process. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10489-023-04575-8#Tab9">9</a> shows the performance of the compression-resistant backdoor attack against the ResNet-18 model under different training epochs on CIFAR-10 dataset.
</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-9"><figure><figcaption class="c-article-table__figcaption"><b id="Tab9" data-test="table-caption">Table 9 The Performance of the Compression-Resistant Backdoor Attack against the ResNet-18 Model under Different Training Epochs on CIFAR-10 Dataset</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10489-023-04575-8/tables/9" aria-label="Full size table 9"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>In Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10489-023-04575-8#Tab9">9</a>, as the training epoch increases, the <i>TA</i>, <i>ASR</i>, <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i></sub>, <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i>2000</sub> and <i>A</i><i>S</i><i>R</i><sub><i>w</i><i>e</i><i>b</i><i>p</i></sub> increase gradually. Specifically, after training the model for 60, 80 and 100 epochs, the <i>TA</i> is 83.22%, 83.44% and 83.41% respectively, which indicates that the variation of <i>TA</i> is negligible. In addition, after the model is trained for 60, 80 and 100 epochs respectively, the <i>ASR</i> is 97.56%, 97.84% and 99.03% respectively, and the <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i></sub>, <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i>2000</sub> and <i>A</i><i>S</i><i>R</i><sub><i>w</i><i>e</i><i>b</i><i>p</i></sub> are greater than 97.85%, 95.52% and 97.53% respectively. The experimental results show that the change of <i>TA</i> is negligible after training the model for more than 80 epochs, and the <i>ASR</i> is close to 100% when the model is trained for 100 epochs. Thus, 100 is a suitable value for the training epochs.</p>
              
                <h3 class="c-article__sub-heading" id="FPar10">Different Initial Learning Rates</h3>
                <p>We evaluate the influence of different initial learning rates on the proposed backdoor attack. The initial learning rate is set to 0.01, 0.1 and 1 respectively. The learning rate is multiplied by 0.1 at 40 and 70 epochs. For example, when the initial learning rate is set to 0.1, the learning rate will be 0.01 and 0.001 at 40 and 70 epochs respectively. The training process is terminated at 100 epochs.</p>
                <p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10489-023-04575-8#Tab10">10</a> presents the performance of the compression-resistant backdoor attack against the ResNet-18 model under different initial learning rates on CIFAR-10 dataset. When the initial learning rate is set to 0.1, compared with the other two initial learning rate settings, the <i>TA</i> is the highest, and the <i>ASR</i> of the compression-resistant backdoor attack is also the highest. Specifically, when the initial learning rate is set to 0.1, the <i>TA</i> is 83.41%, and the <i>ASR</i>, <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i></sub>, <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i>2000</sub> and <i>A</i><i>S</i><i>R</i><sub><i>w</i><i>e</i><i>b</i><i>p</i></sub> is 99.03%, 98.77%, 97.69% and 98.93% respectively. It is demonstrated that 0.1 is an appropriate setting for initial learning rate.
</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-10"><figure><figcaption class="c-article-table__figcaption"><b id="Tab10" data-test="table-caption">Table 10 The Performance of the Compression-Resistant Backdoor Attack against the ResNet-18 Model under Different Initial Learning Rates on CIFAR-10 Dataset</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10489-023-04575-8/tables/10" aria-label="Full size table 10"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              
                <h3 class="c-article__sub-heading" id="FPar11">Different Values of Hyper-parameter <i>α</i>
                        </h3>
                <p> During the feature consistency training, the hyper-parameter <i>α</i> determines the weight of the feature consistency loss in the final objective function. In the above experiments, we initialize <i>α</i> as 0.1. To investigate the impact of <i>α</i> on the compression-resistant backdoor attack, we evaluate the <i>TA</i> and <i>ASR</i> with different values of the hyper-parameter <i>α</i>. Specifically, <i>α</i> is set to 0.01, 0.05, 0.1, 0.5 and 1 respectively.</p>
                <p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10489-023-04575-8#Tab11">11</a> reports the performance of the compression-resistant backdoor attack against the ResNet-18 model under different values of the hyper-parameter <i>α</i> on CIFAR-10 dataset. It can be seen that, when <i>α</i> is set to 0.1, the <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i></sub>, <i>A</i><i>S</i><i>R</i><sub><i>j</i><i>p</i><i>e</i><i>g</i>2000</sub> and <i>A</i><i>S</i><i>R</i><sub><i>w</i><i>e</i><i>b</i><i>p</i></sub> are 98.77%, 97.69% and 98.93% respectively. However, when <i>α</i> is smaller than 0.1, after image compression, the <i>ASR</i> of the compressed backdoor instances is less than 97.01%. When <i>α</i> is larger than 0.1, the <i>ASR</i> of the compressed backdoor instances is less than 94%. The experimental results demonstrate that 0.1 is a suitable value for the hyper-parameter <i>α</i>.
</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-11"><figure><figcaption class="c-article-table__figcaption"><b id="Tab11" data-test="table-caption">Table 11 The Performance of the Compression-Resistant Backdoor Attack against the ResNet-18 Model under Different Values of Hyper-parameter <i>α</i> on CIFAR-10 Dataset</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10489-023-04575-8/tables/11" aria-label="Full size table 11"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <h3 class="c-article__sub-heading" id="Sec13"><span class="c-article-section__title-number">4.4 </span>Generalization ability of the proposed compression-resistant backdoor attack</h3><p>In this section, we discuss the generalization ability of the proposed compression-resistant backdoor attack on unseen image compression methods. The backdoor trigger used in this attack is Guassian noise trigger [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Zhang J., Gu Z., Jang J., Wu H., Stoecklin M.P., Huang H., Molloy I.M. (2018) Protecting intellectual property of deep neural networks with watermarking. In: Proceedings of the Asia Conference on Computer and Communications Security , pp 159–172" href="/article/10.1007/s10489-023-04575-8#ref-CR22" id="ref-link-section-d136029957e8616">22</a>] (Trigger1). In the training stage, we use normal backdoor images and compressed backdoor images (compressed by only one kind of image compression) to train the DNN to conduct the compression-resistant backdoor attack. In the test stage, five different image compression methods (i.e., JPEG [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Wallace G.K. (1992) The JPEG still picture compression standard. IEEE Trans Consum Electron 38(1):30–44" href="/article/10.1007/s10489-023-04575-8#ref-CR3" id="ref-link-section-d136029957e8619">3</a>], JPEG2000 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Skodras A., Christopoulos C.A., Ebrahimi T. (2001) The JPEG 2000 still image compression standard. IEEE Signal Process Mag 18(5):36–58" href="/article/10.1007/s10489-023-04575-8#ref-CR4" id="ref-link-section-d136029957e8622">4</a>], WEBP [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Ginesu G., Pintus M., Giusto D.D. (2012) Objective assessment of the WebP image coding algorithm. Signal Processing: Image Communication 27(8):867–874" href="/article/10.1007/s10489-023-04575-8#ref-CR5" id="ref-link-section-d136029957e8625">5</a>], AV1 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Chen Y., Mukherjee D., Han J., Grange A., Xu Y., Liu Z., Parker S., Chen C., Su H., Joshi U., Chiang C., Wang Y., Wilkins P., Bankoski J., Trudeau L.N., Egge N.E., Valin J., Davies T., Midtskogen S., Norkin A., Rivaz P.D. (2018) An overview of core coding tools in the AV1 video codec. In: Picture Coding Symposium, pp 41–45" href="/article/10.1007/s10489-023-04575-8#ref-CR30" id="ref-link-section-d136029957e8628">30</a>], BPG [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Bellard F. (2022) BPG Image format. &#xA;                https://bellard.org/bpg&#xA;                &#xA;              " href="/article/10.1007/s10489-023-04575-8#ref-CR31" id="ref-link-section-d136029957e8632">31</a>]) are utilized to evaluate the performance of the compression-resistant backdoor attack. In this section, the <i>quality</i> of JPEG, WEBP and AV1 is set to 50. The <i>quality</i> of BPG is set to 25. The <i>quality layers</i> of JPEG2000 is set to 30.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10489-023-04575-8#Fig7">7</a> shows the results of the compression-resistant backdoor attack when facing unseen image compression methods. When only one compression method is used during the training, the backdoor attack can resist multiple unseen image compression methods. For example, when only the JPEG2000 compression is used during the training, the backdoor attack still shows robustness to unseen JPEG, WEBP, AV1 and BPG compression methods in the test stage. Specifically, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10489-023-04575-8#Fig7">7</a>(a), when the backdoor attack is performed on the ResNet-18 model on the CIFAR-10 dataset, the <i>A</i><i>S</i><i>R</i><sub><i>a</i><i>v</i>1</sub> of the compression-resistant backdoor attack is 88.7%. As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10489-023-04575-8#Fig7">7</a>(b), when the backdoor attack is performed on the VGG-16 model on the VGGFace dataset, the <i>A</i><i>S</i><i>R</i><sub><i>a</i><i>v</i>1</sub> of the compression-resistant backdoor attack is 78.3%. In summary, the proposed compression-resistant backdoor attack has good generalization ability to multiple unseen image compression methods.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7" data-title="Fig. 7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10489-023-04575-8/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10489-023-04575-8/MediaObjects/10489_2023_4575_Fig7_HTML.png?as=webp"><img aria-describedby="Fig7" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10489-023-04575-8/MediaObjects/10489_2023_4575_Fig7_HTML.png" alt="figure 7" loading="lazy" width="685" height="1047"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Generalization ability of the proposed compression-resistant backdoor attack on unseen image compression methods: (a) on ResNet-18 model; (b) on VGG-16 model. The horizontal axis indicates that only one kind of image compression method is used in the feature consistency training</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10489-023-04575-8/figures/7" data-track-dest="link:Figure7 Full size image" aria-label="Full size image figure 7" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec14"><span class="c-article-section__title-number">4.5 </span>Robustness against backdoor defenses</h3><p>The performance of the proposed compression-resistant backdoor on resisting backdoor detection methods is the same as the uncompressed backdoor. The reasons are as follows. We do not design any loss function to optimize the performance of the backdoor attack resisting backdoor detection methods. Two loss functions including feature consistency loss and cross-entropy loss are used to optimize the training process: the feature consistency loss is used to improve the robustness against image compression, and the cross-entropy loss is used to keep the normal performance of the classification task. The proposed method only increases the robustness against image compressions, and does not change the robustness of backdoor attacks against backdoor defenses.</p></div></div></section><section data-title="Conclusion"><div class="c-article-section" id="Sec15-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec15"><span class="c-article-section__title-number">5 </span>Conclusion</h2><div class="c-article-section__content" id="Sec15-content"><p>Backdoor images transmitted on the Internet may be compressed by image compression methods. In this paper, we reveal that common backdoor attacks are vulnerable to image compressions. To this end, we propose a compression-resistant backdoor attack based on feature consistency training which can resist multiple image compression methods. Experimental results demonstrate that, under various parameter settings (e.g., different types of triggers, different <i>quality</i> of image compression, different backdoor injection rates, different training epochs, different initial learning rates, and different values of hyper-parameter <i>α</i>), the proposed backdoor attack is robust to image compressions. Furthermore, the proposed compression-resistant backdoor attack has good generalization ability to resist unseen image compression methods.</p></div></div></section>
                            </div>
                        
                    

                    <section data-title="Data Availability"><div class="c-article-section" id="data-availability-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="data-availability">Data Availability</h2><div class="c-article-section__content" id="data-availability-content">
            
            <p>The data are available from the corresponding author upon reasonable request.</p>
          </div></div></section><section data-title="Notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1" data-counter="1."><div class="c-article-footnote--listed__content"><p><a href="https://github.com/python-pillow/Pillow">https://github.com/python-pillow/Pillow</a></p></div></li></ol></div></div></section><div id="MagazineFulltextArticleBodySuffix"><section aria-labelledby="Bib1" data-title="References"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references" data-track-component="outbound reference"><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="1."><p class="c-article-references__text" id="ref-CR1">Wang Z., Guo H., Zhang Z., Song M., Zheng S., Wang Q., Niu B. (2020) Towards compression-resistant privacy-preserving photo sharing on social networks. In: the 21st ACM International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing, pp 81–90</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="2."><p class="c-article-references__text" id="ref-CR2">Wan S., Wu T., Hsu H., Wong W.H., Lee C. (2020) Feature consistency training with JPEG compressed images. IEEE Trans Circuits Syst Video Technol 30(12):4769–4780</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1109/TCSVT.2019.2959815" data-track-action="article reference" href="https://doi.org/10.1109%2FTCSVT.2019.2959815" aria-label="Article reference 2" data-doi="10.1109/TCSVT.2019.2959815">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 2" href="http://scholar.google.com/scholar_lookup?&amp;title=Feature%20consistency%20training%20with%20JPEG%20compressed%20images&amp;journal=IEEE%20Trans%20Circuits%20Syst%20Video%20Technol&amp;doi=10.1109%2FTCSVT.2019.2959815&amp;volume=30&amp;issue=12&amp;pages=4769-4780&amp;publication_year=2020&amp;author=Wan%2CS&amp;author=Wu%2CT&amp;author=Hsu%2CH&amp;author=Wong%2CWH&amp;author=Lee%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="3."><p class="c-article-references__text" id="ref-CR3">Wallace G.K. (1992) The JPEG still picture compression standard. IEEE Trans Consum Electron 38(1):30–44</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1109/30.125072" data-track-action="article reference" href="https://doi.org/10.1109%2F30.125072" aria-label="Article reference 3" data-doi="10.1109/30.125072">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 3" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20JPEG%20still%20picture%20compression%20standard&amp;journal=IEEE%20Trans%20Consum%20Electron&amp;doi=10.1109%2F30.125072&amp;volume=38&amp;issue=1&amp;pages=30-44&amp;publication_year=1992&amp;author=Wallace%2CGK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="4."><p class="c-article-references__text" id="ref-CR4">Skodras A., Christopoulos C.A., Ebrahimi T. (2001) The JPEG 2000 still image compression standard. IEEE Signal Process Mag 18(5):36–58</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1109/79.952804" data-track-action="article reference" href="https://doi.org/10.1109%2F79.952804" aria-label="Article reference 4" data-doi="10.1109/79.952804">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="math reference" href="http://www.emis.de/MATH-item?0990.68071" aria-label="MATH reference 4">MATH</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 4" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20JPEG%202000%20still%20image%20compression%20standard&amp;journal=IEEE%20Signal%20Process%20Mag&amp;doi=10.1109%2F79.952804&amp;volume=18&amp;issue=5&amp;pages=36-58&amp;publication_year=2001&amp;author=Skodras%2CA&amp;author=Christopoulos%2CCA&amp;author=Ebrahimi%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="5."><p class="c-article-references__text" id="ref-CR5">Ginesu G., Pintus M., Giusto D.D. (2012) Objective assessment of the WebP image coding algorithm. Signal Processing: Image Communication 27(8):867–874</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 5" href="http://scholar.google.com/scholar_lookup?&amp;title=Objective%20assessment%20of%20the%20WebP%20image%20coding%20algorithm&amp;journal=Signal%20Processing%3A%20Image%20Communication&amp;volume=27&amp;issue=8&amp;pages=867-874&amp;publication_year=2012&amp;author=Ginesu%2CG&amp;author=Pintus%2CM&amp;author=Giusto%2CDD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="6."><p class="c-article-references__text" id="ref-CR6">Gu T., Liu K., Dolan-Gavitt B., Garg S. (2019) BadNets: Evaluating backdooring attacks on deep neural networks. IEEE Access 7:47230–47244</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1109/ACCESS.2019.2909068" data-track-action="article reference" href="https://doi.org/10.1109%2FACCESS.2019.2909068" aria-label="Article reference 6" data-doi="10.1109/ACCESS.2019.2909068">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 6" href="http://scholar.google.com/scholar_lookup?&amp;title=BadNets%3A%20Evaluating%20backdooring%20attacks%20on%20deep%20neural%20networks&amp;journal=IEEE%20Access&amp;doi=10.1109%2FACCESS.2019.2909068&amp;volume=7&amp;pages=47230-47244&amp;publication_year=2019&amp;author=Gu%2CT&amp;author=Liu%2CK&amp;author=Dolan-Gavitt%2CB&amp;author=Garg%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="7."><p class="c-article-references__text" id="ref-CR7">Chen X., Liu C., Li B., Lu K., Song D. (2017) Targeted backdoor attacks on deep learning systems using data poisoning. arXiv:<a href="http://arxiv.org/abs/1712.05526">1712.05526</a>, 1–18</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="8."><p class="c-article-references__text" id="ref-CR8">Liu Y., Ma S., Aafer Y., Lee W., Zhai J., Wang W., Zhang X. (2018) Trojaning attack on neural networks. In: 25th Annual Network and Distributed System Security Symposium, pp 1–15</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="9."><p class="c-article-references__text" id="ref-CR9">Li S., Xue M., Zhao B.Z.H., Zhu H., Zhang X. (2021) Invisible backdoor attacks on deep neural networks via steganography and regularization. IEEE Trans Dependable Secure Comput 18(5):2088–2105</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 9" href="http://scholar.google.com/scholar_lookup?&amp;title=Invisible%20backdoor%20attacks%20on%20deep%20neural%20networks%20via%20steganography%20and%20regularization&amp;journal=IEEE%20Trans%20Dependable%20Secure%20Comput&amp;volume=18&amp;issue=5&amp;pages=2088-2105&amp;publication_year=2021&amp;author=Li%2CS&amp;author=Xue%2CM&amp;author=Zhao%2CBZH&amp;author=Zhu%2CH&amp;author=Zhang%2CX">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="10."><p class="c-article-references__text" id="ref-CR10">Zhang J., Chen D., Huang Q., Liao J., Zhang W., Feng H., Hua G., Yu N. (2022) Poison ink: Robust and invisible backdoor attack. IEEE Transactions on Image Processin 31:5691–5705</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1109/TIP.2022.3201472" data-track-action="article reference" href="https://doi.org/10.1109%2FTIP.2022.3201472" aria-label="Article reference 10" data-doi="10.1109/TIP.2022.3201472">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 10" href="http://scholar.google.com/scholar_lookup?&amp;title=Poison%20ink%3A%20Robust%20and%20invisible%20backdoor%20attack&amp;journal=IEEE%20Transactions%20on%20Image%20Processin&amp;doi=10.1109%2FTIP.2022.3201472&amp;volume=31&amp;pages=5691-5705&amp;publication_year=2022&amp;author=Zhang%2CJ&amp;author=Chen%2CD&amp;author=Huang%2CQ&amp;author=Liao%2CJ&amp;author=Zhang%2CW&amp;author=Feng%2CH&amp;author=Hua%2CG&amp;author=Yu%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="11."><p class="c-article-references__text" id="ref-CR11">Zhong H., Liao C., Squicciarini A.C., Zhu S., Miller D.J. (2020) Backdoor embedding in convolutional neural network models via invisible perturbation. In: 10th ACM Conference on Data and Application Security and Privacy, pp 97–108</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="12."><p class="c-article-references__text" id="ref-CR12">Xue M., He C., Wang J., Liu W. (2022) One-to-N &amp; N-to-One: Two advanced backdoor attacks against deep learning models. IEEE Trans Dependable Secure Comput 19(3):1562–1578</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1109/TDSC.2020.3028448" data-track-action="article reference" href="https://doi.org/10.1109%2FTDSC.2020.3028448" aria-label="Article reference 12" data-doi="10.1109/TDSC.2020.3028448">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 12" href="http://scholar.google.com/scholar_lookup?&amp;title=One-to-N%20%26%20N-to-One%3A%20Two%20advanced%20backdoor%20attacks%20against%20deep%20learning%20models&amp;journal=IEEE%20Trans%20Dependable%20Secure%20Comput&amp;doi=10.1109%2FTDSC.2020.3028448&amp;volume=19&amp;issue=3&amp;pages=1562-1578&amp;publication_year=2022&amp;author=Xue%2CM&amp;author=He%2CC&amp;author=Wang%2CJ&amp;author=Liu%2CW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="13."><p class="c-article-references__text" id="ref-CR13">Salem A., Wen R., Backes M., Ma S., Zhang Y. (2020) Dynamic backdoor attacks against machine learning models. arXiv:<a href="http://arxiv.org/abs/2003.03675">2003.03675</a>, 1–18</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="14."><p class="c-article-references__text" id="ref-CR14">Xue M., He C., Wu Y., Sun S., Zhang Y., Wang J., Liu W. (2022) PTB: Robust physical backdoor attacks against deep neural networks in real world. Computer &amp; Security. 118(102726):1–15</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 14" href="http://scholar.google.com/scholar_lookup?&amp;title=PTB%3A%20Robust%20physical%20backdoor%20attacks%20against%20deep%20neural%20networks%20in%20real%20world&amp;journal=Computer%20%26%20Security.&amp;volume=118&amp;issue=102726&amp;pages=1-15&amp;publication_year=2022&amp;author=Xue%2CM&amp;author=He%2CC&amp;author=Wu%2CY&amp;author=Sun%2CS&amp;author=Zhang%2CY&amp;author=Wang%2CJ&amp;author=Liu%2CW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="15."><p class="c-article-references__text" id="ref-CR15">Shin R., Song D. (2017) JPEG-resistant adversarial images. In: NIPS Workshop on Machine Learning and Computer Security, vol 1, pp 1–6</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="16."><p class="c-article-references__text" id="ref-CR16">Cao S., Zou Q., Mao X., Ye D., Wang Z. (2021) Metric learning for anti-compression facial forgery detection. In: ACM Multimedia Conference, pp 1929–1937</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="17."><p class="c-article-references__text" id="ref-CR17">Cheng S., Liu Y., Ma S., Zhang X. (2021) Deep feature space Trojan attack of neural networks by controlled detoxification. In: 35th AAAI Conference on Artificial Intelligence, pp 1148–1156</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="18."><p class="c-article-references__text" id="ref-CR18">Gong X., Chen Y., Wang Q., Huang H., Meng L., Shen C., Zhang Q. (2021) Defense-resistant backdoor attacks against deep neural networks in outsourced cloud environment. IEEE J Sel Areas Commun 39(8):2617–2631</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1109/JSAC.2021.3087237" data-track-action="article reference" href="https://doi.org/10.1109%2FJSAC.2021.3087237" aria-label="Article reference 18" data-doi="10.1109/JSAC.2021.3087237">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 18" href="http://scholar.google.com/scholar_lookup?&amp;title=Defense-resistant%20backdoor%20attacks%20against%20deep%20neural%20networks%20in%20outsourced%20cloud%20environment&amp;journal=IEEE%20J%20Sel%20Areas%20Commun&amp;doi=10.1109%2FJSAC.2021.3087237&amp;volume=39&amp;issue=8&amp;pages=2617-2631&amp;publication_year=2021&amp;author=Gong%2CX&amp;author=Chen%2CY&amp;author=Wang%2CQ&amp;author=Huang%2CH&amp;author=Meng%2CL&amp;author=Shen%2CC&amp;author=Zhang%2CQ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="19."><p class="c-article-references__text" id="ref-CR19">Nguyen T.A., Tran A.T. (2021) WaNet - Imperceptible warping-based backdoor attack. In: 9th International Conference on Learning Representations, pp 1–16</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="20."><p class="c-article-references__text" id="ref-CR20">Rumelhart D.E., Hinton G.E., Williams R.J. (1986) Learning representations by back-propagating errors. Nature 323(6088):533–536</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/323533a0" data-track-action="article reference" href="https://doi.org/10.1038%2F323533a0" aria-label="Article reference 20" data-doi="10.1038/323533a0">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="math reference" href="http://www.emis.de/MATH-item?1369.68284" aria-label="MATH reference 20">MATH</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 20" href="http://scholar.google.com/scholar_lookup?&amp;title=Learning%20representations%20by%20back-propagating%20errors&amp;journal=Nature&amp;doi=10.1038%2F323533a0&amp;volume=323&amp;issue=6088&amp;pages=533-536&amp;publication_year=1986&amp;author=Rumelhart%2CDE&amp;author=Hinton%2CGE&amp;author=Williams%2CRJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="21."><p class="c-article-references__text" id="ref-CR21">Xue M., He C., Sun S., Wang J., Liu W. (2021) Robust backdoor attacks against deep neural networks in real physical world. In: 20th IEEE International Conference on Trust, Security and Privacy in Computing and Communications, pp 620–626</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="22."><p class="c-article-references__text" id="ref-CR22">Zhang J., Gu Z., Jang J., Wu H., Stoecklin M.P., Huang H., Molloy I.M. (2018) Protecting intellectual property of deep neural networks with watermarking. In: Proceedings of the Asia Conference on Computer and Communications Security , pp 159–172</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="23."><p class="c-article-references__text" id="ref-CR23">Krizhevsky A. (2009) Learning multiple layers of features from tiny images. Technical report, University of Toronto</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="24."><p class="c-article-references__text" id="ref-CR24">Parkhi O.M., Vedaldi A., Zisserman A. (2015) Deep face recognition. In: Proceedings of the British Machine Vision Conference, pp 1–12</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="25."><p class="c-article-references__text" id="ref-CR25">Krizhevsky A., Sutskever I., Hinton G.E. (2017) Imagenet classification with deep convolutional neural networks. Commun ACM 60(6):84–90</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1145/3065386" data-track-action="article reference" href="https://doi.org/10.1145%2F3065386" aria-label="Article reference 25" data-doi="10.1145/3065386">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 25" href="http://scholar.google.com/scholar_lookup?&amp;title=Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks&amp;journal=Commun%20ACM&amp;doi=10.1145%2F3065386&amp;volume=60&amp;issue=6&amp;pages=84-90&amp;publication_year=2017&amp;author=Krizhevsky%2CA&amp;author=Sutskever%2CI&amp;author=Hinton%2CGE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="26."><p class="c-article-references__text" id="ref-CR26">He K., Zhang X., Ren S., Sun J. (2016) Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp 770–778</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="27."><p class="c-article-references__text" id="ref-CR27">Simonyan K., Zisserman A. (2015) Very deep convolutional networks for large-scale image recognition. In: 3rd International Conference on Learning Representations, pp 1–14</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="28."><p class="c-article-references__text" id="ref-CR28">Goodfellow I., Bengio Y., Courville A. (2016) Deep Learning. MIT press, Cambridge</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="math reference" href="http://www.emis.de/MATH-item?1373.68009" aria-label="MATH reference 28">MATH</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 28" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20Learning&amp;publication_year=2016&amp;author=Goodfellow%2CI&amp;author=Bengio%2CY&amp;author=Courville%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="29."><p class="c-article-references__text" id="ref-CR29">Wenger E., Passananti J., Bhagoji A.N., Yao Y., Zheng H., Zhao B.Y. (2021) Backdoor attacks against deep learning systems in the physical world. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 6206–6215</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="30."><p class="c-article-references__text" id="ref-CR30">Chen Y., Mukherjee D., Han J., Grange A., Xu Y., Liu Z., Parker S., Chen C., Su H., Joshi U., Chiang C., Wang Y., Wilkins P., Bankoski J., Trudeau L.N., Egge N.E., Valin J., Davies T., Midtskogen S., Norkin A., Rivaz P.D. (2018) An overview of core coding tools in the AV1 video codec. In: Picture Coding Symposium, pp 41–45</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="31."><p class="c-article-references__text" id="ref-CR31">Bellard F. (2022) BPG Image format. <a href="https://bellard.org/bpg">https://bellard.org/bpg</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="32."><p class="c-article-references__text" id="ref-CR32">Zhang T. (2004) Solving large scale linear prediction problems using stochastic gradient descent algorithms. In: International Conference on Machine Learning, pp 1–8</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="33."><p class="c-article-references__text" id="ref-CR33">Tramèr F., Kurakin A., Papernot N., Goodfellow I.J., Boneh D., McDaniel P.D. (2018) Ensemble adversarial training: Attacks and defenses. In: 6th International Conference on Learning Representations, pp 1–20</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="34."><p class="c-article-references__text" id="ref-CR34">Gao Y., Wu D., Zhang J., Gan G., Xia S., Niu G., Sugiyama M. (2022) On the effectiveness of adversarial training against backdoor attacks. arXiv:<a href="http://arxiv.org/abs/2202.10627">2202.10627</a>, 1–12</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="35."><p class="c-article-references__text" id="ref-CR35">Geiping J., Fowl L., Somepalli G., Goldblum M., Moeller M., Goldstein T. (2021) What doesn’t kill you makes you robust(er): Adversarial training against poisons and backdoors. arXiv:<a href="http://arxiv.org/abs/2102.13624">2102.13624</a>, 1–25</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="36."><p class="c-article-references__text" id="ref-CR36">Sarkar E., Benkraouda H., Maniatakos M. (2020) FaceHack: Triggering backdoored facial recognition systems using facial characteristics. arXiv:<a href="http://arxiv.org/abs/2006.11623">2006.11623</a>, 1–13</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1007/s10489-023-04575-8?format=refman&amp;flavour=references">Download references<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section></div><section data-title="Acknowledgments"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>This work is supported by the National Natural Science Foundation of China (No. 61602241), and CCF-NSFOCUS Kun-Peng Scientific Research Fund (No. CCF-NSFOCUS 2021012).</p></div></div></section><section aria-labelledby="author-information" data-title="Author information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Authors and Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China</p><p class="c-article-author-affiliation__authors-list">Mingfu Xue, Xin Wang, Shichang Sun, Yushu Zhang &amp; Jian Wang</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China</p><p class="c-article-author-affiliation__authors-list">Weiqiang Liu</p></li></ol><div class="u-js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Mingfu-Xue"><span class="c-article-authors-search__title u-h3 js-search-name">Mingfu Xue</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=Mingfu%20Xue" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Mingfu%20Xue" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Mingfu%20Xue%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Xin-Wang"><span class="c-article-authors-search__title u-h3 js-search-name">Xin Wang</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=Xin%20Wang" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Xin%20Wang" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Xin%20Wang%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Shichang-Sun"><span class="c-article-authors-search__title u-h3 js-search-name">Shichang Sun</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=Shichang%20Sun" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Shichang%20Sun" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Shichang%20Sun%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Yushu-Zhang"><span class="c-article-authors-search__title u-h3 js-search-name">Yushu Zhang</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=Yushu%20Zhang" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Yushu%20Zhang" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Yushu%20Zhang%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Jian-Wang"><span class="c-article-authors-search__title u-h3 js-search-name">Jian Wang</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=Jian%20Wang" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jian%20Wang" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jian%20Wang%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Weiqiang-Liu"><span class="c-article-authors-search__title u-h3 js-search-name">Weiqiang Liu</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=Weiqiang%20Liu" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Weiqiang%20Liu" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Weiqiang%20Liu%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" href="mailto:mingfu.xue@nuaa.edu.cn">Mingfu Xue</a>.</p></div></div></section><section data-title="Ethics declarations"><div class="c-article-section" id="ethics-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="ethics">Ethics declarations</h2><div class="c-article-section__content" id="ethics-content">
            
            
              <h3 class="c-article__sub-heading" id="FPar12">
                <b>Conflict of Interests</b>
              </h3>
              <p>The authors declare that there are no conflicts of interest regarding the publication of this paper.</p>
            
          </div></div></section><section data-title="Additional information"><div class="c-article-section" id="additional-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="additional-information">Additional information</h2><div class="c-article-section__content" id="additional-information-content"><h3 class="c-article__sub-heading">Publisher’s note</h3><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div></div></section><section data-title="Rights and permissions"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p>Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law.</p><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Compression-resistant%20backdoor%20attack%20against%20deep%20neural%20networks&amp;author=Mingfu%20Xue%20et%20al&amp;contentID=10.1007%2Fs10489-023-04575-8&amp;copyright=The%20Author%28s%29%2C%20under%20exclusive%20licence%20to%20Springer%20Science%2BBusiness%20Media%2C%20LLC%2C%20part%20of%20Springer%20Nature&amp;publication=0924-669X&amp;publicationDate=2023-04-12&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info" data-title="About this article"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10489-023-04575-8" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10489-023-04575-8" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>"></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Xue, M., Wang, X., Sun, S. <i>et al.</i> Compression-resistant backdoor attack against deep neural networks.
                    <i>Appl Intell</i>  (2023). https://doi.org/10.1007/s10489-023-04575-8</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" data-track-external="" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1007/s10489-023-04575-8?format=refman&amp;flavour=citation">Download citation<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2023-03-12">12 March 2023</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2023-04-12">12 April 2023</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value">https://doi.org/10.1007/s10489-023-04575-8</span></p></li></ul><div data-component="share-box" class="u-mt-0"><div class="c-article-share-box u-display-none" hidden=""><h3 class="c-article__sub-heading u-mt-0">Share this article</h3><p class="c-article-share-box__description">Anyone you share the following link with will be able to read this content:</p><button class="js-get-share-url c-article-share-box__button" type="button" id="get-share-url" data-track="click" data-track-label="button" data-track-external="" data-track-action="get shareable link">Get shareable link</button><div class="js-no-share-url-container u-display-none" hidden=""><p class="js-c-article-share-box__no-sharelink-info c-article-share-box__no-sharelink-info">Sorry, a shareable link is not currently available for this article.</p></div><div class="js-share-url-container u-display-none" hidden=""><p class="js-share-url c-article-share-box__only-read-input" id="share-url" data-track="click" data-track-label="button" data-track-action="select share url"></p><button class="js-copy-share-url c-article-share-box__button--link-like" type="button" id="copy-share-url" data-track="click" data-track-label="button" data-track-action="copy share url" data-track-external="">Copy to clipboard</button></div><p class="js-c-article-share-box__additional-info c-article-share-box__additional-info">
                            Provided by the Springer Nature SharedIt content-sharing initiative
                        </p></div></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span>Artificial intelligence security</span></li><li class="c-article-subject-list__subject"><span>Backdoor attack</span></li><li class="c-article-subject-list__subject"><span>Compression resistance</span></li><li class="c-article-subject-list__subject"><span>Deep neural networks</span></li><li class="c-article-subject-list__subject"><span>Feature consistency training</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                

                <div data-test="download-article-link-wrapper" class="js-context-bar-sticky-point-desktop">
                    
    <div class="c-pdf-container">
        <div class="c-pdf-download u-clear-both u-mb-16">
            <a href="/content/pdf/10.1007/s10489-023-04575-8.pdf?pdf=button" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button" data-track-external download>
                
                    <span class="c-pdf-download__text">Download PDF</span>
                    <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
                
            </a>
        </div>
    </div>


                </div>

                
                    
    <div class="app-checklist-banner" data-test="article-checklist-banner">
        <div class="app-checklist-banner__body">
            <h3 class="app-checklist-banner__title">Working on a manuscript?</h3>
            <a class="app-checklist-banner__link" data-track="click" data-track-category="pre-submission-checklist" data-track-action="clicked article page checklist banner test 2 old version" data-track-label="link" href="https://beta.springernature.com/pre-submission?journalId=10489"
            data-test="article-checklist-banner-link">Avoid the common mistakes
            <svg class="app-checklist-banner__arrow-icon" aria-hidden="true" focusable="false">
                <use xlink:href="#icon-springer-arrow-right"></use>
            </svg>
            </a>
        </div>
        <div class="app-checklist-banner__icon-container">
        <svg class="app-checklist-banner__paper-icon" aria-hidden="true" focusable="false">
            <use xlink:href="#icon-checklist-banner"></use>
        </svg>
        </div>
    </div>

                

                <div data-test="collections">
                    
    

                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="u-lazy-ad-wrapper u-mt-16 u-hide" data-component-mpu><div class="c-ad c-ad--300x250">
    <div class="c-ad__inner">
        <p class="c-ad__label">Advertisement</p>
        <div id="div-gpt-ad-MPU1"
             class="div-gpt-ad grade-c-hide"
             data-pa11y-ignore
             data-gpt
             data-gpt-unitpath="/270604982/springerlink/10489/article"
             data-gpt-sizes="300x250" data-test="MPU1-ad"
             data-gpt-targeting="pos=MPU1;articleid=s10489-023-04575-8;">
        </div>
    </div>
</div>

</div>
                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>

    
    <script>
            
    </script>
    
        
    <footer class="app-footer" role="contentinfo" data-test="springerlink-footer">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" data-cc-action="preferences" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a href="https://support.springer.com/en/support/home">FAQ</a></li>
                <li><a id="contactus-footer-link" href="https://support.springer.com/en/support/solutions/articles/6000206179-contacting-us">Contact us</a></li>
                <li><a href="https://www.springer.com/gp/shop/promo/affiliate/springer-nature">Affiliate program</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 193.145.247.253</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            IK4-Ikerlan IK4-Ikerlan (3991460755)  - Universidad del País Vasco (3902463196)  - Universidad del Pais Vasco (1000862991)  - Springer  National Consortium Spain Springer National Consortium Spain (3991436001) 
        </p>
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-b88bf25ad4.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2023 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
    </footer>



    </div>
    
    

    
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 10 10" xmlns="http://www.w3.org/2000/svg">
            <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill="currentColor" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
        <symbol id="icon-info" viewBox="0 0 18 18">
            <path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm0 7h-1.5l-.11662113.00672773c-.49733868.05776511-.88337887.48043643-.88337887.99327227 0 .47338693.32893365.86994729.77070917.97358929l.1126697.01968298.11662113.00672773h.5v3h-.5l-.11662113.0067277c-.42082504.0488782-.76196299.3590206-.85696816.7639815l-.01968298.1126697-.00672773.1166211.00672773.1166211c.04887817.4208251.35902055.761963.76398144.8569682l.1126697.019683.11662113.0067277h3l.1166211-.0067277c.4973387-.0577651.8833789-.4804365.8833789-.9932723 0-.4733869-.3289337-.8699473-.7707092-.9735893l-.1126697-.019683-.1166211-.0067277h-.5v-4l-.00672773-.11662113c-.04887817-.42082504-.35902055-.76196299-.76398144-.85696816l-.1126697-.01968298zm0-3.25c-.69035594 0-1.25.55964406-1.25 1.25s.55964406 1.25 1.25 1.25 1.25-.55964406 1.25-1.25-.55964406-1.25-1.25-1.25z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="icon-success" viewBox="0 0 18 18">
            <path d="M9 0a9 9 0 110 18A9 9 0 019 0zm3.486 4.982l-4.718 5.506L5.14 8.465a.991.991 0 00-1.423.133 1.06 1.06 0 00.13 1.463l3.407 2.733a1 1 0 001.387-.133l5.385-6.334a1.06 1.06 0 00-.116-1.464.991.991 0 00-1.424.119z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="icon-chevron-down" viewBox="0 0 16 16">
            <path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)"/>
        </symbol>
        <symbol id="icon-warning" viewBox="0 0 18 18">
            <path d="m9 11.75c.69035594 0 1.25.5596441 1.25 1.25s-.55964406 1.25-1.25 1.25-1.25-.5596441-1.25-1.25.55964406-1.25 1.25-1.25zm.41320045-7.75c.55228475 0 1.00000005.44771525 1.00000005 1l-.0034543.08304548-.3333333 4c-.043191.51829212-.47645714.91695452-.99654578.91695452h-.15973424c-.52008864 0-.95335475-.3986624-.99654576-.91695452l-.33333333-4c-.04586475-.55037702.36312325-1.03372649.91350028-1.07959124l.04148683-.00259031zm-.41320045 14c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="icon-plus" viewBox="0 0 16 16">
            <path d="m2.00087166 7h4.99912834v-4.99912834c0-.55276616.44386482-1.00087166 1-1.00087166.55228475 0 1 .44463086 1 1.00087166v4.99912834h4.9991283c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-4.9991283v4.9991283c0 .5527662-.44386482 1.0008717-1 1.0008717-.55228475 0-1-.4446309-1-1.0008717v-4.9991283h-4.99912834c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="icon-minus" viewBox="0 0 16 16">
            <path d="m2.00087166 7h11.99825664c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-11.99825664c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="icon-error" viewBox="0 0 18 18">
            <path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm2.8630343 4.71100931-2.8630343 2.86303426-2.86303426-2.86303426c-.39658757-.39658757-1.03281091-.39438847-1.4265779-.00062147-.39651227.39651226-.39348876 1.03246767.00062147 1.4265779l2.86303426 2.86303426-2.86303426 2.8630343c-.39658757.3965875-.39438847 1.0328109-.00062147 1.4265779.39651226.3965122 1.03246767.3934887 1.4265779-.0006215l2.86303426-2.8630343 2.8630343 2.8630343c.3965875.3965876 1.0328109.3943885 1.4265779.0006215.3965122-.3965123.3934887-1.0324677-.0006215-1.4265779l-2.8630343-2.8630343 2.8630343-2.86303426c.3965876-.39658757.3943885-1.03281091.0006215-1.4265779-.3965123-.39651227-1.0324677-.39348876-1.4265779.00062147z" fill="currentColor" fill-rule="evenodd"/>
        </symbol>
        <symbol id="icon-springer-arrow-left">
            <path d="M15 7a1 1 0 000-2H3.385l2.482-2.482a.994.994 0 00.02-1.403 1.001 1.001 0 00-1.417 0L.294 5.292a1.001 1.001 0 000 1.416l4.176 4.177a.991.991 0 001.4.016 1 1 0 00-.003-1.42L3.385 7H15z"/>
        </symbol>
        <symbol id="icon-springer-arrow-right">
            <path d="M1 7a1 1 0 010-2h11.615l-2.482-2.482a.994.994 0 01-.02-1.403 1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L12.615 7H1z"/>
        </symbol>
        <symbol id="icon-arrow-up" viewBox="0 0 16 16">
            <path d="m12.716625 4.46975946-4.03074003-4.17620792c-.37758093-.39120768-.98937525-.39160691-1.367372.0000316l-4.03091981 4.1763942c-.37759778.39122514-.38381821 1.01908149-.01600053 1.40017357.37750607.39113012.98772445.3930364 1.37006824-.00310603l2.39538588-2.48183446v11.61478958l.00649339.1166211c.055753.4973387.46370161.8833789.95867408.8833789.49497246 0 .90292107-.3860402.95867408-.8833789l.00649338-.1166211v-11.61478958l2.39518592 2.4816273c.3791392.39282216.9863753.40056173 1.3541929.01946965.3775061-.39113012.3778444-1.02492687-.0001355-1.41654791z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="icon-tick" viewBox="0 0 24 24">
            <path d="M12,24 C5.372583,24 0,18.627417 0,12 C0,5.372583 5.372583,0 12,0 C18.627417,0 24,5.372583 24,12 C24,18.627417 18.627417,24 12,24 Z M7.657,10.79 C7.45285634,10.6137568 7.18569967,10.5283283 6.91717333,10.5534259 C6.648647,10.5785236 6.40194824,10.7119794 6.234,10.923 C5.87705269,11.3666969 5.93445559,12.0131419 6.364,12.387 L10.261,15.754 C10.6765468,16.112859 11.3037113,16.0695601 11.666,15.657 L17.759,8.713 C18.120307,8.27302248 18.0695334,7.62621189 17.644,7.248 C17.4414817,7.06995024 17.1751516,6.9821166 16.9064461,7.00476032 C16.6377406,7.02740404 16.3898655,7.15856958 16.22,7.368 L10.768,13.489 L7.657,10.79 Z"/>
        </symbol>
        <symbol id="icon-expand-image" viewBox="0 0 18 18">
            <path d="m7.49754099 11.9178212c.38955542-.3895554.38761957-1.0207846-.00290473-1.4113089-.39324695-.3932469-1.02238878-.3918247-1.41130883-.0029047l-4.10273549 4.1027355.00055454-3.5103985c.00008852-.5603185-.44832171-1.006032-1.00155062-1.0059446-.53903074.0000852-.97857527.4487442-.97866268 1.0021075l-.00093318 5.9072465c-.00008751.553948.44841131 1.001882 1.00174994 1.0017946l5.906983-.0009331c.5539233-.0000875 1.00197907-.4486389 1.00206646-1.0018679.00008515-.5390307-.45026621-.9784332-1.00588841-.9783454l-3.51010549.0005545zm3.00571741-5.83449376c-.3895554.38955541-.3876196 1.02078454.0029047 1.41130883.393247.39324696 1.0223888.39182478 1.4113089.00290473l4.1027355-4.10273549-.0005546 3.5103985c-.0000885.56031852.4483217 1.006032 1.0015506 1.00594461.5390308-.00008516.9785753-.44874418.9786627-1.00210749l.0009332-5.9072465c.0000875-.553948-.4484113-1.00188204-1.0017499-1.00179463l-5.906983.00093313c-.5539233.00008751-1.0019791.44863892-1.0020665 1.00186784-.0000852.53903074.4502662.97843325 1.0058884.97834547l3.5101055-.00055449z" fill="currentColor" fill-rule="evenodd"/>
        </symbol>
        <symbol id="icon-close" viewBox="0 0 16 16">
            <path d="m2.29679575 12.2772478c-.39658757.3965876-.39438847 1.0328109-.00062148 1.4265779.39651227.3965123 1.03246768.3934888 1.42657791-.0006214l4.27724782-4.27724787 4.2772478 4.27724787c.3965876.3965875 1.0328109.3943884 1.4265779.0006214.3965123-.3965122.3934888-1.0324677-.0006214-1.4265779l-4.27724787-4.2772478 4.27724787-4.27724782c.3965875-.39658757.3943884-1.03281091.0006214-1.42657791-.3965122-.39651226-1.0324677-.39348875-1.4265779.00062148l-4.2772478 4.27724782-4.27724782-4.27724782c-.39658757-.39658757-1.03281091-.39438847-1.42657791-.00062148-.39651226.39651227-.39348875 1.03246768.00062148 1.42657791l4.27724782 4.27724782z" fill="currentColor" fill-rule="evenodd"/>
        </symbol>
        <symbol id="icon-chevron-right" viewBox="0 0 7 12">
            <path d="M2.782 5 .3 2.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 0 1 1.417 0l4.176 4.177a1.001 1.001 0 0 1 0 1.416l-4.176 4.177a.991.991 0 0 1-1.4.016A1 1 0 0 1 .3 9.481L2.782 7l1.013-.998L2.782 5Z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="icon-checklist-banner" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 56.69 56.69" style="enable-background:new 0 0 56.69 56.69" xml:space="preserve">
            <path style="fill:none" d="M0 0h56.69v56.69H0z"/><defs><path id="a" d="M0 .74h56.72v55.24H0z"/></defs><clipPath id="b"><use xlink:href="#a" style="overflow:visible"/></clipPath><path d="M21.14 34.46c0-6.77 5.48-12.26 12.24-12.26s12.24 5.49 12.24 12.26-5.48 12.26-12.24 12.26c-6.76-.01-12.24-5.49-12.24-12.26zm19.33 10.66 10.23 9.22s1.21 1.09 2.3-.12l2.09-2.32s1.09-1.21-.12-2.3l-10.23-9.22m-19.29-5.92c0-4.38 3.55-7.94 7.93-7.94s7.93 3.55 7.93 7.94c0 4.38-3.55 7.94-7.93 7.94-4.38-.01-7.93-3.56-7.93-7.94zm17.58 12.99 4.14-4.81" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round"/><path d="M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5m14.42-5.2V4.86s0-2.93-2.93-2.93H4.13s-2.93 0-2.93 2.93v37.57s0 2.93 2.93 2.93h15.01M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round;stroke-linejoin:round"/>
        </symbol>
        <symbol id="icon-get-ftr" viewBox="0 0 24 24">
            <path fill="#0D8D8A" fill-rule="nonzero" d="M24 12c0 6.627-5.373 12-12 12-2.102 0-4.078-.54-5.796-1.49l1.485-1.484A9.96 9.96 0 0 0 12 22c5.523 0 10-4.477 10-10a9.96 9.96 0 0 0-.974-4.31l1.484-1.486A11.946 11.946 0 0 1 24 12ZM12 0c2.102 0 4.079.54 5.797 1.49l-1.485 1.485A9.96 9.96 0 0 0 12 2C6.477 2 2 6.477 2 12c0 1.544.35 3.006.975 4.312L1.49 17.797A11.946 11.946 0 0 1 0 12C0 5.373 5.373 0 12 0Z"/>
            <circle cx="12" cy="12" r="5.333" fill="#096A73"/>
        </symbol>
        <symbol id="icon-github" viewBox="0 0 100 100">
            <path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z" fill="#24292f"/>
        </symbol>
        <symbol id="icon-search-filter" viewBox="0 0 29 29">
            <defs><style>.cls-1{fill:none;}</style></defs><title/><g data-name="Layer 2" id="Layer_2"><path d="M28,9H11a1,1,0,0,1,0-2H28a1,1,0,0,1,0,2Z"/><path d="M7,9H4A1,1,0,0,1,4,7H7A1,1,0,0,1,7,9Z"/><path d="M21,17H4a1,1,0,0,1,0-2H21a1,1,0,0,1,0,2Z"/><path d="M11,25H4a1,1,0,0,1,0-2h7a1,1,0,0,1,0,2Z"/><path d="M9,11a3,3,0,1,1,3-3A3,3,0,0,1,9,11ZM9,7a1,1,0,1,0,1,1A1,1,0,0,0,9,7Z"/><path d="M23,19a3,3,0,1,1,3-3A3,3,0,0,1,23,19Zm0-4a1,1,0,1,0,1,1A1,1,0,0,0,23,15Z"/><path d="M13,27a3,3,0,1,1,3-3A3,3,0,0,1,13,27Zm0-4a1,1,0,1,0,1,1A1,1,0,0,0,13,23Z"/><path d="M28,17H25a1,1,0,0,1,0-2h3a1,1,0,0,1,0,2Z"/><path d="M28,25H15a1,1,0,0,1,0-2H28a1,1,0,0,1,0,2Z"/></g><g id="frame"><rect class="cls-1" height="32" width="32"/></g>
        </symbol>
        <symbol id="icon-book" viewBox="0 0 18 18">
            <path
                d="m4 13v-11h1v11h11v-11h-13c-.55228475 0-1 .44771525-1 1v10.2675644c.29417337-.1701701.63571286-.2675644 1-.2675644zm12 1h-13c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1h13zm0 3h-13c-1.1045695 0-2-.8954305-2-2v-12c0-1.1045695.8954305-2 2-2h13c.5522847 0 1 .44771525 1 1v14c0 .5522847-.4477153 1-1 1zm-8.5-13h6c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1 2h4c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-4c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z"
                fill-rule="evenodd"/>
        </symbol>
        <symbol id="icon-submit-open" viewBox="0 0 16 17">
            <path d="M12 0c1.10457 0 2 .895431 2 2v5c0 .276142-.223858.5-.5.5S13 7.276142 13 7V2c0-.512836-.38604-.935507-.883379-.993272L12 1H6v3c0 1.10457-.89543 2-2 2H1v8c0 .512836.38604.935507.883379.993272L2 15h6.5c.276142 0 .5.223858.5.5s-.223858.5-.5.5H2c-1.104569 0-2-.89543-2-2V5.828427c0-.530433.210714-1.039141.585786-1.414213L4.414214.585786C4.789286.210714 5.297994 0 5.828427 0H12Zm3.41 11.14c.250899.250899.250274.659726 0 .91-.242954.242954-.649606.245216-.9-.01l-1.863671-1.900337.001043 5.869492c0 .356992-.289839.637138-.647372.637138-.347077 0-.647371-.285256-.647371-.637138l-.001043-5.869492L9.5 12.04c-.253166.258042-.649726.260274-.9.01-.242954-.242954-.252269-.657731 0-.91l2.942184-2.951303c.250908-.250909.66127-.252277.91353-.000017L15.41 11.14ZM5 1.413 1.413 5H4c.552285 0 1-.447715 1-1V1.413ZM11 3c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Zm0 2c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Z" fill-rule="nonzero"/>
        </symbol>
    </svg>

</body>
</html>


