% Encoding: UTF-8

 
@TechReport{Greshake2023,
  author     = {Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario},
  title      = {More than you've asked for: {A} {Comprehensive} {Analysis} of {Novel} {Prompt} {Injection} {Threats} to {Application}-{Integrated} {Large} {Language} {Models}},
  year       = {2023},
  month      = feb,
  note       = {ZSCC: 0000000 arXiv:2302.12173 [cs] type: article},
  abstract   = {We are currently witnessing dramatic advances in the capabilities of Large Language Models (LLMs). They are already being adopted in practice and integrated into many systems, including integrated development environments (IDEs) and search engines. The functionalities of current LLMs can be modulated via natural language prompts, while their exact internal functionality remains implicit and unassessable. This property, which makes them adaptable to even unseen tasks, might also make them susceptible to targeted adversarial prompting. Recently, several ways to misalign LLMs using Prompt Injection (PI) attacks have been introduced. In such attacks, an adversary can prompt the LLM to produce malicious content or override the original instructions and the employed filtering schemes. Recent work showed that these attacks are hard to mitigate, as state-of-the-art LLMs are instruction-following. So far, these attacks assumed that the adversary is directly prompting the LLM. In this work, we show that augmenting LLMs with retrieval and API calling capabilities (so-called Application-Integrated LLMs) induces a whole new set of attack vectors. These LLMs might process poisoned content retrieved from the Web that contains malicious prompts pre-injected and selected by adversaries. We demonstrate that an attacker can indirectly perform such PI attacks. Based on this key insight, we systematically analyze the resulting threat landscape of Application-Integrated LLMs and discuss a variety of new attack vectors. To demonstrate the practical viability of our attacks, we implemented specific demonstrations of the proposed attacks within synthetic applications. In summary, our work calls for an urgent evaluation of current mitigation techniques and an investigation of whether new techniques are needed to defend LLMs against these threats.},
  doi        = {10.48550/arXiv.2302.12173},
  file       = {:greshake_more_2023 - More Than You've Asked For_ a Comprehensive Analysis of Novel Prompt Injection Threats to Application Integrated Large Language Models.pdf:PDF;:Greshake2023 - More Than You've Asked For_ a Comprehensive Analysis of Novel Prompt Injection Threats to Application Integrated Large Language Models.html:URL},
  keywords   = {Computer Science - Cryptography and Security, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society},
  ranking    = {rank3},
  readstatus = {read},
  school     = {arXiv},
  shorttitle = {More than you've asked for},
  url        = {http://arxiv.org/abs/2302.12173},
  urldate    = {2023-02-28},
}

'
@TechReport{Kawar2022,
  author     = {Kawar, Bahjat and Zada, Shiran and Lang, Oran and Tov, Omer and Chang, Huiwen and Dekel, Tali and Mosseri, Inbar and Irani, Michal},
  title      = {Imagic: {Text}-{Based} {Real} {Image} {Editing} with {Diffusion} {Models}},
  year       = {2022},
  month      = oct,
  note       = {arXiv:2210.09276 [cs] type: article},
  abstract   = {Text-conditioned image editing has recently attracted considerable interest. However, most methods are currently either limited to specific editing types (e.g., object overlay, style transfer), or apply to synthetically generated images, or require multiple input images of a common object. In this paper we demonstrate, for the very first time, the ability to apply complex (e.g., non-rigid) text-guided semantic edits to a single real image. For example, we can change the posture and composition of one or multiple objects inside an image, while preserving its original characteristics. Our method can make a standing dog sit down or jump, cause a bird to spread its wings, etc. -- each within its single high-resolution natural image provided by the user. Contrary to previous work, our proposed method requires only a single input image and a target text (the desired edit). It operates on real images, and does not require any additional inputs (such as image masks or additional views of the object). Our method, which we call "Imagic", leverages a pre-trained text-to-image diffusion model for this task. It produces a text embedding that aligns with both the input image and the target text, while fine-tuning the diffusion model to capture the image-specific appearance. We demonstrate the quality and versatility of our method on numerous inputs from various domains, showcasing a plethora of high quality complex semantic image edits, all within a single unified framework.},
  doi        = {10.48550/arXiv.2210.09276},
  file       = {:Kawar2022 - Imagic_ Text Based Real Image Editing with Diffusion Models.pdf:PDF},
  groups     = {Others, To read},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  school     = {arXiv},
  shorttitle = {Imagic},
  url        = {http://arxiv.org/abs/2210.09276},
  urldate    = {2022-10-19},
}

 
@TechReport{Singer2022,
  author     = {Singer, Uriel and Polyak, Adam and Hayes, Thomas and Yin, Xi and An, Jie and Zhang, Songyang and Hu, Qiyuan and Yang, Harry and Ashual, Oron and Gafni, Oran and Parikh, Devi and Gupta, Sonal and Taigman, Yaniv},
  title      = {Make-{A}-{Video}: {Text}-to-{Video} {Generation} without {Text}-{Video} {Data}},
  year       = {2022},
  month      = sep,
  note       = {arXiv:2209.14792 [cs] type: article},
  abstract   = {We propose Make-A-Video -- an approach for directly translating the tremendous recent progress in Text-to-Image (T2I) generation to Text-to-Video (T2V). Our intuition is simple: learn what the world looks like and how it is described from paired text-image data, and learn how the world moves from unsupervised video footage. Make-A-Video has three advantages: (1) it accelerates training of the T2V model (it does not need to learn visual and multimodal representations from scratch), (2) it does not require paired text-video data, and (3) the generated videos inherit the vastness (diversity in aesthetic, fantastical depictions, etc.) of today's image generation models. We design a simple yet effective way to build on T2I models with novel and effective spatial-temporal modules. First, we decompose the full temporal U-Net and attention tensors and approximate them in space and time. Second, we design a spatial temporal pipeline to generate high resolution and frame rate videos with a video decoder, interpolation model and two super resolution models that can enable various applications besides T2V. In all aspects, spatial and temporal resolution, faithfulness to text, and quality, Make-A-Video sets the new state-of-the-art in text-to-video generation, as determined by both qualitative and quantitative measures.},
  doi        = {10.48550/arXiv.2209.14792},
  file       = {:Singer2022 - Make a Video_ Text to Video Generation without Text Video Data.pdf:PDF},
  groups     = {Others},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  school     = {arXiv},
  shorttitle = {Make-{A}-{Video}},
  url        = {http://arxiv.org/abs/2209.14792},
  urldate    = {2022-10-07},
}

 
@TechReport{Yang2022,
  author     = {Yang, Ziqing and He, Xinlei and Li, Zheng and Backes, Michael and Humbert, Mathias and Berrang, Pascal and Zhang, Yang},
  title      = {Data {Poisoning} {Attacks} {Against} {Multimodal} {Encoders}},
  year       = {2022},
  month      = sep,
  note       = {arXiv:2209.15266 [cs] type: article},
  abstract   = {Traditional machine learning (ML) models usually rely on large-scale labeled datasets to achieve strong performance. However, such labeled datasets are often challenging and expensive to obtain. Also, the predefined categories limit the model's ability to generalize to other visual concepts as additional labeled data is required. On the contrary, the newly emerged multimodal model, which contains both visual and linguistic modalities, learns the concept of images from the raw text. It is a promising way to solve the above problems as it can use easy-to-collect image-text pairs to construct the training dataset and the raw texts contain almost unlimited categories according to their semantics. However, learning from a large-scale unlabeled dataset also exposes the model to the risk of potential poisoning attacks, whereby the adversary aims to perturb the model's training dataset to trigger malicious behaviors in it. Previous work mainly focuses on the visual modality. In this paper, we instead focus on answering two questions: (1) Is the linguistic modality also vulnerable to poisoning attacks? and (2) Which modality is most vulnerable? To answer the two questions, we conduct three types of poisoning attacks against CLIP, the most representative multimodal contrastive learning framework. Extensive evaluations on different datasets and model architectures show that all three attacks can perform well on the linguistic modality with only a relatively low poisoning rate and limited epochs. Also, we observe that the poisoning effect differs between different modalities, i.e., with lower MinRank in the visual modality and with higher Hit@K when K is small in the linguistic modality. To mitigate the attacks, we propose both pre-training and post-training defenses. We empirically show that both defenses can significantly reduce the attack performance while preserving the model's utility.},
  doi        = {10.48550/arXiv.2209.15266},
  file       = {:Yang2022 - Data Poisoning Attacks against Multimodal Encoders.pdf:PDF},
  groups     = {Backdoors, Attack},
  keywords   = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
  ranking    = {rank4},
  readstatus = {read},
  school     = {arXiv},
  url        = {http://arxiv.org/abs/2209.15266},
  urldate    = {2022-10-07},
}

 
@TechReport{Salem2021,
  author   = {Salem, Ahmed and Backes, Michael and Zhang, Yang},
  title    = {Get a {Model}! {Model} {Hijacking} {Attack} {Against} {Machine} {Learning} {Models}},
  year     = {2021},
  month    = nov,
  note     = {arXiv:2111.04394 [cs] type: article},
  abstract = {Machine learning (ML) has established itself as a cornerstone for various critical applications ranging from autonomous driving to authentication systems. However, with this increasing adoption rate of machine learning models, multiple attacks have emerged. One class of such attacks is training time attack, whereby an adversary executes their attack before or during the machine learning model training. In this work, we propose a new training time attack against computer vision based machine learning models, namely model hijacking attack. The adversary aims to hijack a target model to execute a different task than its original one without the model owner noticing. Model hijacking can cause accountability and security risks since a hijacked model owner can be framed for having their model offering illegal or unethical services. Model hijacking attacks are launched in the same way as existing data poisoning attacks. However, one requirement of the model hijacking attack is to be stealthy, i.e., the data samples used to hijack the target model should look similar to the model's original training dataset. To this end, we propose two different model hijacking attacks, namely Chameleon and Adverse Chameleon, based on a novel encoder-decoder style ML model, namely the Camouflager. Our evaluation shows that both of our model hijacking attacks achieve a high attack success rate, with a negligible drop in model utility.},
  annote   = {Comment: To Appear in NDSS 2022},
  doi      = {10.48550/arXiv.2111.04394},
  file     = {:Salem2021 - Get a Model! Model Hijacking Attack against Machine Learning Models.pdf:PDF},
  keywords = {Computer Science - Cryptography and Security, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2111.04394},
  urldate  = {2022-10-07},
}

@TechReport{Salem2020,
  author     = {Salem, Ahmed and Backes, Michael and Zhang, Yang},
  title      = {Don't {Trigger} {Me}! {A} {Triggerless} {Backdoor} {Attack} {Against} {Deep} {Neural} {Networks}},
  year       = {2020},
  month      = oct,
  note       = {arXiv:2010.03282 [cs] type: article},
  abstract   = {Backdoor attack against deep neural networks is currently being profoundly investigated due to its severe security consequences. Current state-of-the-art backdoor attacks require the adversary to modify the input, usually by adding a trigger to it, for the target model to activate the backdoor. This added trigger not only increases the difficulty of launching the backdoor attack in the physical world, but also can be easily detected by multiple defense mechanisms. In this paper, we present the first triggerless backdoor attack against deep neural networks, where the adversary does not need to modify the input for triggering the backdoor. Our attack is based on the dropout technique. Concretely, we associate a set of target neurons that are dropped out during model training with the target label. In the prediction phase, the model will output the target label when the target neurons are dropped again, i.e., the backdoor attack is launched. This triggerless feature of our attack makes it practical in the physical world. Extensive experiments show that our triggerless backdoor attack achieves a perfect attack success rate with a negligible damage to the model's utility.},
  doi        = {10.48550/arXiv.2010.03282},
  file       = {:Salem2020 - Don't Trigger Me! a Triggerless Backdoor Attack against Deep Neural Networks.pdf:PDF},
  groups     = {Backdoors, Attack},
  keywords   = {Computer Science - Cryptography and Security, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
  ranking    = {rank3},
  readstatus = {read},
  school     = {arXiv},
  url        = {http://arxiv.org/abs/2010.03282},
  urldate    = {2022-10-07},
}

@InProceedings{Salem2022,
  author     = {Salem, Ahmed and Wen, Rui and Backes, Michael and Ma, Shiqing and Zhang, Yang},
  booktitle  = {2022 {IEEE} 7th {European} {Symposium} on {Security} and {Privacy} ({EuroS}\&{P})},
  title      = {Dynamic {Backdoor} {Attacks} {Against} {Machine} {Learning} {Models}},
  year       = {2022},
  month      = jun,
  pages      = {703--718},
  abstract   = {Machine learning (ML) has made tremendous progress during the past decade and is being adopted in various critical real-world applications. However, recent research has shown that ML models are vulnerable to multiple security and privacy attacks. In particular, backdoor attacks against ML models have recently raised a lot of awareness. A successful backdoor attack can cause severe consequences, such as allowing an adversary to bypass critical authentication systems. Current backdooring techniques rely on adding static triggers (with fixed patterns and locations) on ML model inputs which are prone to detection by the current backdoor detection mechanisms. In this paper, we propose the first class of dynamic backdooring techniques against deep neural networks (DNN), namely Random Backdoor, Backdoor Generating Network (BaN), and conditional Backdoor Generating Network (c-BaN). Triggers generated by our techniques can have random patterns and locations, which reduce the efficacy of the current backdoor detection mechanisms. In particular, BaN and c-BaN based on a novel generative network are the first two schemes that algorithmically generate triggers. Moreover, c-BaN is the first conditional backdooring technique that given a target label, it can generate a target-specific trigger. Both BaN and c-BaN are essentially a general framework which renders the adversary the flexibility for further customizing backdoor attacks. We extensively evaluate our techniques on three benchmark datasets: MNIST, CelebA, and CIFAR-10. Our techniques achieve almost perfect attack performance on back-doored data with a negligible utility loss. We further show that our techniques can bypass current state-of-the-art defense mechanisms against backdoor attacks, including ABS, Februus, MNTD, Neural Cleanse, and STRIP.},
  doi        = {10.1109/EuroSP53844.2022.00049},
  file       = {:Salem2022 - Dynamic Backdoor Attacks against Machine Learning Models.pdf:PDF},
  groups     = {Backdoors, Attack},
  keywords   = {Deep learning, Training, Privacy, Strips, Neural networks, Authentication, Benchmark testing, Backdoor attack, Machine learning security},
  ranking    = {rank2},
  readstatus = {read},
}

@InProceedings{Chen2021,
  author     = {Chen, Xiaoyi and Salem, Ahmed and Chen, Dingfan and Backes, Michael and Ma, Shiqing and Shen, Qingni and Wu, Zhonghai and Zhang, Yang},
  booktitle  = {Annual {Computer} {Security} {Applications} {Conference}},
  title      = {{BadNL}: {Backdoor} {Attacks} against {NLP} {Models} with {Semantic}-preserving {Improvements}},
  year       = {2021},
  address    = {New York, NY, USA},
  month      = dec,
  pages      = {554--569},
  publisher  = {Association for Computing Machinery},
  series     = {{ACSAC} '21},
  abstract   = {Deep neural networks (DNNs) have progressed rapidly during the past decade and have been deployed in various real-world applications. Meanwhile, DNN models have been shown to be vulnerable to security and privacy attacks. One such attack that has attracted a great deal of attention recently is the backdoor attack. Specifically, the adversary poisons the target model’s training set to mislead any input with an added secret trigger to a target class. Previous backdoor attacks predominantly focus on computer vision (CV) applications, such as image classification. In this paper, we perform a systematic investigation of backdoor attack on NLP models, and propose BadNL, a general NLP backdoor attack framework including novel attack methods. Specifically, we propose three methods to construct triggers, namely BadChar, BadWord, and BadSentence, including basic and semantic-preserving variants. Our attacks achieve an almost perfect attack success rate with a negligible effect on the original model’s utility. For instance, using the BadChar, our backdoor attack achieves a 98.9\% attack success rate with yielding a utility improvement of 1.5\% on the SST-5 dataset when only poisoning 3\% of the original set. Moreover, we conduct a user study to prove that our triggers can well preserve the semantics from humans perspective.},
  doi        = {10.1145/3485832.3485837},
  file       = {:Chen2021 - BadNL_ Backdoor Attacks against NLP Models with Semantic Preserving Improvements.pdf:PDF},
  groups     = {Backdoors, Attack},
  isbn       = {9781450385794},
  keywords   = {NLP, backdoor attack, semantic-preserving},
  shorttitle = {{BadNL}},
  url        = {https://doi.org/10.1145/3485832.3485837},
  urldate    = {2022-10-07},
}

'
@InProceedings{Jia2022,
  author     = {Jia, Jinyuan and Liu, Yupei and Gong, Neil Zhenqiang},
  booktitle  = {2022 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
  title      = {{BadEncoder}: {Backdoor} {Attacks} to {Pre}-trained {Encoders} in {Self}-{Supervised} {Learning}},
  year       = {2022},
  month      = may,
  note       = {ISSN: 2375-1207},
  pages      = {2043--2059},
  abstract   = {Self-supervised learning in computer vision aims to pre-train an image encoder using a large amount of unlabeled images or (image, text) pairs. The pre-trained image encoder can then be used as a feature extractor to build downstream classifiers for many downstream tasks with a small amount of or no labeled training data. In this work, we propose BadEncoder, the first backdoor attack to self-supervised learning. In particular, our BadEncoder injects backdoors into a pre-trained image encoder such that the downstream classifiers built based on the backdoored image encoder for different downstream tasks simultaneously inherit the backdoor behavior. We formulate our BadEncoder as an optimization problem and we propose a gradient descent based method to solve it, which produces a backdoored image encoder from a clean one. Our extensive empirical evaluation results on multiple datasets show that our BadEncoder achieves high attack success rates while preserving the accuracy of the downstream classifiers. We also show the effectiveness of BadEncoder using two publicly available, real-world image encoders, i.e., Google’s image encoder pre-trained on ImageNet and OpenAI’s Contrastive Language-Image Pre-training (CLIP) image encoder pre-trained on 400 million (image, text) pairs collected from the Internet. Moreover, we consider defenses including Neural Cleanse and MNTD (empirical defenses) as well as PatchGuard (a provable defense). Our results show that these defenses are insufficient to defend against BadEncoder, highlighting the needs for new defenses against our BadEncoder. Our code is publicly available at: https://github.com/jjy1994/BadEncoder.},
  doi        = {10.1109/SP46214.2022.9833644},
  file       = {:Jia2022 - BadEncoder_ Backdoor Attacks to Pre Trained Encoders in Self Supervised Learning.pdf:PDF},
  groups     = {To read, Attack},
  issn       = {2375-1207},
  keywords   = {Privacy, Computer vision, Training data, Self-supervised learning, Feature extraction, Internet, Security},
  ranking    = {rank3},
  readstatus = {read},
  shorttitle = {{BadEncoder}},
}

'
@InProceedings{Xu2021,
  author     = {Xu, Jing and Xue, Minhui (Jason) and Picek, Stjepan},
  booktitle  = {Proceedings of the 3rd {ACM} {Workshop} on {Wireless} {Security} and {Machine} {Learning}},
  title      = {Explainability-based {Backdoor} {Attacks} {Against} {Graph} {Neural} {Networks}},
  year       = {2021},
  address    = {New York, NY, USA},
  month      = jun,
  pages      = {31--36},
  publisher  = {Association for Computing Machinery},
  series     = {{WiseML} '21},
  abstract   = {Backdoor attacks represent a serious threat to neural network models. A backdoored model will misclassify the trigger-embedded inputs into an attacker-chosen target label while performing normally on other benign inputs. There are already numerous works on backdoor attacks on neural networks, but only a few works consider graph neural networks (GNNs). As such, there is no intensive research on explaining the impact of trigger injecting position on the performance of backdoor attacks on GNNs. To bridge this gap, we conduct an experimental investigation on the performance of backdoor attacks on GNNs. We apply two powerful GNN explainability approaches to select the optimal trigger injecting position to achieve two attacker objectives - high attack success rate and low clean accuracy drop. Our empirical results on benchmark datasets and state-of-the-art neural network models demonstrate the proposed method's effectiveness in selecting trigger injecting position for backdoor attacks on GNNs. For instance, on the node classification task, the backdoor attack with trigger injecting position selected by GraphLIME reaches over 84\% attack success rate with less than 2.5\% accuracy drop.},
  doi        = {10.1145/3468218.3469046},
  file       = {:Xu2021 - Explainability Based Backdoor Attacks against Graph Neural Networks.pdf:PDF},
  groups     = {To read, Attack},
  isbn       = {9781450385619},
  keywords   = {graph neural networks, explainability, backdoor attacks},
  readstatus = {skimmed},
  url        = {https://doi.org/10.1145/3468218.3469046},
  urldate    = {2022-10-17},
}

'
@Misc{,
  title      = {Trojaning Attack on Neural Networks - Trojaning Attack on Neural Networks},
  accessdate = {2022-10-17},
  file       = {:- Trojaning Attack on Neural Networks Trojaning Attack on Neural Networks.pdf:PDF},
  groups     = {Backdoors, Attack},
  ranking    = {rank4},
  readstatus = {read},
  url        = {https://docs.lib.purdue.edu/cgi/viewcontent.cgi?article=2782&context=cstech},
}

'
@InProceedings{Nguyen2020,
  author     = {Nguyen, Tuan Anh and Tran, Anh},
  booktitle  = {Advances in {Neural} {Information} {Processing} {Systems}},
  title      = {Input-{Aware} {Dynamic} {Backdoor} {Attack}},
  year       = {2020},
  pages      = {3454--3464},
  publisher  = {Curran Associates, Inc.},
  volume     = {33},
  abstract   = {In recent years, neural backdoor attack has been considered to be a potential security threat to deep learning systems. Such systems, while achieving the state-of-the-art performance on clean data, perform abnormally on inputs with predefined triggers. Current backdoor techniques, however, rely on uniform trigger patterns, which are easily detected and mitigated by current defense methods. In this work, we propose a novel backdoor attack technique in which the triggers vary from input to input. To achieve this goal, we implement an input-aware trigger generator driven by diversity loss. A novel cross-trigger test is applied to enforce trigger nonreusablity, making backdoor verification impossible. Experiments show that our method is efficient in various attack scenarios as well as multiple datasets. We further demonstrate that our backdoor can bypass the state of the art defense methods. An analysis with a famous neural network inspector again proves the stealthiness of the proposed attack. Our code is publicly available.},
  file       = {:Nguyen2020 - Input Aware Dynamic Backdoor Attack.pdf:PDF},
  groups     = {Backdoors, To read, Attack},
  readstatus = {read},
  url        = {https://proceedings.neurips.cc/paper/2020/hash/234e691320c0ad5b45ee3c96d0d7b8f8-Abstract.html},
  urldate    = {2022-10-17},
}

'
@TechReport{Weber2022,
  author     = {Weber, Maurice and Xu, Xiaojun and Karlaš, Bojan and Zhang, Ce and Li, Bo},
  title      = {{RAB}: {Provable} {Robustness} {Against} {Backdoor} {Attacks}},
  year       = {2022},
  month      = aug,
  note       = {arXiv:2003.08904 [cs, stat] type: article},
  abstract   = {Recent studies have shown that deep neural networks (DNNs) are vulnerable to adversarial attacks, including evasion and backdoor (poisoning) attacks. On the defense side, there have been intensive efforts on improving both empirical and provable robustness against evasion attacks; however, the provable robustness against backdoor attacks still remains largely unexplored. In this paper, we focus on certifying the machine learning model robustness against general threat models, especially backdoor attacks. We first provide a unified framework via randomized smoothing techniques and show how it can be instantiated to certify the robustness against both evasion and backdoor attacks. We then propose the first robust training process, RAB, to smooth the trained model and certify its robustness against backdoor attacks. We prove the robustness bound for machine learning models trained with RAB and prove that our robustness bound is tight. In addition, we theoretically show that it is possible to train the robust smoothed models efficiently for simple models such as K-nearest neighbor classifiers, and we propose an exact smooth-training algorithm that eliminates the need to sample from a noise distribution for such models. Empirically, we conduct comprehensive experiments for different machine learning (ML) models such as DNNs, support vector machines, and K-NN models on MNIST, CIFAR-10, and ImageNette datasets and provide the first benchmark for certified robustness against backdoor attacks. In addition, we evaluate K-NN models on a spambase tabular dataset to demonstrate the advantages of the proposed exact algorithm. Both the theoretic analysis and the comprehensive evaluation on diverse ML models and datasets shed light on further robust learning strategies against general training time attacks.},
  annote     = {Comment: IEEE Symposium on Security and Privacy 2023},
  doi        = {10.48550/arXiv.2003.08904},
  file       = {:Weber2022 - RAB_ Provable Robustness against Backdoor Attacks.pdf:PDF},
  groups     = {Defenses, Backdoors, To read},
  keywords   = {Computer Science - Machine Learning, Statistics - Machine Learning},
  school     = {arXiv},
  shorttitle = {{RAB}},
  url        = {http://arxiv.org/abs/2003.08904},
  urldate    = {2022-10-17},
}

'
@TechReport{Li2022,
  author     = {Li, Linyi and Xie, Tao and Li, Bo},
  title      = {{SoK}: {Certified} {Robustness} for {Deep} {Neural} {Networks}},
  year       = {2022},
  month      = sep,
  note       = {arXiv:2009.04131 [cs, stat] type: article},
  abstract   = {Great advances in deep neural networks (DNNs) have led to state-of-the-art performance on a wide range of tasks. However, recent studies have shown that DNNs are vulnerable to adversarial attacks, which have brought great concerns when deploying these models to safety-critical applications such as autonomous driving. Different defense approaches have been proposed against adversarial attacks, including: a) empirical defenses, which can usually be adaptively attacked again without providing robustness certification; and b) certifiably robust approaches, which consist of robustness verification providing the lower bound of robust accuracy against any attacks under certain conditions and corresponding robust training approaches. In this paper, we systematize certifiably robust approaches and related practical and theoretical implications and findings. We also provide the first comprehensive benchmark on existing robustness verification and training approaches on different datasets. In particular, we 1) provide a taxonomy for the robustness verification and training approaches, as well as summarize the methodologies for representative algorithms, 2) reveal the characteristics, strengths, limitations, and fundamental connections among these approaches, 3) discuss current research progresses, theoretical barriers, main challenges, and future directions for certifiably robust approaches for DNNs, and 4) provide an open-sourced unified platform to evaluate 20+ representative certifiably robust approaches.},
  annote     = {Comment: To appear at 2023 IEEE Symposium on Security and Privacy (SP); 14 pages for the main text; benchmark \& tool website: http://sokcertifiedrobustness.github.io/},
  doi        = {10.48550/arXiv.2009.04131},
  file       = {:Li2022 - SoK_ Certified Robustness for Deep Neural Networks.pdf:PDF},
  groups     = {Defenses, To read},
  keywords   = {Computer Science - Machine Learning, Computer Science - Cryptography and Security, Statistics - Machine Learning},
  school     = {arXiv},
  shorttitle = {{SoK}},
  url        = {http://arxiv.org/abs/2009.04131},
  urldate    = {2022-10-17},
}

'
@InProceedings{Liu2019,
  author     = {Liu, Yingqi and Lee, Wen-Chuan and Tao, Guanhong and Ma, Shiqing and Aafer, Yousra and Zhang, Xiangyu},
  booktitle  = {Proceedings of the 2019 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
  title      = {{ABS}: {Scanning} {Neural} {Networks} for {Back}-doors by {Artificial} {Brain} {Stimulation}},
  year       = {2019},
  address    = {New York, NY, USA},
  month      = nov,
  pages      = {1265--1282},
  publisher  = {Association for Computing Machinery},
  series     = {{CCS} '19},
  abstract   = {This paper presents a technique to scan neural network based AI models to determine if they are trojaned. Pre-trained AI models may contain back-doors that are injected through training or by transforming inner neuron weights. These trojaned models operate normally when regular inputs are provided, and mis-classify to a specific output label when the input is stamped with some special pattern called trojan trigger. We develop a novel technique that analyzes inner neuron behaviors by determining how output activations change when we introduce different levels of stimulation to a neuron. The neurons that substantially elevate the activation of a particular output label regardless of the provided input is considered potentially compromised. Trojan trigger is then reverse-engineered through an optimization procedure using the stimulation analysis results, to confirm that a neuron is truly compromised. We evaluate our system ABS on 177 trojaned models that are trojaned with various attack methods that target both the input space and the feature space, and have various trojan trigger sizes and shapes, together with 144 benign models that are trained with different data and initial weight values. These models belong to 7 different model structures and 6 different datasets, including some complex ones such as ImageNet, VGG-Face and ResNet110. Our results show that ABS is highly effective, can achieve over 90\% detection rate for most cases (and many 100\%), when only one input sample is provided for each output label. It substantially out-performs the state-of-the-art technique Neural Cleanse that requires a lot of input samples and small trojan triggers to achieve good performance.},
  doi        = {10.1145/3319535.3363216},
  file       = {:Liu2019 - ABS_ Scanning Neural Networks for Back Doors by Artificial Brain Stimulation.pdf:PDF},
  groups     = {Defenses},
  isbn       = {9781450367479},
  keywords   = {deep learning system, ai trojan attacks, artificial brain stimulation},
  readstatus = {read},
  shorttitle = {{ABS}},
  url        = {https://doi.org/10.1145/3319535.3363216},
  urldate    = {2022-10-20},
}

'
@InProceedings{Arp2022,
  author   = {Arp, Daniel and Quiring, Erwin and Pendlebury, Feargus and Warnecke, Alexander and Pierazzi, Fabio and Wressnegger, Christian and Cavallaro, Lorenzo and Rieck, Konrad},
  title    = {Dos and {Don}'ts of {Machine} {Learning} in {Computer} {Security}},
  year     = {2022},
  pages    = {3971--3988},
  file     = {:Arp2022 - Dos and Don'ts of Machine Learning in Computer Security.pdf:PDF},
  groups   = {Others, To read},
  isbn     = {9781939133311},
  language = {en},
  url      = {https://www.usenix.org/conference/usenixsecurity22/presentation/arp},
  urldate  = {2022-10-20},
}

'
@InProceedings{Pan2022,
  author   = {Pan, Xudong and Zhang, Mi and Sheng, Beina and Zhu, Jiaming and Yang, Min},
  title    = {Hidden {Trigger} {Backdoor} {Attack} on \{{NLP}\} {Models} via {Linguistic} {Style} {Manipulation}},
  year     = {2022},
  pages    = {3611--3628},
  file     = {:pan_hidden_2022 - Hidden Trigger Backdoor Attack on NLP_ Models Via Linguistic Style Manipulation.pdf:PDF},
  groups   = {To read, Attack},
  isbn     = {9781939133311},
  language = {en},
  url      = {https://www.usenix.org/conference/usenixsecurity22/presentation/pan-hidden},
  urldate  = {2022-10-20},
}

'
@TechReport{Qi2021,
  author     = {Qi, Fanchao and Chen, Yangyi and Zhang, Xurui and Li, Mukai and Liu, Zhiyuan and Sun, Maosong},
  title      = {Mind the {Style} of {Text}! {Adversarial} and {Backdoor} {Attacks} {Based} on {Text} {Style} {Transfer}},
  year       = {2021},
  month      = oct,
  note       = {arXiv:2110.07139 [cs] type: article},
  abstract   = {Adversarial attacks and backdoor attacks are two common security threats that hang over deep learning. Both of them harness task-irrelevant features of data in their implementation. Text style is a feature that is naturally irrelevant to most NLP tasks, and thus suitable for adversarial and backdoor attacks. In this paper, we make the first attempt to conduct adversarial and backdoor attacks based on text style transfer, which is aimed at altering the style of a sentence while preserving its meaning. We design an adversarial attack method and a backdoor attack method, and conduct extensive experiments to evaluate them. Experimental results show that popular NLP models are vulnerable to both adversarial and backdoor attacks based on text style transfer -- the attack success rates can exceed 90\% without much effort. It reflects the limited ability of NLP models to handle the feature of text style that has not been widely realized. In addition, the style transfer-based adversarial and backdoor attack methods show superiority to baselines in many aspects. All the code and data of this paper can be obtained at https://github.com/thunlp/StyleAttack.},
  annote     = {Comment: Accepted by the main conference of EMNLP 2021 as a long paper. The camera-ready version},
  doi        = {10.48550/arXiv.2110.07139},
  file       = {:Qi2021 - Mind the Style of Text! Adversarial and Backdoor Attacks Based on Text Style Transfer.pdf:PDF},
  groups     = {To read, Attack},
  keywords   = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
  readstatus = {read},
  school     = {arXiv},
  url        = {http://arxiv.org/abs/2110.07139},
  urldate    = {2022-10-20},
}

@Article{Bagdasaryan2020,
  author        = {Eugene Bagdasaryan and Vitaly Shmatikov},
  title         = {Blind Backdoors in Deep Learning Models},
  year          = {2020},
  month         = may,
  abstract      = {We investigate a new method for injecting backdoors into machine learning models, based on compromising the loss-value computation in the model-training code. We use it to demonstrate new classes of backdoors strictly more powerful than those in the prior literature: single-pixel and physical backdoors in ImageNet models, backdoors that switch the model to a covert, privacy-violating task, and backdoors that do not require inference-time input modifications. Our attack is blind: the attacker cannot modify the training data, nor observe the execution of his code, nor access the resulting model. The attack code creates poisoned training inputs "on the fly," as the model is training, and uses multi-objective optimization to achieve high accuracy on both the main and backdoor tasks. We show how a blind attack can evade any known defense and propose new ones.},
  archiveprefix = {arXiv},
  eprint        = {2005.03823},
  file          = {:Bagdasaryan2020 - Blind Backdoors in Deep Learning Models.pdf:PDF},
  groups        = {Attack, To read},
  keywords      = {cs.CR, cs.CV, cs.LG},
  primaryclass  = {cs.CR},
  readstatus    = {skimmed},
}

'
@TechReport{Aytekin2022,
  author     = {Aytekin, Caglar},
  title      = {Neural {Networks} are {Decision} {Trees}},
  year       = {2022},
  month      = oct,
  note       = {arXiv:2210.05189 [cs] type: article},
  abstract   = {In this manuscript, we show that any feedforward neural network having piece-wise linear activation functions can be represented as a decision tree. The representation is equivalence and not an approximation, thus keeping the accuracy of the neural network exactly as is. We believe that this work paves the way to tackle the black-box nature of neural networks. We share equivalent trees of some neural networks and show that besides providing interpretability, tree representation can also achieve some computational advantages. The analysis holds both for fully connected and convolutional networks, which may or may not also include skip connections and/or normalizations.},
  annote     = {Comment: This paper has significant overlaps with some other papers, kindly read \&cite others that are indicated in last paragraph of Introduction section},
  doi        = {10.48550/arXiv.2210.05189},
  file       = {:Aytekin2022 - Neural Networks Are Decision Trees.pdf:PDF},
  groups     = {Others, To read},
  keywords   = {Computer Science - Machine Learning},
  readstatus = {skimmed},
  school     = {arXiv},
  url        = {http://arxiv.org/abs/2210.05189},
  urldate    = {2022-10-21},
}

'
@InProceedings{Wang2018,
  author     = {Wang, Yaxing and Wu, Chenshen and Herranz, Luis and van de Weijer, Joost and Gonzalez-Garcia, Abel and Raducanu, Bogdan},
  title      = {Transferring {GANs}: generating images from limited data},
  year       = {2018},
  pages      = {218--234},
  file       = {:Wang2018 - Transferring GANs_ Generating Images from Limited Data.pdf:PDF},
  groups     = {To read, Others},
  shorttitle = {Transferring {GANs}},
  url        = {https://openaccess.thecvf.com/content_ECCV_2018/html/yaxing_wang_Transferring_GANs_generating_ECCV_2018_paper.html},
  urldate    = {2022-10-26},
}

@Article{Zhao2018,
  author     = {Zhao, Yue and Li, Meng and Lai, Liangzhen and Suda, Naveen and Civin, Damon and Chandra, Vikas},
  title      = {Federated {Learning} with {Non}-{IID} {Data}},
  year       = {2018},
  note       = {arXiv:1806.00582 [cs, stat]},
  abstract   = {Federated learning enables resource-constrained edge compute devices, such as mobile phones and IoT devices, to learn a shared model for prediction, while keeping the training data local. This decentralized approach to train models provides privacy, security, regulatory and economic benefits. In this work, we focus on the statistical challenge of federated learning when local data is non-IID. We first show that the accuracy of federated learning reduces significantly, by up to 55\% for neural networks trained for highly skewed non-IID data, where each client device trains only on a single class of data. We further show that this accuracy reduction can be explained by the weight divergence, which can be quantified by the earth mover's distance (EMD) between the distribution over classes on each device and the population distribution. As a solution, we propose a strategy to improve training on non-IID data by creating a small subset of data which is globally shared between all the edge devices. Experiments show that accuracy can be increased by 30\% for the CIFAR-10 dataset with only 5\% globally shared data.},
  doi        = {10.48550/arXiv.1806.00582},
  file       = {:Zhao2018 - Federated Learning with Non IID Data.pdf:PDF},
  groups     = {Others, To read},
  keywords   = {Computer Science - Machine Learning, Statistics - Machine Learning},
  readstatus = {read},
  url        = {http://arxiv.org/abs/1806.00582},
  urldate    = {2022-10-26},
}

'
@InProceedings{Gulrajani2017,
  author     = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},
  booktitle  = {Advances in {Neural} {Information} {Processing} {Systems}},
  title      = {Improved {Training} of {Wasserstein} {GANs}},
  year       = {2017},
  publisher  = {Curran Associates, Inc.},
  volume     = {30},
  abstract   = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only poor samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models with continuous generators. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.},
  file       = {:Gulrajani2017 - Improved Training of Wasserstein GANs.pdf:PDF},
  groups     = {Others, To read},
  readstatus = {read},
  url        = {https://proceedings.neurips.cc/paper/2017/hash/892c3b1c6dccd52936e27cbd0ff683d6-Abstract.html},
  urldate    = {2022-10-26},
}

'
@TechReport{Ji2019,
  author     = {Ji, Yu and Liu, Zixin and Hu, Xing and Wang, Peiqi and Zhang, Youhui},
  title      = {Programmable {Neural} {Network} {Trojan} for {Pre}-{Trained} {Feature} {Extractor}},
  year       = {2019},
  month      = jan,
  note       = {arXiv:1901.07766 [cs] type: article},
  abstract   = {Neural network (NN) trojaning attack is an emerging and important attack model that can broadly damage the system deployed with NN models. Existing studies have explored the outsourced training attack scenario and transfer learning attack scenario in some small datasets for specific domains, with limited numbers of fixed target classes. In this paper, we propose a more powerful trojaning attack method for both outsourced training attack and transfer learning attack, which outperforms existing studies in the capability, generality, and stealthiness. First, The attack is programmable that the malicious misclassification target is not fixed and can be generated on demand even after the victim's deployment. Second, our trojan attack is not limited in a small domain; one trojaned model on a large-scale dataset can affect applications of different domains that reuse its general features. Thirdly, our trojan design is hard to be detected or eliminated even if the victims fine-tune the whole model.},
  doi        = {10.48550/arXiv.1901.07766},
  file       = {:Ji2019 - Programmable Neural Network Trojan for Pre Trained Feature Extractor.pdf:PDF},
  groups     = {Backdoors, To read, Attack},
  keywords   = {Computer Science - Cryptography and Security, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
  readstatus = {read},
  relevance  = {relevant},
  school     = {arXiv},
  url        = {http://arxiv.org/abs/1901.07766},
  urldate    = {2022-11-10},
}

'
@TechReport{Fel2022,
  author     = {Fel, Thomas and Felipe, Ivan and Linsley, Drew and Serre, Thomas},
  title      = {Harmonizing the object recognition strategies of deep neural networks with humans},
  year       = {2022},
  month      = nov,
  note       = {arXiv:2211.04533 [cs] type: article},
  abstract   = {The many successes of deep neural networks (DNNs) over the past decade have largely been driven by computational scale rather than insights from biological intelligence. Here, we explore if these trends have also carried concomitant improvements in explaining the visual strategies humans rely on for object recognition. We do this by comparing two related but distinct properties of visual strategies in humans and DNNs: where they believe important visual features are in images and how they use those features to categorize objects. Across 84 different DNNs trained on ImageNet and three independent datasets measuring the where and the how of human visual strategies for object recognition on those images, we find a systematic trade-off between DNN categorization accuracy and alignment with human visual strategies for object recognition. State-of-the-art DNNs are progressively becoming less aligned with humans as their accuracy improves. We rectify this growing issue with our neural harmonizer: a general-purpose training routine that both aligns DNN and human visual strategies and improves categorization accuracy. Our work represents the first demonstration that the scaling laws that are guiding the design of DNNs today have also produced worse models of human vision. We release our code and data at https://serre-lab.github.io/Harmonization to help the field build more human-like DNNs.},
  annote     = {Comment: Published at NeurIPS 2022},
  doi        = {10.48550/arXiv.2211.04533},
  file       = {:Fel2022 - Harmonizing the Object Recognition Strategies of Deep Neural Networks with Humans.pdf:PDF},
  groups     = {Others},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
  readstatus = {read},
  school     = {arXiv},
  url        = {http://arxiv.org/abs/2211.04533},
  urldate    = {2022-11-11},
}

'
@InProceedings{Doan2021,
  author     = {Doan, Khoa and Lao, Yingjie and Zhao, Weijie and Li, Ping},
  title      = {{LIRA}: {Learnable}, {Imperceptible} and {Robust} {Backdoor} {Attacks}},
  year       = {2021},
  pages      = {11966--11976},
  file       = {:Doan2021 - LIRA_ Learnable, Imperceptible and Robust Backdoor Attacks.pdf:PDF},
  groups     = {Attack},
  language   = {en},
  readstatus = {read},
  shorttitle = {{LIRA}},
  url        = {https://openaccess.thecvf.com/content/ICCV2021/html/Doan_LIRA_Learnable_Imperceptible_and_Robust_Backdoor_Attacks_ICCV_2021_paper.html},
  urldate    = {2022-11-17},
}

'
@InProceedings{Bagdasaryan2021,
  author     = {Bagdasaryan, Eugene and Shmatikov, Vitaly},
  title      = {Blind {Backdoors} in {Deep} {Learning} {Models}},
  year       = {2021},
  pages      = {1505--1521},
  file       = {:Bagdasaryan2021 - Blind Backdoors in Deep Learning Models.pdf:PDF},
  isbn       = {9781939133243},
  language   = {en},
  readstatus = {skimmed},
  url        = {https://www.usenix.org/conference/usenixsecurity21/presentation/bagdasaryan},
  urldate    = {2022-11-23},
}

@InProceedings{Goldwasser2022,
  author   = {Goldwasser, Shafi and Kim, Michael and Vaikuntanathan, Vinod and Or, Mit and Ias, Zamir},
  title    = {Planting Undetectable Backdoors in Machine Learning Models},
  year     = {2022},
  month    = {4},
  abstract = {Given the computational cost and technical expertise required to train machine learning models, users may delegate the task of learning to a service provider. Delegation of learning has clear benefits, and at the same time raises serious concerns of trust. This work studies possible abuses of power by untrusted learners. We show how a malicious learner can plant an undetectable backdoor into a classifier. On the surface, such a backdoored classifier behaves normally, but in reality, the learner maintains a mechanism for changing the classification of any input, with only a slight perturbation. Importantly, without the appropriate "backdoor key," the mechanism is hidden and cannot be detected by any computationally-bounded observer. We demonstrate two frameworks for planting undetectable backdoors, with incomparable guarantees.},
  date     = {2022-04-14},
  day      = {14},
  eprint   = {arXiv:2204.06974v1[cs.LG]},
  file     = {:/home/gorka/Downloads/2204.06974.pdf:PDF},
}

@InProceedings{Saha1195,
  author     = {Saha, Aniruddha and Subramanya, Akshayvarun and Pirsiavash, Hamed},
  booktitle  = {The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI-20)},
  title      = {Hidden Trigger Backdoor Attacks},
  year       = {1195},
  abstract   = {With the success of deep learning algorithms in various domains, studying adversarial attacks to secure deep models in real world applications has become an important research topic. Backdoor attacks are a form of adversarial attacks on deep networks where the attacker provides poisoned data to the victim to train the model with, and then activates the attack by showing a specific small trigger pattern at the test time. Most state-of-the-art backdoor attacks either provide mislabeled poisoning data that is possible to identify by visual inspection, reveal the trigger in the poisoned data, or use noise to hide the trigger. We propose a novel form of backdoor attack where poisoned data look natural with correct labels and also more importantly, the attacker hides the trigger in the poisoned data and keeps the trigger secret until the test time. We perform an extensive study on various image classification settings and show that our attack can fool the model by pasting the trigger at random locations on unseen images although the model performs well on clean data. We also show that our proposed attack cannot be easily defended using a state-of-the-art defense algorithm for backdoor attacks.},
  file       = {:Saha1195 - Hidden Trigger Backdoor Attacks.pdf:PDF},
  groups     = {Backdoors, Attack},
  keywords   = {Vision},
  ranking    = {rank4},
  readstatus = {read},
}

 
@TechReport{Li2021a,
  author     = {Li, Yiming and Zhai, Tongqing and Wu, Baoyuan and Jiang, Yong and Li, Zhifeng and Xia, Shutao},
  title      = {Rethinking the {Trigger} of {Backdoor} {Attack}},
  year       = {2021},
  month      = jan,
  note       = {ZSCC: NoCitationData[s0] arXiv:2004.04692 [cs] type: article},
  abstract   = {Backdoor attack intends to inject hidden backdoor into the deep neural networks (DNNs), such that the prediction of the infected model will be maliciously changed if the hidden backdoor is activated by the attacker-defined trigger, while it performs well on benign samples. Currently, most of existing backdoor attacks adopted the setting of {\textbackslash}emph\{static\} trigger, \$i.e.,\$ triggers across the training and testing images follow the same appearance and are located in the same area. In this paper, we revisit this attack paradigm by analyzing the characteristics of the static trigger. We demonstrate that such an attack paradigm is vulnerable when the trigger in testing images is not consistent with the one used for training. We further explore how to utilize this property for backdoor defense, and discuss how to alleviate such vulnerability of existing attacks.},
  annote     = {Comment: 18 pages},
  doi        = {10.48550/arXiv.2004.04692},
  file       = {arXiv Fulltext PDF:https\://arxiv.org/pdf/2004.04692.pdf:application/pdf;arXiv.org Snapshot:http\://arxiv.org/abs/2004.04692:text/html},
  groups     = {Backdoors, Defenses},
  keywords   = {Computer Science - Cryptography and Security, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
  ranking    = {rank1},
  readstatus = {read},
  school     = {arXiv},
  url        = {http://arxiv.org/abs/2004.04692},
  urldate    = {2023-02-14},
}

@Misc{,
  title      = {Invisible Backdoor Attack With Sample-Specific Triggers - Li_Invisible_Backdoor_Attack_With_Sample-Specific_Triggers_ICCV_2021_paper},
  accessdate = {2023-02-14},
  file       = {:- Invisible Backdoor Attack with Sample Specific Triggers Li_Invisible_Backdoor_Attack_With_Sample Specific_Triggers_ICCV_2021_paper.pdf:PDF},
  groups     = {Attack, Backdoors},
  ranking    = {rank2},
  readstatus = {read},
  url        = {https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Invisible_Backdoor_Attack_With_Sample-Specific_Triggers_ICCV_2021_paper.pdf},
}

@InProceedings{Chen2000,
  author     = {Zhenzhu Chen and Shang Wang and Anmin Fu and Yansong Gao and Shui Yu and Robert H. Deng},
  title      = {LinkBreaker: Breaking the Backdoor-Trigger Link in DNNs via Neurons Consistency Check},
  year       = {2000},
  publisher  = {IEEE},
  abstract   = {IEEE Transactions on Information Forensics and Security;2022;17; ;10.1109/TIFS.2022.3175616},
  file       = {://wsl.localhost/Ubuntu/home/gorka/Git/references/LinkBreaker_Breaking_the_Backdoor-Trigger_Link_in_DNNs_via_Neurons_Consistency_Check.pdf:PDF},
  groups     = {Backdoors, Defenses},
  keywords   = {Backdoor attack, defense, deep learning, AI security},
  ranking    = {rank1},
  readstatus = {read},
}

 
@TechReport{Carlini2022,
  author     = {Carlini, Nicholas and Terzis, Andreas},
  title      = {Poisoning and {Backdooring} {Contrastive} {Learning}},
  year       = {2022},
  month      = mar,
  note       = {ZSCC: NoCitationData[s0] arXiv:2106.09667 [cs] type: article},
  abstract   = {Multimodal contrastive learning methods like CLIP train on noisy and uncurated training datasets. This is cheaper than labeling datasets manually, and even improves out-of-distribution robustness. We show that this practice makes backdoor and poisoning attacks a significant threat. By poisoning just 0.01\% of a dataset (e.g., just 300 images of the 3 million-example Conceptual Captions dataset), we can cause the model to misclassify test images by overlaying a small patch. Targeted poisoning attacks, whereby the model misclassifies a particular test input with an adversarially-desired label, are even easier requiring control of 0.0001\% of the dataset (e.g., just three out of the 3 million images). Our attacks call into question whether training on noisy and uncurated Internet scrapes is desirable.},
  doi        = {10.48550/arXiv.2106.09667},
  file       = {:carlini_poisoning_2022 - Poisoning and Backdooring Contrastive Learning.pdf:PDF;:Carlini2022 - Poisoning and Backdooring Contrastive Learning.html:URL},
  groups     = {Backdoors, Attack},
  keywords   = {Computer Science - Machine Learning},
  ranking    = {rank5},
  readstatus = {read},
  school     = {arXiv},
  url        = {http://arxiv.org/abs/2106.09667},
  urldate    = {2023-02-14},
}

 
@TechReport{Nguyen2021,
  author   = {Nguyen, Anh and Tran, Anh},
  title    = {{WaNet} -- {Imperceptible} {Warping}-based {Backdoor} {Attack}},
  year     = {2021},
  month    = mar,
  note     = {ZSCC: 0000135 arXiv:2102.10369 [cs] type: article},
  abstract = {With the thriving of deep learning and the widespread practice of using pre-trained networks, backdoor attacks have become an increasing security threat drawing many research interests in recent years. A third-party model can be poisoned in training to work well in normal conditions but behave maliciously when a trigger pattern appears. However, the existing backdoor attacks are all built on noise perturbation triggers, making them noticeable to humans. In this paper, we instead propose using warping-based triggers. The proposed backdoor outperforms the previous methods in a human inspection test by a wide margin, proving its stealthiness. To make such models undetectable by machine defenders, we propose a novel training mode, called the ``noise mode. The trained networks successfully attack and bypass the state-of-the-art defense methods on standard classification datasets, including MNIST, CIFAR-10, GTSRB, and CelebA. Behavior analyses show that our backdoors are transparent to network inspection, further proving this novel attack mechanism's efficiency.},
  annote   = {Comment: Accepted to ICLR 2021},
  doi      = {10.48550/arXiv.2102.10369},
  file     = {:nguyen_wanet_2021 - WaNet Imperceptible Warping Based Backdoor Attack.pdf:PDF;:Nguyen2021 - WaNet Imperceptible Warping Based Backdoor Attack.html:URL},
  groups   = {Attack, Backdoors},
  keywords = {Computer Science - Cryptography and Security, Computer Science - Computer Vision and Pattern Recognition},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2102.10369},
  urldate  = {2023-02-15},
}

@Misc{,
  title      = {Attention is All you Need - NIPS-2017-attention-is-all-you-need-Paper},
  accessdate = {2023-02-15},
  file       = {:- Attention Is All You Need NIPS 2017 Attention Is All You Need Paper.pdf:PDF},
  groups     = {Others},
  url        = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
}

 
@InProceedings{Liu2022,
  author    = {Liu, Qianhui and Xing, Dong and Feng, Lang and Tang, Huajin and Pan, Gang},
  booktitle = {{ICASSP} 2022 - 2022 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
  title     = {Event-{Based} {Multimodal} {Spiking} {Neural} {Network} with {Attention} {Mechanism}},
  year      = {2022},
  month     = may,
  note      = {ZSCC: 0000002 ISSN: 2379-190X},
  pages     = {8922--8926},
  abstract  = {Human brain can effectively integrate visual and auditory information. Dynamic Vision Sensor (DVS) and Dynamic Audio Sensor (DAS) are event-based sensors imitating the mechanism of human retina and cochlea. Since the sensors record the visual and auditory input as asynchronous discrete events, they are inherently suitable to cooperate with the spiking neural network (SNN). Existing works of SNNs for processing events mainly focus on unimodality, however, audiovisual multimodal SNNs are still limited. In this paper, we propose an end-to-end event-based multimodal spiking neural network. The network consists of visual and auditory unimodal subnetworks and a novel attention-based cross-modal subnetwork for fusion. The attention mechanism measures the significance of each modality and allocates the weights to two modalities. We evaluate our proposed multimodal network on an event-based audiovisual joint dataset (MNIST-DVS and N-TIDIGITS datasets). Experimental results show the performance improvement of this multimodal network and the effectiveness of our proposed attention mechanism.},
  doi       = {10.1109/ICASSP43922.2022.9746865},
  file      = {:liu_event-based_2022 - Event Based Multimodal Spiking Neural Network with Attention Mechanism.pdf:PDF},
  issn      = {2379-190X},
  keywords  = {Weight measurement, Visualization, Convolution, Conferences, Ear, Vision sensors, Retina, spiking neural networks, multimodal learning, dynamic vision sensors, dynamic audio sensors},
  priority  = {prio3},
}

 
@TechReport{Dosovitskiy2021,
  author     = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  title      = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
  year       = {2021},
  month      = jun,
  note       = {ZSCC: NoCitationData[s0] arXiv:2010.11929 [cs] version: 2 type: article},
  abstract   = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  annote     = {Comment: Fine-tuning code and pre-trained models are available at https://github.com/google-research/vision\_transformer. ICLR camera-ready version with 2 small modifications: 1) Added a discussion of CLS vs GAP classifier in the appendix, 2) Fixed an error in exaFLOPs computation in Figure 5 and Table 6 (relative performance of models is basically not affected)},
  doi        = {10.48550/arXiv.2010.11929},
  file       = {:dosovitskiy_image_2021 - An Image Is Worth 16x16 Words_ Transformers for Image Recognition at Scale.pdf:PDF;:Dosovitskiy2021 - An Image Is Worth 16x16 Words_ Transformers for Image Recognition at Scale.html:URL},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  ranking    = {rank5},
  readstatus = {read},
  school     = {arXiv},
  shorttitle = {An {Image} is {Worth} 16x16 {Words}},
  url        = {http://arxiv.org/abs/2010.11929},
  urldate    = {2023-02-16},
}

 
@TechReport{Subramanya2022,
  author     = {Subramanya, Akshayvarun and Saha, Aniruddha and Koohpayegani, Soroush Abbasi and Tejankar, Ajinkya and Pirsiavash, Hamed},
  title      = {Backdoor {Attacks} on {Vision} {Transformers}},
  year       = {2022},
  month      = jun,
  note       = {ZSCC: 0000002 arXiv:2206.08477 [cs] type: article},
  abstract   = {Vision Transformers (ViT) have recently demonstrated exemplary performance on a variety of vision tasks and are being used as an alternative to CNNs. Their design is based on a self-attention mechanism that processes images as a sequence of patches, which is quite different compared to CNNs. Hence it is interesting to study if ViTs are vulnerable to backdoor attacks. Backdoor attacks happen when an attacker poisons a small part of the training data for malicious purposes. The model performance is good on clean test images, but the attacker can manipulate the decision of the model by showing the trigger at test time. To the best of our knowledge, we are the first to show that ViTs are vulnerable to backdoor attacks. We also find an intriguing difference between ViTs and CNNs - interpretation algorithms effectively highlight the trigger on test images for ViTs but not for CNNs. Based on this observation, we propose a test-time image blocking defense for ViTs which reduces the attack success rate by a large margin. Code is available here: https://github.com/UCDvision/backdoor\_transformer.git},
  doi        = {10.48550/arXiv.2206.08477},
  file       = {:subramanya_backdoor_2022 - Backdoor Attacks on Vision Transformers.pdf:PDF;:Subramanya2022 - Backdoor Attacks on Vision Transformers.html:URL},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
  ranking    = {rank2},
  readstatus = {read},
  school     = {arXiv},
  url        = {http://arxiv.org/abs/2206.08477},
  urldate    = {2023-02-16},
}

 
@TechReport{Lv2021,
  author     = {Lv, Peizhuo and Ma, Hualong and Zhou, Jiachen and Liang, Ruigang and Chen, Kai and Zhang, Shengzhi and Yang, Yunfei},
  title      = {{DBIA}: {Data}-free {Backdoor} {Injection} {Attack} against {Transformer} {Networks}},
  year       = {2021},
  month      = nov,
  note       = {ZSCC: 0000004 arXiv:2111.11870 [cs] version: 1 type: article},
  abstract   = {Recently, transformer architecture has demonstrated its significance in both Natural Language Processing (NLP) and Computer Vision (CV) tasks. Though other network models are known to be vulnerable to the backdoor attack, which embeds triggers in the model and controls the model behavior when the triggers are presented, little is known whether such an attack is still valid on the transformer models and if so, whether it can be done in a more cost-efficient manner. In this paper, we propose DBIA, a novel data-free backdoor attack against the CV-oriented transformer networks, leveraging the inherent attention mechanism of transformers to generate triggers and injecting the backdoor using the poisoned surrogate dataset. We conducted extensive experiments based on three benchmark transformers, i.e., ViT, DeiT and Swin Transformer, on two mainstream image classification tasks, i.e., CIFAR10 and ImageNet. The evaluation results demonstrate that, consuming fewer resources, our approach can embed backdoors with a high success rate and a low impact on the performance of the victim transformers. Our code is available at https://anonymous.4open.science/r/DBIA-825D.},
  doi        = {10.48550/arXiv.2111.11870},
  file       = {:lv_dbia__2021 - DBIA_ Data Free Backdoor Injection Attack against Transformer Networks.pdf:PDF},
  groups     = {Attack, Backdoors},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
  ranking    = {rank3},
  readstatus = {read},
  school     = {arXiv},
  shorttitle = {{DBIA}},
  url        = {http://arxiv.org/abs/2111.11870},
  urldate    = {2023-02-17},
}

 
@TechReport{Chen2017,
  author     = {Chen, Xinyun and Liu, Chang and Li, Bo and Lu, Kimberly and Song, Dawn},
  title      = {Targeted {Backdoor} {Attacks} on {Deep} {Learning} {Systems} {Using} {Data} {Poisoning}},
  year       = {2017},
  month      = dec,
  note       = {ZSCC: 0000992 arXiv:1712.05526 [cs] type: article},
  abstract   = {Deep learning models have achieved high performance on many tasks, and thus have been applied to many security-critical scenarios. For example, deep learning-based face recognition systems have been used to authenticate users to access many security-sensitive applications like payment apps. Such usages of deep learning systems provide the adversaries with sufficient incentives to perform attacks against these systems for their adversarial purposes. In this work, we consider a new type of attacks, called backdoor attacks, where the attacker's goal is to create a backdoor into a learning-based authentication system, so that he can easily circumvent the system by leveraging the backdoor. Specifically, the adversary aims at creating backdoor instances, so that the victim learning system will be misled to classify the backdoor instances as a target label specified by the adversary. In particular, we study backdoor poisoning attacks, which achieve backdoor attacks using poisoning strategies. Different from all existing work, our studied poisoning strategies can apply under a very weak threat model: (1) the adversary has no knowledge of the model and the training set used by the victim system; (2) the attacker is allowed to inject only a small amount of poisoning samples; (3) the backdoor key is hard to notice even by human beings to achieve stealthiness. We conduct evaluation to demonstrate that a backdoor adversary can inject only around 50 poisoning samples, while achieving an attack success rate of above 90\%. We are also the first work to show that a data poisoning attack can create physically implementable backdoors without touching the training process. Our work demonstrates that backdoor poisoning attacks pose real threats to a learning system, and thus highlights the importance of further investigation and proposing defense strategies against them.},
  doi        = {10.48550/arXiv.1712.05526},
  file       = {:chen_targeted_2017 - Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning.pdf:PDF},
  keywords   = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
  ranking    = {rank2},
  readstatus = {skimmed},
  school     = {arXiv},
  url        = {http://arxiv.org/abs/1712.05526},
  urldate    = {2023-02-21},
}

 
@TechReport{Quiring2020,
  author   = {Quiring, Erwin and Rieck, Konrad},
  title    = {Backdooring and {Poisoning} {Neural} {Networks} with {Image}-{Scaling} {Attacks}},
  year     = {2020},
  month    = mar,
  note     = {ZSCC: 0000048 arXiv:2003.08633 [cs] type: article},
  abstract = {Backdoors and poisoning attacks are a major threat to the security of machine-learning and vision systems. Often, however, these attacks leave visible artifacts in the images that can be visually detected and weaken the efficacy of the attacks. In this paper, we propose a novel strategy for hiding backdoor and poisoning attacks. Our approach builds on a recent class of attacks against image scaling. These attacks enable manipulating images such that they change their content when scaled to a specific resolution. By combining poisoning and image-scaling attacks, we can conceal the trigger of backdoors as well as hide the overlays of clean-label poisoning. Furthermore, we consider the detection of image-scaling attacks and derive an adaptive attack. In an empirical evaluation, we demonstrate the effectiveness of our strategy. First, we show that backdoors and poisoning work equally well when combined with image-scaling attacks. Second, we demonstrate that current detection defenses against image-scaling attacks are insufficient to uncover our manipulations. Overall, our work provides a novel means for hiding traces of manipulations, being applicable to different poisoning approaches.},
  annote   = {Comment: IEEE Deep Learning and Security Workshop (DLS) 2020},
  doi      = {10.48550/arXiv.2003.08633},
  file     = {:quiring_backdooring_2020 - Backdooring and Poisoning Neural Networks with Image Scaling Attacks.pdf:PDF;:Quiring2020 - Backdooring and Poisoning Neural Networks with Image Scaling Attacks.html:URL},
  keywords = {Computer Science - Cryptography and Security, Computer Science - Computer Vision and Pattern Recognition},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2003.08633},
  urldate  = {2023-02-21},
}

 
@TechReport{Zhao2020,
  author   = {Zhao, Shihao and Ma, Xingjun and Zheng, Xiang and Bailey, James and Chen, Jingjing and Jiang, Yu-Gang},
  title    = {Clean-{Label} {Backdoor} {Attacks} on {Video} {Recognition} {Models}},
  year     = {2020},
  month    = jun,
  note     = {ZSCC: 0000145 arXiv:2003.03030 [cs] type: article},
  abstract = {Deep neural networks (DNNs) are vulnerable to backdoor attacks which can hide backdoor triggers in DNNs by poisoning training data. A backdoored model behaves normally on clean test images, yet consistently predicts a particular target class for any test examples that contain the trigger pattern. As such, backdoor attacks are hard to detect, and have raised severe security concerns in real-world applications. Thus far, backdoor research has mostly been conducted in the image domain with image classification models. In this paper, we show that existing image backdoor attacks are far less effective on videos, and outline 4 strict conditions where existing attacks are likely to fail: 1) scenarios with more input dimensions (eg. videos), 2) scenarios with high resolution, 3) scenarios with a large number of classes and few examples per class (a "sparse dataset"), and 4) attacks with access to correct labels (eg. clean-label attacks). We propose the use of a universal adversarial trigger as the backdoor trigger to attack video recognition models, a situation where backdoor attacks are likely to be challenged by the above 4 strict conditions. We show on benchmark video datasets that our proposed backdoor attack can manipulate state-of-the-art video models with high success rates by poisoning only a small proportion of training data (without changing the labels). We also show that our proposed backdoor attack is resistant to state-of-the-art backdoor defense/detection methods, and can even be applied to improve image backdoor attacks. Our proposed video backdoor attack not only serves as a strong baseline for improving the robustness of video models, but also provides a new perspective for more understanding more powerful backdoor attacks.},
  annote   = {Comment: CVPR2020},
  doi      = {10.48550/arXiv.2003.03030},
  file     = {:zhao_clean-label_2020 - Clean Label Backdoor Attacks on Video Recognition Models.pdf:PDF;:Zhao2020 - Clean Label Backdoor Attacks on Video Recognition Models.html:URL},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  priority = {prio2},
  ranking  = {rank1},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2003.03030},
  urldate  = {2023-02-21},
}

 
@TechReport{Tancik2020,
  author     = {Tancik, Matthew and Mildenhall, Ben and Ng, Ren},
  title      = {{StegaStamp}: {Invisible} {Hyperlinks} in {Physical} {Photographs}},
  year       = {2020},
  month      = mar,
  note       = {ZSCC: 0000138 arXiv:1904.05343 [cs] type: article},
  abstract   = {Printed and digitally displayed photos have the ability to hide imperceptible digital data that can be accessed through internet-connected imaging systems. Another way to think about this is physical photographs that have unique QR codes invisibly embedded within them. This paper presents an architecture, algorithms, and a prototype implementation addressing this vision. Our key technical contribution is StegaStamp, a learned steganographic algorithm to enable robust encoding and decoding of arbitrary hyperlink bitstrings into photos in a manner that approaches perceptual invisibility. StegaStamp comprises a deep neural network that learns an encoding/decoding algorithm robust to image perturbations approximating the space of distortions resulting from real printing and photography. We demonstrates real-time decoding of hyperlinks in photos from in-the-wild videos that contain variation in lighting, shadows, perspective, occlusion and viewing distance. Our prototype system robustly retrieves 56 bit hyperlinks after error correction - sufficient to embed a unique code within every photo on the internet.},
  annote     = {Comment: CVPR 2020, Project page: http://www.matthewtancik.com/stegastamp},
  doi        = {10.48550/arXiv.1904.05343},
  file       = {:tancik_stegastamp__2020 - StegaStamp_ Invisible Hyperlinks in Physical Photographs.pdf:PDF;:Tancik2020 - StegaStamp_ Invisible Hyperlinks in Physical Photographs.html:URL},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  ranking    = {rank3},
  readstatus = {skimmed},
  school     = {arXiv},
  shorttitle = {{StegaStamp}},
  url        = {http://arxiv.org/abs/1904.05343},
  urldate    = {2023-02-21},
}

 
@TechReport{Carlini2023,
  author   = {Carlini, Nicholas and Jagielski, Matthew and Choquette-Choo, Christopher A. and Paleka, Daniel and Pearce, Will and Anderson, Hyrum and Terzis, Andreas and Thomas, Kurt and Tramèr, Florian},
  title    = {Poisoning {Web}-{Scale} {Training} {Datasets} is {Practical}},
  year     = {2023},
  month    = feb,
  note     = {ZSCC: NoCitationData[s0] arXiv:2302.10149 [cs] type: article},
  abstract = {Deep learning models are often trained on distributed, webscale datasets crawled from the internet. In this paper, we introduce two new dataset poisoning attacks that intentionally introduce malicious examples to a model's performance. Our attacks are immediately practical and could, today, poison 10 popular datasets. Our first attack, split-view poisoning, exploits the mutable nature of internet content to ensure a dataset annotator's initial view of the dataset differs from the view downloaded by subsequent clients. By exploiting specific invalid trust assumptions, we show how we could have poisoned 0.01\% of the LAION-400M or COYO-700M datasets for just \$60 USD. Our second attack, frontrunning poisoning, targets web-scale datasets that periodically snapshot crowd-sourced content -- such as Wikipedia -- where an attacker only needs a time-limited window to inject malicious examples. In light of both attacks, we notify the maintainers of each affected dataset and recommended several low-overhead defenses.},
  doi      = {10.48550/arXiv.2302.10149},
  file     = {:carlini_poisoning_2023 - Poisoning Web Scale Training Datasets Is Practical.pdf:PDF;:Carlini2023 - Poisoning Web Scale Training Datasets Is Practical.html:URL},
  keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2302.10149},
  urldate  = {2023-02-22},
}

 
@TechReport{Turner2019,
  author     = {Turner, Alexander and Tsipras, Dimitris and Madry, Aleksander},
  title      = {Label-{Consistent} {Backdoor} {Attacks}},
  year       = {2019},
  month      = dec,
  note       = {ZSCC: 0000160 arXiv:1912.02771 [cs, stat] type: article},
  abstract   = {Deep neural networks have been demonstrated to be vulnerable to backdoor attacks. Specifically, by injecting a small number of maliciously constructed inputs into the training set, an adversary is able to plant a backdoor into the trained model. This backdoor can then be activated during inference by a backdoor trigger to fully control the model's behavior. While such attacks are very effective, they crucially rely on the adversary injecting arbitrary inputs that are---often blatantly---mislabeled. Such samples would raise suspicion upon human inspection, potentially revealing the attack. Thus, for backdoor attacks to remain undetected, it is crucial that they maintain label-consistency---the condition that injected inputs are consistent with their labels. In this work, we leverage adversarial perturbations and generative models to execute efficient, yet label-consistent, backdoor attacks. Our approach is based on injecting inputs that appear plausible, yet are hard to classify, hence causing the model to rely on the (easier-to-learn) backdoor trigger.},
  doi        = {10.48550/arXiv.1912.02771},
  file       = {:turner_label-consistent_2019 - Label Consistent Backdoor Attacks.pdf:PDF;:Turner2019 - Label Consistent Backdoor Attacks.html:URL},
  keywords   = {Statistics - Machine Learning, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
  readstatus = {read},
  school     = {arXiv},
  url        = {http://arxiv.org/abs/1912.02771},
  urldate    = {2023-02-22},
}

 
@TechReport{Madry2019,
  author   = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  title    = {Towards {Deep} {Learning} {Models} {Resistant} to {Adversarial} {Attacks}},
  year     = {2019},
  month    = sep,
  note     = {ZSCC: NoCitationData[s0] arXiv:1706.06083 [cs, stat] type: article},
  abstract = {Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist\_challenge and https://github.com/MadryLab/cifar10\_challenge.},
  annote   = {Comment: ICLR'18},
  doi      = {10.48550/arXiv.1706.06083},
  file     = {:madry_towards_2019 - Towards Deep Learning Models Resistant to Adversarial Attacks.pdf:PDF;:Madry2019 - Towards Deep Learning Models Resistant to Adversarial Attacks.html:URL},
  keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
  priority = {prio1},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1706.06083},
  urldate  = {2023-02-22},
}

@Article{Chen2021a,
  author     = {Xuan Chen and Yuena Ma and Shiwei Lu},
  journal    = {IEEE Access},
  title      = {Use Procedural Noise to Achieve Backdoor Attack},
  year       = {2021},
  issn       = {2169-3536},
  pages      = {127204--127216},
  volume     = {9},
  abstract   = {In recent years, more researchers pay their attention to the security of artificial intelligence. The backdoor attack is one of the threats and has a powerful, stealthy attack ability. There exist a growing trend towards the triggers is that become dynamic and global. In this paper, we propose a novel global backdoor trigger that is generated by procedural noise. Compared with most triggers, ours are much stealthy and straightforward to implement. In fact, there exist three types of procedural noise, and we evaluate the attack ability of triggers generated by them on the different classification datasets, including CIFAR-10, GTSRB, CelebA, and ImageNet12. The experiment results show that our attack approach can bypass most defense approaches, even for the inspections of humans. We only need poison 5%–10% training data, but the attack success rate(ASR) can reach over 99%. To test the robustness of the backdoor model against the corruption methods that in practice, we introduce 17 corruption methods and compute the accuracy, ASR of the backdoor model with them. The facts show that our backdoor model has strong robustness for most corruption methods, which means it can be applied in reality. Our code is available at https://github.com/928082786/pnoiseattack.},
  date       = {2021},
  doi        = {10.1109/ACCESS.2021.3110239},
  file       = {:Chen2021a - Use Procedural Noise to Achieve Backdoor Attack.html:URL;:Chen2021a - Use Procedural Noise to Achieve Backdoor Attack.pdf:PDF},
  keywords   = {Data models, Computational modeling, Training, Deep learning, Training data, Toxicology, Robustness, backdoor attack, procedural noise, global trigger},
  publisher  = {IEEE},
  ranking    = {rank2},
  readstatus = {read},
}

 
@TechReport{Liu2022a,
  author   = {Liu, Yang and Zhang, Yao and Wang, Yixin and Hou, Feng and Yuan, Jin and Tian, Jiang and Zhang, Yang and Shi, Zhongchao and Fan, Jianping and He, Zhiqiang},
  title    = {A {Survey} of {Visual} {Transformers}},
  year     = {2022},
  month    = dec,
  note     = {ZSCC: NoCitationData[s0] arXiv:2111.06091 [cs] type: article},
  abstract = {Transformer, an attention-based encoder-decoder model, has already revolutionized the field of natural language processing (NLP). Inspired by such significant achievements, some pioneering works have recently been done on employing Transformer-liked architectures in the computer vision (CV) field, which have demonstrated their effectiveness on three fundamental CV tasks (classification, detection, and segmentation) as well as multiple sensory data stream (images, point clouds, and vision-language data). Because of their competitive modeling capabilities, the visual Transformers have achieved impressive performance improvements over multiple benchmarks as compared with modern Convolution Neural Networks (CNNs). In this survey, we have reviewed over one hundred of different visual Transformers comprehensively according to three fundamental CV tasks and different data stream types, where a taxonomy is proposed to organize the representative methods according to their motivations, structures, and application scenarios. Because of their differences on training settings and dedicated vision tasks, we have also evaluated and compared all these existing visual Transformers under different configurations. Furthermore, we have revealed a series of essential but unexploited aspects that may empower such visual Transformers to stand out from numerous architectures, e.g., slack high-level semantic embeddings to bridge the gap between the visual Transformers and the sequential ones. Finally, three promising research directions are suggested for future investment. We will continue to update the latest articles and their released source codes at https://github.com/liuyang-ict/awesome-visual-transformers.},
  annote   = {Comment: Accepted by IEEE Transactions on Neural Networks and Learning Systems (TNNLS)},
  doi      = {10.48550/arXiv.2111.06091},
  file     = {:liu_survey_2022 - A Survey of Visual Transformers.pdf:PDF;:Liu2022a - A Survey of Visual Transformers.html:URL},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  priority = {prio1},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2111.06091},
  urldate  = {2023-02-27},
}

 
@InProceedings{Bagdasaryan2022,
  author     = {Bagdasaryan, Eugene and Shmatikov, Vitaly},
  booktitle  = {2022 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
  title      = {Spinning {Language} {Models}: {Risks} of {Propaganda}-{As}-{A}-{Service} and {Countermeasures}},
  year       = {2022},
  month      = may,
  note       = {ZSCC: 0000007 ISSN: 2375-1207},
  pages      = {769--786},
  abstract   = {We investigate a new threat to neural sequence-to-sequence (seq2seq) models: training-time attacks that cause models to “spin” their outputs so as to support an adversary-chosen sentiment or point of view—but only when the input contains adversary-chosen trigger words. For example, a spinned 1 summarization model outputs positive summaries of any text that mentions the name of some individual or organization.Model spinning introduces a “meta-backdoor” into a model. Whereas conventional backdoors cause models to produce incorrect outputs on inputs with the trigger, outputs of spinned models preserve context and maintain standard accuracy metrics, yet also satisfy a meta-task chosen by the adversary.Model spinning enables propaganda-as-a-service, where propaganda is defined as biased speech. An adversary can create customized language models that produce desired spins for chosen triggers, then deploy these models to generate disinformation (a platform attack), or else inject them into ML training pipelines (a supply-chain attack), transferring malicious functionality to downstream models trained by victims.To demonstrate the feasibility of model spinning, we develop a new backdooring technique. It stacks an adversarial meta-task (e.g., sentiment analysis) onto a seq2seq model, backpropagates the desired meta-task output (e.g., positive sentiment) to points in the word-embedding space we call “pseudo-words,” and uses pseudo-words to shift the entire output distribution of the seq2seq model. We evaluate this attack on language generation, summarization, and translation models with different triggers and meta-tasks such as sentiment, toxicity, and entailment. Spinned models largely maintain their accuracy metrics (ROUGE and BLEU) while shifting their outputs to satisfy the adversary’s meta-task. We also show that, in the case of a supply-chain attack, the spin functionality transfers to downstream models.Finally, we propose a black-box, meta-task-independent defense that, given a list of candidate triggers, can detect models that selectively apply spin to inputs with any of these triggers.1We use “spinned” rather than “spun” to match how the word is used in public relations.},
  doi        = {10.1109/SP46214.2022.9833572},
  file       = {:Bagdasaryan2022 - Spinning Language Models_ Risks of Propaganda As a Service and Countermeasures.html:URL;:Bagdasaryan2022 - Spinning Language Models_ Risks of Propaganda As a Service and Countermeasures.pdf:PDF},
  issn       = {2375-1207},
  keywords   = {Measurement, Training, Analytical models, Toxicology, Training data, Media, Data models},
  priority   = {prio1},
  shorttitle = {Spinning {Language} {Models}},
}

 
@TechReport{Liu2023,
  author     = {Liu, Yugeng and Li, Zheng and Backes, Michael and Shen, Yun and Zhang, Yang},
  title      = {Backdoor {Attacks} {Against} {Dataset} {Distillation}},
  year       = {2023},
  month      = jan,
  note       = {ZSCC: 0000001 arXiv:2301.01197 [cs] type: article},
  abstract   = {Dataset distillation has emerged as a prominent technique to improve data efficiency when training machine learning models. It encapsulates the knowledge from a large dataset into a smaller synthetic dataset. A model trained on this smaller distilled dataset can attain comparable performance to a model trained on the original training dataset. However, the existing dataset distillation techniques mainly aim at achieving the best trade-off between resource usage efficiency and model utility. The security risks stemming from them have not been explored. This study performs the first backdoor attack against the models trained on the data distilled by dataset distillation models in the image domain. Concretely, we inject triggers into the synthetic data during the distillation procedure rather than during the model training stage, where all previous attacks are performed. We propose two types of backdoor attacks, namely NAIVEATTACK and DOORPING. NAIVEATTACK simply adds triggers to the raw data at the initial distillation phase, while DOORPING iteratively updates the triggers during the entire distillation procedure. We conduct extensive evaluations on multiple datasets, architectures, and dataset distillation techniques. Empirical evaluation shows that NAIVEATTACK achieves decent attack success rate (ASR) scores in some cases, while DOORPING reaches higher ASR scores (close to 1.0) in all cases. Furthermore, we conduct a comprehensive ablation study to analyze the factors that may affect the attack performance. Finally, we evaluate multiple defense mechanisms against our backdoor attacks and show that our attacks can practically circumvent these defense mechanisms.},
  doi        = {10.48550/arXiv.2301.01197},
  file       = {:liu_backdoor_2023 - Backdoor Attacks against Dataset Distillation.pdf:PDF;:Liu2023 - Backdoor Attacks against Dataset Distillation.html:URL},
  keywords   = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
  ranking    = {rank2},
  readstatus = {read},
  school     = {arXiv},
  url        = {http://arxiv.org/abs/2301.01197},
  urldate    = {2023-03-01},
}

 
@TechReport{Sha2022,
  author     = {Sha, Zeyang and He, Xinlei and Berrang, Pascal and Humbert, Mathias and Zhang, Yang},
  title      = {Fine-{Tuning} {Is} {All} {You} {Need} to {Mitigate} {Backdoor} {Attacks}},
  year       = {2022},
  month      = dec,
  note       = {ZSCC: 0000000 arXiv:2212.09067 [cs] type: article},
  abstract   = {Backdoor attacks represent one of the major threats to machine learning models. Various efforts have been made to mitigate backdoors. However, existing defenses have become increasingly complex and often require high computational resources or may also jeopardize models' utility. In this work, we show that fine-tuning, one of the most common and easy-to-adopt machine learning training operations, can effectively remove backdoors from machine learning models while maintaining high model utility. Extensive experiments over three machine learning paradigms show that fine-tuning and our newly proposed super-fine-tuning achieve strong defense performance. Furthermore, we coin a new term, namely backdoor sequela, to measure the changes in model vulnerabilities to other attacks before and after the backdoor has been removed. Empirical evaluation shows that, compared to other defense methods, super-fine-tuning leaves limited backdoor sequela. We hope our results can help machine learning model owners better protect their models from backdoor threats. Also, it calls for the design of more advanced attacks in order to comprehensively assess machine learning models' backdoor vulnerabilities.},
  annote     = {Comment: 17 pages, 17 figures},
  doi        = {10.48550/arXiv.2212.09067},
  file       = {:sha_fine-tuning_2022 - Fine Tuning Is All You Need to Mitigate Backdoor Attacks.pdf:PDF;:Sha2022 - Fine Tuning Is All You Need to Mitigate Backdoor Attacks.html:URL},
  keywords   = {Computer Science - Cryptography and Security, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
  ranking    = {rank2},
  readstatus = {read},
  school     = {arXiv},
  url        = {http://arxiv.org/abs/2212.09067},
  urldate    = {2023-03-01},
}

@Conference{Zhang2021,
  author     = {Quan Zhang and Yifeng Ding and Yongqiang Tian and Jianmin Guo and Min Yuan and Yu Jiang},
  booktitle  = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
  title      = {AdvDoor: adversarial backdoor attack of deep learning system},
  year       = {2021},
  address    = {New York, NY, USA},
  month      = {7},
  pages      = {127–138},
  publisher  = {Association for Computing Machinery},
  series     = {ISSTA 2021},
  abstract   = {Deep Learning (DL) system has been widely used in many critical applications, such as autonomous vehicles and unmanned aerial vehicles. However, their security is threatened by backdoor attack, which is achieved by adding artificial patterns on specific training data. Existing attack methods normally poison the data using a patch, and they can be easily detected by existing detection methods. In this work, we propose the Adversarial Backdoor, which utilizes the Targeted Universal Adversarial Perturbation (TUAP) to hide the anomalies in DL models and confuse existing powerful detection methods. With extensive experiments, it is demonstrated that Adversarial Backdoor can be injected stably with an attack success rate around 98%. Moreover, Adversarial Backdoor can bypass state-of-the-art backdoor detection methods. More specifically, only around 37% of the poisoned models can be caught, and less than 29% of the poisoned data cannot bypass the detection. In contrast, for the patch backdoor, all the poisoned models and more than 80% of the poisoned data will be detected. This work intends to alarm the researchers and developers of this potential threat and to inspire the designing of effective detection methods.},
  day        = {11},
  doi        = {10.1145/3460319.3464809},
  file       = {:Zhang2021 - AdvDoor_ Adversarial Backdoor Attack of Deep Learning System.pdf:PDF},
  isbn       = {9781450384599},
  keywords   = {Adversarial Attack, Backdoor Attack, Deep Learning System},
  location   = {Virtual, Denmark},
  pagetotal  = {12},
  ranking    = {rank1},
  readstatus = {read},
  url        = {https://doi.org/10.1145/3460319.3464809},
}

@Article{Chen2022,
  author    = {Chien-Lun Chen and Sara Babakniya and Marco Paolieri and Leana Golubchik},
  title     = {Defending against Poisoning Backdoor Attacks on Federated Meta-learning},
  year      = {2022},
  month     = {9},
  pages     = {1–25},
  abstract  = {Federated learning allows multiple users to collaboratively train a shared classification model while preserving data privacy. This approach, where model updates are aggregated by a central server, was shown to be vulnerable to poisoning backdoor attacks: a malicious user can alter the shared model to arbitrarily classify specific inputs from a given class. In this article, we analyze the effects of backdoor attacks on federated meta-learning, where users train a model that can be adapted to different sets of output classes using only a few examples. While the ability to adapt could, in principle, make federated learning frameworks more robust to backdoor attacks (when new training examples are benign), we find that even one-shot attacks can be very successful and persist after additional training. To address these vulnerabilities, we propose a defense mechanism inspired by matching networks, where the class of an input is predicted from the similarity of its features with a support set of labeled examples. By removing the decision logic from the model shared with the federation, the success and persistence of backdoor attacks are greatly reduced.},
  address   = {New York, NY, USA},
  booktitle = {ACM Trans. Intell. Syst. Technol.},
  day       = {23},
  doi       = {10.1145/3523062},
  file      = {:Chen2022 - Defending against Poisoning Backdoor Attacks on Federated Meta Learning.html:URL},
  keywords  = {Federated learning, attention mechanism, backdoor attacks, matching networks, meta-learning, poisoning attacks, security and privacy},
  pagetotal = {25},
  priority  = {prio1},
  publisher = {Association for Computing Machinery},
  url       = {https://doi.org/10.1145/3523062},
}

@Article{Wenger2020,
  author        = {Emily Wenger and Josephine Passananti and Arjun Bhagoji and Yuanshun Yao and Haitao Zheng and Ben Y. Zhao},
  title         = {Backdoor Attacks Against Deep Learning Systems in the Physical World},
  year          = {2020},
  month         = jun,
  abstract      = {Backdoor attacks embed hidden malicious behaviors into deep learning models, which only activate and cause misclassifications on model inputs containing a specific trigger. Existing works on backdoor attacks and defenses, however, mostly focus on digital attacks that use digitally generated patterns as triggers. A critical question remains unanswered: can backdoor attacks succeed using physical objects as triggers, thus making them a credible threat against deep learning systems in the real world? We conduct a detailed empirical study to explore this question for facial recognition, a critical deep learning task. Using seven physical objects as triggers, we collect a custom dataset of 3205 images of ten volunteers and use it to study the feasibility of physical backdoor attacks under a variety of real-world conditions. Our study reveals two key findings. First, physical backdoor attacks can be highly successful if they are carefully configured to overcome the constraints imposed by physical objects. In particular, the placement of successful triggers is largely constrained by the target model's dependence on key facial features. Second, four of today's state-of-the-art defenses against (digital) backdoors are ineffective against physical backdoors, because the use of physical objects breaks core assumptions used to construct these defenses. Our study confirms that (physical) backdoor attacks are not a hypothetical phenomenon but rather pose a serious real-world threat to critical classification tasks. We need new and more robust defenses against backdoors in the physical world.},
  archiveprefix = {arXiv},
  eprint        = {2006.14580},
  file          = {:- Backdoor Attacks against Deep Learning Systems in the Physical World.pdf:PDF},
  keywords      = {cs.CV, cs.CR, cs.LG},
  primaryclass  = {cs.CV},
}

@Article{Luo2022,
  author        = {Chengxiao Luo and Yiming Li and Yong Jiang and Shu-Tao Xia},
  title         = {Untargeted Backdoor Attack against Object Detection},
  year          = {2022},
  month         = nov,
  abstract      = {Recent studies revealed that deep neural networks (DNNs) are exposed to backdoor threats when training with third-party resources (such as training samples or backbones). The backdoored model has promising performance in predicting benign samples, whereas its predictions can be maliciously manipulated by adversaries based on activating its backdoors with pre-defined trigger patterns. Currently, most of the existing backdoor attacks were conducted on the image classification under the targeted manner. In this paper, we reveal that these threats could also happen in object detection, posing threatening risks to many mission-critical applications ($e.g.$, pedestrian detection and intelligent surveillance systems). Specifically, we design a simple yet effective poison-only backdoor attack in an untargeted manner, based on task characteristics. We show that, once the backdoor is embedded into the target model by our attack, it can trick the model to lose detection of any object stamped with our trigger patterns. We conduct extensive experiments on the benchmark dataset, showing its effectiveness in both digital and physical-world settings and its resistance to potential defenses.},
  archiveprefix = {arXiv},
  eprint        = {2211.05638},
  file          = {:http\://arxiv.org/pdf/2211.05638v1:PDF},
  keywords      = {cs.CV, cs.AI, cs.CR, cs.LG},
  primaryclass  = {cs.CV},
}

@Article{Ma2022,
  author        = {Hua Ma and Yinshan Li and Yansong Gao and Alsharif Abuadbba and Zhi Zhang and Anmin Fu and Hyoungshick Kim and Said F. Al-Sarawi and Nepal Surya and Derek Abbott},
  title         = {Dangerous Cloaking: Natural Trigger based Backdoor Attacks on Object Detectors in the Physical World},
  year          = {2022},
  month         = jan,
  abstract      = {Deep learning models have been shown to be vulnerable to recent backdoor attacks. A backdoored model behaves normally for inputs containing no attacker-secretly-chosen trigger and maliciously for inputs with the trigger. To date, backdoor attacks and countermeasures mainly focus on image classification tasks. And most of them are implemented in the digital world with digital triggers. Besides the classification tasks, object detection systems are also considered as one of the basic foundations of computer vision tasks. However, there is no investigation and understanding of the backdoor vulnerability of the object detector, even in the digital world with digital triggers. For the first time, this work demonstrates that existing object detectors are inherently susceptible to physical backdoor attacks. We use a natural T-shirt bought from a market as a trigger to enable the cloaking effect--the person bounding-box disappears in front of the object detector. We show that such a backdoor can be implanted from two exploitable attack scenarios into the object detector, which is outsourced or fine-tuned through a pretrained model. We have extensively evaluated three popular object detection algorithms: anchor-based Yolo-V3, Yolo-V4, and anchor-free CenterNet. Building upon 19 videos shot in real-world scenes, we confirm that the backdoor attack is robust against various factors: movement, distance, angle, non-rigid deformation, and lighting. Specifically, the attack success rate (ASR) in most videos is 100% or close to it, while the clean data accuracy of the backdoored model is the same as its clean counterpart. The latter implies that it is infeasible to detect the backdoor behavior merely through a validation set. The averaged ASR still remains sufficiently high to be 78% in the transfer learning attack scenarios evaluated on CenterNet. See the demo video on https://youtu.be/Q3HOF4OobbY.},
  archiveprefix = {arXiv},
  eprint        = {2201.08619},
  file          = {:http\://arxiv.org/pdf/2201.08619v2:PDF},
  keywords      = {cs.CV, cs.AI, cs.CR},
  primaryclass  = {cs.CV},
}

 
@InProceedings{Li2021,
  author     = {Li, Yuanchun and Hua, Jiayi and Wang, Haoyu and Chen, Chunyang and Liu, Yunxin},
  booktitle  = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
  title      = {{DeepPayload}: {Black}-box {Backdoor} {Attack} on {Deep} {Learning} {Models} through {Neural} {Payload} {Injection}},
  year       = {2021},
  month      = may,
  note       = {ZSCC: 0000025 ISSN: 1558-1225},
  pages      = {263--274},
  abstract   = {Deep learning models are increasingly used in mobile applications as critical components. Unlike the program bytecode whose vulnerabilities and threats have been widely-discussed, whether and how the deep learning models deployed in the applications can be compromised are not well-understood since Neural Networks are usually viewed as a black box. In this paper, we introduce a highly practical backdoor attack achieved with a set of reverse-engineering techniques over compiled deep learning models. The core of the attack is a neural conditional branch constructed with a trigger detector and several operators and injected into the victim model as a malicious payload. The attack is effective as the conditional logic can be flexibly customized by the attacker, and scalable as it does not require any prior knowledge from the original model. We evaluated the attack effectiveness using 5 state-of-the-art deep learning models and real-world samples collected from 30 users. The results demonstrated that the injected backdoor can be triggered with a success rate of 93.5\%, while only brought less than 2ms latency overhead and no more than 1.4\% accuracy decrease. We further conducted an empirical study on real-world mobile deep learning apps collected from Google Play. We found 54 apps that were vulnerable to our attack, including popular and security-critical ones. The results call for the awareness of deep learning application developers and auditors to enhance the protection of deployed models.},
  doi        = {10.1109/ICSE43902.2021.00035},
  file       = {:Li2021 - DeepPayload_ Black Box Backdoor Attack on Deep Learning Models through Neural Payload Injection.pdf:PDF},
  issn       = {1558-1225},
  keywords   = {Deep learning, Training, Neural networks, Internet, Mobile applications, Payloads, Software engineering, deep learning, backdoor attack, mobile applications, reverse engineering, malicious payload},
  priority   = {prio1},
  shorttitle = {{DeepPayload}},
}

 
@TechReport{He2022,
  author     = {He, Mingrui and Chen, Tianyu and Zhou, Haoyi and Zhang, Shanghang and Li, Jianxin},
  title      = {{BadRes}: {Reveal} the {Backdoors} through {Residual} {Connection}},
  year       = {2022},
  month      = sep,
  note       = {ZSCC: 0000000 arXiv:2209.07125 [cs] type: article},
  abstract   = {Generally, residual connections are indispensable network components in building CNNs and Transformers for various downstream tasks in CV and VL, which encourages skip shortcuts between network blocks. However, the layer-by-layer loopback residual connections may also hurt the model's robustness by allowing unsuspecting input. In this paper, we proposed a simple yet strong backdoor attack method - BadRes, where the residual connections play as a turnstile to be deterministic on clean inputs while unpredictable on poisoned ones. We have performed empirical evaluations on four datasets with ViT and BEiT models, and the BadRes achieves 97\% attack success rate while receiving zero performance degradation on clean data. Moreover, we analyze BadRes with state-of-the-art defense methods and reveal the fundamental weakness lying in residual connections.},
  annote     = {Comment: 16pages, 9 figures},
  doi        = {10.48550/arXiv.2209.07125},
  file       = {:he_badres__2022 - BadRes_ Reveal the Backdoors through Residual Connection.pdf:PDF;:He2022 - BadRes_ Reveal the Backdoors through Residual Connection.html:URL},
  keywords   = {Computer Science - Cryptography and Security},
  priority   = {prio2},
  school     = {arXiv},
  shorttitle = {{BadRes}},
  url        = {http://arxiv.org/abs/2209.07125},
  urldate    = {2023-03-02},
}

 
@TechReport{Smith2018,
  author     = {Smith, Leslie N. and Topin, Nicholay},
  title      = {Super-{Convergence}: {Very} {Fast} {Training} of {Neural} {Networks} {Using} {Large} {Learning} {Rates}},
  year       = {2018},
  month      = may,
  note       = {ZSCC: NoCitationData[s0] arXiv:1708.07120 [cs, stat] type: article},
  abstract   = {In this paper, we describe a phenomenon, which we named "super-convergence", where neural networks can be trained an order of magnitude faster than with standard training methods. The existence of super-convergence is relevant to understanding why deep networks generalize well. One of the key elements of super-convergence is training with one learning rate cycle and a large maximum learning rate. A primary insight that allows super-convergence training is that large learning rates regularize the training, hence requiring a reduction of all other forms of regularization in order to preserve an optimal regularization balance. We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate. Experiments demonstrate super-convergence for Cifar-10/100, MNIST and Imagenet datasets, and resnet, wide-resnet, densenet, and inception architectures. In addition, we show that super-convergence provides a greater boost in performance relative to standard training when the amount of labeled training data is limited. The architectures and code to replicate the figures in this paper are available at github.com/lnsmith54/super-convergence. See http://www.fast.ai/2018/04/30/dawnbench-fastai/ for an application of super-convergence to win the DAWNBench challenge (see https://dawn.cs.stanford.edu/benchmark/).},
  annote     = {Comment: This paper was significantly revised to show super-convergence as a general fast training methodology},
  doi        = {10.48550/arXiv.1708.07120},
  file       = {:smith_super-convergence__2018 - Super Convergence_ Very Fast Training of Neural Networks Using Large Learning Rates.pdf:PDF;:Smith2018 - Super Convergence_ Very Fast Training of Neural Networks Using Large Learning Rates.html:URL},
  keywords   = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
  priority   = {prio2},
  school     = {arXiv},
  shorttitle = {Super-{Convergence}},
  url        = {http://arxiv.org/abs/1708.07120},
  urldate    = {2023-03-02},
}

 
@Article{Jiang2022,
  author   = {Jiang, Kai and Peng, Peng and Lian, Youzao and Xu, Weisheng},
  journal  = {Journal of Visual Communication and Image Representation},
  title    = {The encoding method of position embeddings in vision transformer},
  year     = {2022},
  issn     = {1047-3203},
  month    = nov,
  note     = {ZSCC: 0000000},
  pages    = {103664},
  volume   = {89},
  abstract = {In contrast to Convolutional Neural Networks (CNNs), Vision Transformers (ViT) cannot capture sequence ordering of input tokens and require position embeddings. As a learnable fixed-dimension vector, the position embedding improves accuracy while limiting the migration of the model between different input sizes. Hence, this paper conducts an empirical study on position embeddings of pre-trained models, which mainly focuses on two questions: (1) What do the position embeddings learn from training? (2) How do the position embeddings affect the self-attention modules? This paper analyzes the pattern of position embedding in pre-trained models and finds that the linear combination of Gabor filters and edge markers can fit the learned position embeddings well. The Gabor filters and edge markers can occupy some channels to append the position information, and the edge markers have flowed to values in self-attention modules. The experimental results can guide future work to choose suitable position embeddings.},
  doi      = {10.1016/j.jvcir.2022.103664},
  file     = {:jiang_encoding_2022 - The Encoding Method of Position Embeddings in Vision Transformer.html:URL;:Jiang2022 - The Encoding Method of Position Embeddings in Vision Transformer.html:URL;:https\://reader.elsevier.com/reader/sd/pii/S1047320322001845?token=CDD655C8AF698E6FA6F691BCBF7FCC79534BD09EB008FDF9725079D762042206065F89332CB1068296EFE2FDF335B9C9&originRegion=eu-west-1&originCreation=20230303081742:PDF;:The encoding method of position embeddings in vision transformer.pdf:PDF},
  keywords = {Vision transformer, Position embeddings, Gabor filters},
  language = {en},
  priority = {prio1},
  url      = {https://www.sciencedirect.com/science/article/pii/S1047320322001845},
  urldate  = {2023-03-03},
}

 
@InProceedings{Sener2018,
  author    = {Sener, Ozan and Koltun, Vladlen},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  title     = {Multi-{Task} {Learning} as {Multi}-{Objective} {Optimization}},
  year      = {2018},
  publisher = {Curran Associates, Inc.},
  volume    = {31},
  abstract  = {In multi-task learning, multiple tasks are solved jointly, sharing inductive bias between them. Multi-task learning is inherently a multi-objective problem because different tasks may conflict, necessitating a trade-off. A common compromise is to optimize a proxy objective that minimizes a weighted linear combination of per-task losses. However, this workaround is only valid when the tasks do not compete, which is rarely the case. In this paper, we explicitly cast multi-task learning as multi-objective optimization, with the overall objective of finding a Pareto optimal solution. To this end, we use algorithms developed in the gradient-based multi-objective optimization literature. These algorithms are not directly applicable to large-scale learning problems since they scale poorly with the dimensionality of the gradients and the number of tasks. We therefore propose an upper bound for the multi-objective loss and show that it can be optimized efficiently. We further prove that optimizing this upper bound yields a Pareto optimal solution under realistic assumptions. We apply our method to a variety of multi-task deep learning problems including digit classification, scene understanding (joint semantic segmentation, instance segmentation, and depth estimation), and multi-label classification. Our method produces higher-performing models than recent multi-task learning formulations or per-task training.},
  file      = {:Sener2018 - Multi Task Learning As Multi Objective Optimization.pdf:PDF},
  url       = {https://proceedings.neurips.cc/paper/2018/hash/432aca3a1e345e339f35a30c8f65edce-Abstract.html},
  urldate   = {2023-03-21},
}

 
@InProceedings{Qi2023,
  author   = {Qi, Xiangyu and Xie, Tinghao and Li, Yiming and Mahloujifar, Saeed and Mittal, Prateek},
  title    = {Revisiting the {Assumption} of {Latent} {Separability} for {Backdoor} {Defenses}},
  year     = {2023},
  month    = feb,
  abstract = {Recent studies revealed that deep learning is susceptible to backdoor poisoning attacks. An adversary can embed a hidden backdoor into a model to manipulate its predictions by only modifying a few training data, without controlling the training process. Currently, a tangible signature has been widely observed across a diverse set of backdoor poisoning attacks --- models trained on a poisoned dataset tend to learn separable latent representations for poison and clean samples. This latent separation is so pervasive that a family of backdoor defenses directly take it as a default assumption (dubbed latent separability assumption), based on which to identify poison samples via cluster analysis in the latent space. An intriguing question consequently follows: is the latent separation unavoidable for backdoor poisoning attacks? This question is central to understanding whether the assumption of latent separability provides a reliable foundation for defending against backdoor poisoning attacks. In this paper, we design adaptive backdoor poisoning attacks to present counter-examples against this assumption. Our methods include two key components: (1) a set of trigger-planted samples correctly labeled to their semantic classes (other than the target class) that can regularize backdoor learning; (2) asymmetric trigger planting strategies that help to boost attack success rate (ASR) as well as to diversify latent representations of poison samples. Extensive experiments on benchmark datasets verify the effectiveness of our adaptive attacks in bypassing existing latent separation based backdoor defenses. Moreover, our attacks still maintain a high attack success rate with negligible clean accuracy drop. Our studies call for defense designers to take caution when leveraging latent separation as an assumption in their defenses. Our codes are available at https://github.com/Unispac/Circumventing-Backdoor-Defenses.},
  file     = {:Qi2023 - Revisiting the Assumption of Latent Separability for Backdoor Defenses.pdf:PDF},
  language = {en},
  priority = {prio1},
  url      = {https://openreview.net/forum?id=_wSHsgrVali},
  urldate  = {2023-03-23},
}

 
@InProceedings{Ilyas2019,
  author    = {Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  title     = {Adversarial {Examples} {Are} {Not} {Bugs}, {They} {Are} {Features}},
  year      = {2019},
  publisher = {Curran Associates, Inc.},
  volume    = {32},
  abstract  = {Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features (derived from patterns in the data distribution) that are highly predictive, yet brittle and (thus) incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a \{{\textbackslash}em misalignment\} between the (human-specified) notion of robustness and the inherent geometry of the data.},
  file      = {:Ilyas2019 - Adversarial Examples Are Not Bugs, They Are Features.pdf:PDF},
  priority  = {prio1},
  url       = {https://proceedings.neurips.cc/paper/2019/hash/e2c420d928d4bf8ce0ff2ec19b371514-Abstract.html},
  urldate   = {2023-03-24},
}

 
@TechReport{Gan2022,
  author   = {Gan, Leilei and Li, Jiwei and Zhang, Tianwei and Li, Xiaoya and Meng, Yuxian and Wu, Fei and Yang, Yi and Guo, Shangwei and Fan, Chun},
  title    = {Triggerless {Backdoor} {Attack} for {NLP} {Tasks} with {Clean} {Labels}},
  year     = {2022},
  month    = apr,
  note     = {arXiv:2111.07970 [cs] type: article},
  abstract = {Backdoor attacks pose a new threat to NLP models. A standard strategy to construct poisoned data in backdoor attacks is to insert triggers (e.g., rare words) into selected sentences and alter the original label to a target label. This strategy comes with a severe flaw of being easily detected from both the trigger and the label perspectives: the trigger injected, which is usually a rare word, leads to an abnormal natural language expression, and thus can be easily detected by a defense model; the changed target label leads the example to be mistakenly labeled and thus can be easily detected by manual inspections. To deal with this issue, in this paper, we propose a new strategy to perform textual backdoor attacks which do not require an external trigger, and the poisoned samples are correctly labeled. The core idea of the proposed strategy is to construct clean-labeled examples, whose labels are correct but can lead to test label changes when fused with the training set. To generate poisoned clean-labeled examples, we propose a sentence generation model based on the genetic algorithm to cater to the non-differentiable characteristic of text data. Extensive experiments demonstrate that the proposed attacking strategy is not only effective, but more importantly, hard to defend due to its triggerless and clean-labeled nature. Our work marks the first step towards developing triggerless attacking strategies in NLP.},
  annote   = {Comment: Accepted to appear at the main conference of NAACL 2022},
  doi      = {10.48550/arXiv.2111.07970},
  file     = {:Gan2022 - Triggerless Backdoor Attack for NLP Tasks with Clean Labels.pdf:PDF},
  keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2111.07970},
  urldate  = {2023-03-24},
}

 
@Article{Gao2023,
  author     = {Gao, Yinghua and Li, Yiming and Zhu, Linghui and Wu, Dongxian and Jiang, Yong and Xia, Shu-Tao},
  journal    = {Pattern Recognition},
  title      = {Not {All} {Samples} {Are} {Born} {Equal}: {Towards} {Effective} {Clean}-{Label} {Backdoor} {Attacks}},
  year       = {2023},
  issn       = {0031-3203},
  month      = jul,
  pages      = {109512},
  volume     = {139},
  abstract   = {Recent studies demonstrated that deep neural networks (DNNs) are vulnerable to backdoor attacks. The attacked model behaves normally on benign samples, while its predictions are misled whenever adversary-specified trigger patterns appear. Currently, clean-label backdoor attacks are usually regarded as the most stealthy methods in which adversaries can only poison samples from the target class without modifying their labels. However, these attacks can hardly succeed. In this paper, we reveal that the difficulty of clean-label attacks mainly lies in the antagonistic effects of ‘robust features’ related to the target class contained in poisoned samples. Specifically, robust features tend to be easily learned by victim models and thus undermine the learning of trigger patterns. Based on these understandings, we propose a simple yet effective plug-in method to enhance clean-label backdoor attacks by poisoning ‘hard’ instead of random samples. We adopt three classical difficulty metrics as examples to implement our method. We demonstrate that our method can consistently improve vanilla attacks, based on extensive experiments on benchmark datasets.},
  doi        = {10.1016/j.patcog.2023.109512},
  file       = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/abs/pii/S0031320323002121/pdfft?isDTMRedir=true&download=true:application/pdf},
  keywords   = {Backdoor attack, Clean-label attack, Sample selection, Trustworthy ML, AI Security, Deep learning},
  language   = {en},
  shorttitle = {Not {All} {Samples} {Are} {Born} {Equal}},
  url        = {https://www.sciencedirect.com/science/article/pii/S0031320323002121},
  urldate    = {2023-03-24},
}

 
@TechReport{Perin2022,
  author     = {Perin, Guilherme and Wu, Lichao and Picek, Stjepan},
  title      = {I {Know} {What} {Your} {Layers} {Did}: {Layer}-wise {Explainability} of {Deep} {Learning} {Side}-channel {Analysis}},
  year       = {2022},
  number     = {1087},
  abstract   = {Masked cryptographic implementations can be vulnerable to higher-order attacks. For instance, deep neural networks have proven effective for second-order profiling side-channel attacks even in a black-box setting (no prior knowledge of masks and implementation details). While such attacks have been successful, no explanations were provided for understanding why a variety of deep neural networks can (or cannot) learn high-order leakages and what the limitations are. In other words, we lack the explainability on neural network layers combining (or not) unknown and random secret shares, which is a necessary step to defeat, e.g., Boolean masking countermeasures. In this paper, we use information-theoretic metrics to explain the internal activities of deep neural network layers. We propose a novel methodology for the explainability of deep learning-based profiling side-channel analysis (denoted ExDL-SCA) to understand the processing of secret masks. Inspired by the Information Bottleneck theory, our explainability methodology uses perceived information to explain and detect the different phenomena that occur in deep neural networks, such as fitting, compression, and generalization. We provide experimental results on masked AES datasets showing where, what, and why deep neural networks learn relevant features from input trace sets while compressing irrelevant ones, including noise. This paper opens new perspectives for understanding the role of different neural network layers in profiling side-channel attacks.},
  file       = {:Perin2022 - I Know What Your Layers Did_ Layer Wise Explainability of Deep Learning Side Channel Analysis.pdf:PDF},
  keywords   = {Side-channel Analysis, Deep learning, Masking, Explainability, Perceived Information, Information Bottleneck Theory},
  shorttitle = {I {Know} {What} {Your} {Layers} {Did}},
  url        = {https://eprint.iacr.org/2022/1087},
  urldate    = {2023-03-28},
}

@InProceedings{Hong2022,
  author    = {Hong, Sanghyun and Carlini, Nicholas and Kurakin, Alexey},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Handcrafted Backdoors in Deep Neural Networks},
  year      = {2022},
  editor    = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  pages     = {8068--8080},
  publisher = {Curran Associates, Inc.},
  volume    = {35},
  file      = {:Hong2022 - Handcrafted Backdoors in Deep Neural Networks.pdf:PDF},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2022/file/3538a22cd3ceb8f009cc62b9e535c29f-Paper-Conference.pdf},
}

 
@TechReport{Boenisch2021,
  author     = {Boenisch, Franziska and Dziedzic, Adam and Schuster, Roei and Shamsabadi, Ali Shahin and Shumailov, Ilia and Papernot, Nicolas},
  title      = {When the {Curious} {Abandon} {Honesty}: {Federated} {Learning} {Is} {Not} {Private}},
  year       = {2021},
  month      = dec,
  note       = {arXiv:2112.02918 [cs] type: article},
  abstract   = {In federated learning (FL), data does not leave personal devices when they are jointly training a machine learning model. Instead, these devices share gradients with a central party (e.g., a company). Because data never "leaves" personal devices, FL is presented as privacy-preserving. Yet, recently it was shown that this protection is but a thin facade, as even a passive attacker observing gradients can reconstruct data of individual users. In this paper, we argue that prior work still largely underestimates the vulnerability of FL. This is because prior efforts exclusively consider passive attackers that are honest-but-curious. Instead, we introduce an active and dishonest attacker acting as the central party, who is able to modify the shared model's weights before users compute model gradients. We call the modified weights "trap weights". Our active attacker is able to recover user data perfectly and at near zero costs: the attack requires no complex optimization objectives. Instead, it exploits inherent data leakage from model gradients and amplifies this effect by maliciously altering the weights of the shared model. These specificities enable our attack to scale to models trained with large mini-batches of data. Where attackers from prior work require hours to recover a single data point, our method needs milliseconds to capture the full mini-batch of data from both fully-connected and convolutional deep neural networks. Finally, we consider mitigations. We observe that current implementations of differential privacy (DP) in FL are flawed, as they explicitly trust the central party with the crucial task of adding DP noise, and thus provide no protection against a malicious central party. We also consider other defenses and explain why they are similarly inadequate. A significant redesign of FL is required for it to provide any meaningful form of data privacy to users.},
  doi        = {10.48550/arXiv.2112.02918},
  file       = {:Boenisch2021 - When the Curious Abandon Honesty_ Federated Learning Is Not Private.pdf:PDF},
  keywords   = {Computer Science - Machine Learning, Computer Science - Cryptography and Security, Computer Science - Distributed, Parallel, and Cluster Computing},
  school     = {arXiv},
  shorttitle = {When the {Curious} {Abandon} {Honesty}},
  url        = {http://arxiv.org/abs/2112.02918},
  urldate    = {2023-03-31},
}

 
@InProceedings{Tang2020,
  author     = {Tang, Ruixiang and Du, Mengnan and Liu, Ninghao and Yang, Fan and Hu, Xia},
  booktitle  = {Proceedings of the 26th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
  title      = {An {Embarrassingly} {Simple} {Approach} for {Trojan} {Attack} in {Deep} {Neural} {Networks}},
  year       = {2020},
  address    = {New York, NY, USA},
  month      = aug,
  pages      = {218--228},
  publisher  = {Association for Computing Machinery},
  series     = {{KDD} '20},
  abstract   = {With the widespread use of deep neural networks (DNNs) in high-stake applications, the security problem of the DNN models has received extensive attention. In this paper, we investigate a specific security problem called trojan attack, which aims to attack deployed DNN systems relying on the hidden trigger patterns inserted by malicious hackers. We propose a training-free attack approach which is different from previous work, in which trojaned behaviors are injected by retraining model on a poisoned dataset. Specifically, we do not change parameters in the original model but insert a tiny trojan module (TrojanNet) into the target model. The infected model with a malicious trojan can misclassify inputs into a target label when the inputs are stamped with the special trigger. The proposed TrojanNet has several nice properties including (1) it activates by tiny trigger patterns and keeps silent for other signals, (2) it is model-agnostic and could be injected into most DNNs, dramatically expanding its attack scenarios, and (3) the training-free mechanism saves massive training efforts comparing to conventional trojan attack methods. The experimental results show that TrojanNet can inject the trojan into all labels simultaneously (all-label trojan attack) and achieves 100\% attack success rate without affecting model accuracy on original tasks. Experimental analysis further demonstrates that state-of-the-art trojan detection algorithms fail to detect TrojanNet attack. The code is available at https://github.com/trx14/TrojanNet.},
  doi        = {10.1145/3394486.3403064},
  file       = {:Tang2020 - An Embarrassingly Simple Approach for Trojan Attack in Deep Neural Networks.pdf:PDF},
  groups     = {Attack},
  isbn       = {9781450379984},
  keywords   = {deep learning security, anomaly detection, Trojan attack},
  ranking    = {rank4},
  readstatus = {read},
  url        = {https://dl.acm.org/doi/10.1145/3394486.3403064},
  urldate    = {2023-04-03},
}

 
@TechReport{Wang2023,
  author   = {Wang, Peihao and Panda, Rameswar and Hennigen, Lucas Torroba and Greengard, Philip and Karlinsky, Leonid and Feris, Rogerio and Cox, David Daniel and Wang, Zhangyang and Kim, Yoon},
  title    = {Learning to {Grow} {Pretrained} {Models} for {Efficient} {Transformer} {Training}},
  year     = {2023},
  month    = mar,
  note     = {arXiv:2303.00980 [cs] type: article},
  abstract = {Scaling transformers has led to significant breakthroughs in many domains, leading to a paradigm in which larger versions of existing models are trained and released on a periodic basis. New instances of such models are typically trained completely from scratch, despite the fact that they are often just scaled-up versions of their smaller counterparts. How can we use the implicit knowledge in the parameters of smaller, extant models to enable faster training of newer, larger models? This paper describes an approach for accelerating transformer training by learning to grow pretrained transformers, where we learn to linearly map the parameters of the smaller model to initialize the larger model. For tractable learning, we factorize the linear transformation as a composition of (linear) width- and depth-growth operators, and further employ a Kronecker factorization of these growth operators to encode architectural knowledge. Extensive experiments across both language and vision transformers demonstrate that our learned Linear Growth Operator (LiGO) can save up to 50\% computational cost of training from scratch, while also consistently outperforming strong baselines that also reuse smaller pretrained models to initialize larger models.},
  annote   = {Comment: International Conference on Learning Representations (ICLR), 2023},
  doi      = {10.48550/arXiv.2303.00980},
  file     = {:Wang2023 - Learning to Grow Pretrained Models for Efficient Transformer Training.pdf:PDF},
  keywords = {Computer Science - Machine Learning},
  priority = {prio1},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2303.00980},
  urldate  = {2023-04-04},
}

 
@TechReport{Chen2016,
  author     = {Chen, Tianqi and Goodfellow, Ian and Shlens, Jonathon},
  title      = {{Net2Net}: {Accelerating} {Learning} via {Knowledge} {Transfer}},
  year       = {2016},
  month      = apr,
  note       = {arXiv:1511.05641 [cs] type: article},
  abstract   = {We introduce techniques for rapidly transferring the information stored in one neural net into another neural net. The main purpose is to accelerate the training of a significantly larger neural net. During real-world workflows, one often trains very many different neural networks during the experimentation and design process. This is a wasteful process in which each new model is trained from scratch. Our Net2Net technique accelerates the experimentation process by instantaneously transferring the knowledge from a previous network to each new deeper or wider network. Our techniques are based on the concept of function-preserving transformations between neural network specifications. This differs from previous approaches to pre-training that altered the function represented by a neural net when adding layers to it. Using our knowledge transfer mechanism to add depth to Inception modules, we demonstrate a new state of the art accuracy rating on the ImageNet dataset.},
  annote     = {Comment: ICLR 2016 submission},
  doi        = {10.48550/arXiv.1511.05641},
  file       = {:Chen2016 - Net2Net_ Accelerating Learning Via Knowledge Transfer.pdf:PDF},
  keywords   = {Computer Science - Machine Learning},
  priority   = {prio1},
  school     = {arXiv},
  shorttitle = {{Net2Net}},
  url        = {http://arxiv.org/abs/1511.05641},
  urldate    = {2023-04-18},
}

 
@InProceedings{Deshpande2021,
  author     = {Deshpande, Chinmay and Gens, David and Franz, Michael},
  booktitle  = {Proceedings of the 14th {ACM} {Workshop} on {Artificial} {Intelligence} and {Security}},
  title      = {{StackBERT}: {Machine} {Learning} {Assisted} {Static} {Stack} {Frame} {Size} {Recovery} on {Stripped} and {Optimized} {Binaries}},
  year       = {2021},
  address    = {New York, NY, USA},
  month      = nov,
  pages      = {85--95},
  publisher  = {Association for Computing Machinery},
  series     = {{AISec} '21},
  abstract   = {The call stack represents one of the core abstractions that compiler-generated programs leverage to organize binary execution at runtime. For many use cases reasoning about stack accesses of binary functions is crucial: security-sensitive applications may require patching even after deployment, and binary instrumentation, rewriting, and lifting all necessitate detailed knowledge about the function frame layout of the affected program. As no comprehensive solution to the stack symbolization problem exists to date, existing approaches have to resort to workarounds like emulated stack environments, resulting in increased runtime overheads. In this paper we present StackBERT, a framework to statically reason about and reliably recover stack frame information of binary functions in stripped and highly optimized programs. The core idea behind our approach is to formulate binary analysis as a self-supervised learning problem by automatically generating ground truth data from a large corpus of open-source programs. We train a state-of-the-art Transformer model with self-attention and finetune for stack frame size prediction. We show that our finetuned model yields highly accurate estimates of a binary function's stack size from its function body alone across different instruction-set architectures, compiler toolchains, and optimization levels. We successfully verify the static estimates against runtime data through dynamic executions of standard benchmarks and additional studies, demonstrating that StackBERT's predictions generalize to 93.44\% of stripped and highly optimized test binaries not seen during training. We envision these results to be useful for improving binary rewriting and lifting approaches in the future.},
  doi        = {10.1145/3474369.3486865},
  file       = {:Deshpande2021 - StackBERT_ Machine Learning Assisted Static Stack Frame Size Recovery on Stripped and Optimized Binaries.pdf:PDF},
  isbn       = {9781450386579},
  keywords   = {recompilation, stack symbolization, binary lifting, machine learning},
  priority   = {prio1},
  ranking    = {rank1},
  shorttitle = {{StackBERT}},
  url        = {https://dl.acm.org/doi/10.1145/3474369.3486865},
  urldate    = {2023-04-18},
}

 
@InProceedings{Shumailov2021,
  author     = {Shumailov, I and Shumaylov, Zakhar and Kazhdan, Dmitry and Zhao, Yiren and Papernot, Nicolas and Erdogdu, Murat A and Anderson, Ross J},
  booktitle  = {Advances in {Neural} {Information} {Processing} {Systems}},
  title      = {Manipulating {SGD} with {Data} {Ordering} {Attacks}},
  year       = {2021},
  pages      = {18021--18032},
  publisher  = {Curran Associates, Inc.},
  volume     = {34},
  abstract   = {Machine learning is vulnerable to a wide variety of attacks. It is now well understood that by changing the underlying data distribution, an adversary can poison the model trained with it or introduce backdoors. In this paper we present a novel class of training-time attacks that require no changes to the underlying dataset or model architecture, but instead only change the order in which data are supplied to the model. In particular, we find that the attacker can either prevent the model from learning, or poison it to learn behaviours specified by the attacker. Furthermore, we find that even a single adversarially-ordered epoch can be enough to slow down model learning, or even to reset all of the learning progress. Indeed, the attacks presented here are not specific to the model or dataset, but rather target the stochastic nature of modern learning procedures. We extensively evaluate our attacks on computer vision and natural language benchmarks to find that the adversary can disrupt model training and even introduce backdoors.},
  file       = {:Shumailov2021 - Manipulating SGD with Data Ordering Attacks.pdf:PDF},
  ranking    = {rank4},
  readstatus = {read},
  url        = {https://proceedings.neurips.cc/paper/2021/hash/959ab9a0695c467e7caf75431a872e5c-Abstract.html},
  urldate    = {2023-04-19},
}

 
@TechReport{BoberIrizar2022,
  author   = {Bober-Irizar, Mikel and Shumailov, Ilia and Zhao, Yiren and Mullins, Robert and Papernot, Nicolas},
  title    = {Architectural {Backdoors} in {Neural} {Networks}},
  year     = {2022},
  month    = jun,
  note     = {arXiv:2206.07840 [cs] type: article},
  abstract = {Machine learning is vulnerable to adversarial manipulation. Previous literature has demonstrated that at the training stage attackers can manipulate data and data sampling procedures to control model behaviour. A common attack goal is to plant backdoors i.e. force the victim model to learn to recognise a trigger known only by the adversary. In this paper, we introduce a new class of backdoor attacks that hide inside model architectures i.e. in the inductive bias of the functions used to train. These backdoors are simple to implement, for instance by publishing open-source code for a backdoored model architecture that others will reuse unknowingly. We demonstrate that model architectural backdoors represent a real threat and, unlike other approaches, can survive a complete re-training from scratch. We formalise the main construction principles behind architectural backdoors, such as a link between the input and the output, and describe some possible protections against them. We evaluate our attacks on computer vision benchmarks of different scales and demonstrate the underlying vulnerability is pervasive in a variety of training settings.},
  doi      = {10.48550/arXiv.2206.07840},
  file     = {:BoberIrizar2022 - Architectural Backdoors in Neural Networks.pdf:PDF},
  keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2206.07840},
  urldate  = {2023-04-19},
}

 
@TechReport{Gao2020,
  author     = {Gao, Yansong and Xu, Chang and Wang, Derui and Chen, Shiping and Ranasinghe, Damith C. and Nepal, Surya},
  title      = {{STRIP}: {A} {Defence} {Against} {Trojan} {Attacks} on {Deep} {Neural} {Networks}},
  year       = {2020},
  month      = jan,
  note       = {arXiv:1902.06531 [cs] type: article},
  abstract   = {A recent trojan attack on deep neural network (DNN) models is one insidious variant of data poisoning attacks. Trojan attacks exploit an effective backdoor created in a DNN model by leveraging the difficulty in interpretability of the learned model to misclassify any inputs signed with the attacker's chosen trojan trigger. Since the trojan trigger is a secret guarded and exploited by the attacker, detecting such trojan inputs is a challenge, especially at run-time when models are in active operation. This work builds STRong Intentional Perturbation (STRIP) based run-time trojan attack detection system and focuses on vision system. We intentionally perturb the incoming input, for instance by superimposing various image patterns, and observe the randomness of predicted classes for perturbed inputs from a given deployed model---malicious or benign. A low entropy in predicted classes violates the input-dependence property of a benign model and implies the presence of a malicious input---a characteristic of a trojaned input. The high efficacy of our method is validated through case studies on three popular and contrasting datasets: MNIST, CIFAR10 and GTSRB. We achieve an overall false acceptance rate (FAR) of less than 1\%, given a preset false rejection rate (FRR) of 1\%, for different types of triggers. Using CIFAR10 and GTSRB, we have empirically achieved result of 0\% for both FRR and FAR. We have also evaluated STRIP robustness against a number of trojan attack variants and adaptive attacks.},
  annote     = {Comment: 13 pages},
  doi        = {10.48550/arXiv.1902.06531},
  file       = {:Gao2020 - STRIP_ a Defence against Trojan Attacks on Deep Neural Networks.pdf:PDF},
  keywords   = {Computer Science - Cryptography and Security},
  priority   = {prio1},
  school     = {arXiv},
  shorttitle = {{STRIP}},
  url        = {http://arxiv.org/abs/1902.06531},
  urldate    = {2023-04-25},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Others\;0\;0\;0xff00ffff\;\;\;;
1 StaticGroup:Backdoors\;0\;1\;0x0000ffff\;\;\;;
2 StaticGroup:Attack\;0\;0\;0xe64d4dff\;\;\;;
2 StaticGroup:Defenses\;0\;1\;0x8a8a8aff\;\;\;;
}
